---
title: "æ³¨æ„åŠ›æœºåˆ¶åŸç†"
weight: 3
---

# æ³¨æ„åŠ›æœºåˆ¶åŸç†

> æœ¬ç« å°†æ·±å…¥ä»‹ç»è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ•°å­¦åŸç†å’Œè®¡ç®—è¿‡ç¨‹ï¼Œè¿™æ˜¯ç†è§£ vLLM æ ¸å¿ƒä¼˜åŒ–çš„å…³é”®ã€‚

---

## å¼•è¨€

æ³¨æ„åŠ›æœºåˆ¶æ˜¯ Transformer çš„æ ¸å¿ƒåˆ›æ–°ï¼Œä¹Ÿæ˜¯ vLLM ä¼˜åŒ–çš„ä¸»è¦ç›®æ ‡ã€‚ç†è§£æ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—è¿‡ç¨‹ï¼Œå¯¹äºç†è§£ KV Cache å’Œ PagedAttention è‡³å…³é‡è¦ã€‚

---

## 1. æ³¨æ„åŠ›çš„ç›´è§‰ç†è§£

### 1.1 äººç±»æ³¨æ„åŠ›çš„ç±»æ¯”

æƒ³è±¡ä½ åœ¨é˜…è¯»ä¸€ç¯‡æ–‡ç« ï¼Œå½“ä½ çœ‹åˆ°"ä»–"è¿™ä¸ªä»£è¯æ—¶ï¼Œä½ ä¼šè‡ªåŠ¨"å…³æ³¨"å‰æ–‡ä¸­æåˆ°çš„äººåï¼Œä»¥ç†è§£"ä»–"æŒ‡çš„æ˜¯è°ã€‚

è¿™å°±æ˜¯æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒæ€æƒ³ï¼š**è®©æ¨¡å‹å­¦ä¼š"å…³æ³¨"åºåˆ—ä¸­æœ€ç›¸å…³çš„éƒ¨åˆ†**ã€‚

```mermaid
graph LR
    subgraph é˜…è¯»ç†è§£
        T1[å¼ ä¸‰] --> T2[ä»Šå¤©] --> T3[å»äº†] --> T4[å…¬å›­]
        T4 --> T5[ä»–]
        T5 -.->|å…³æ³¨| T1
    end
```

### 1.2 ä»"å…¨å±€è§†é‡"åˆ°"é‡ç‚¹å…³æ³¨"

æ²¡æœ‰æ³¨æ„åŠ›æœºåˆ¶æ—¶ï¼Œæ¨¡å‹åªèƒ½çœ‹åˆ°å›ºå®šçª—å£å†…çš„ä¿¡æ¯ã€‚æœ‰äº†æ³¨æ„åŠ›æœºåˆ¶ï¼š

```mermaid
graph TB
    subgraph å›ºå®šçª—å£
        FW[åªèƒ½çœ‹åˆ°é™„è¿‘å‡ ä¸ª token]
    end

    subgraph æ³¨æ„åŠ›æœºåˆ¶
        ATT[å¯ä»¥å…³æ³¨åºåˆ—ä¸­ä»»æ„ä½ç½®<br/>å¹¶æ ¹æ®ç›¸å…³æ€§åˆ†é…æƒé‡]
    end

    style ATT fill:#c8e6c9
```

---

## 2. è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰è®¡ç®—

### 2.1 Queryã€Keyã€Value çš„å«ä¹‰

è‡ªæ³¨æ„åŠ›ä½¿ç”¨ä¸‰ä¸ªå‘é‡ï¼š

| å‘é‡ | ç±»æ¯” | ä½œç”¨ |
|------|------|------|
| **Query (Q)** | "æˆ‘è¦æ‰¾ä»€ä¹ˆ" | å½“å‰ä½ç½®çš„æŸ¥è¯¢å‘é‡ |
| **Key (K)** | "æˆ‘æ˜¯ä»€ä¹ˆ" | æ¯ä¸ªä½ç½®çš„ç´¢å¼•å‘é‡ |
| **Value (V)** | "æˆ‘çš„å†…å®¹" | æ¯ä¸ªä½ç½®çš„å€¼å‘é‡ |

**ç›´è§‰ç†è§£**ï¼š
- Q æ˜¯"é—®é¢˜"
- K æ˜¯"ç´¢å¼•/æ ‡ç­¾"
- V æ˜¯"å†…å®¹"
- è®¡ç®— Q å’Œæ‰€æœ‰ K çš„ç›¸ä¼¼åº¦ï¼Œç”¨ç›¸ä¼¼åº¦åŠ æƒæ‰€æœ‰ V

### 2.2 è®¡ç®—å…¬å¼

è‡ªæ³¨æ„åŠ›çš„æ ¸å¿ƒå…¬å¼ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$

å…¶ä¸­ï¼š
- $Q$ï¼šQuery çŸ©é˜µï¼Œå½¢çŠ¶ $[seq\_len, d_k]$
- $K$ï¼šKey çŸ©é˜µï¼Œå½¢çŠ¶ $[seq\_len, d_k]$
- $V$ï¼šValue çŸ©é˜µï¼Œå½¢çŠ¶ $[seq\_len, d_v]$
- $d_k$ï¼šKey çš„ç»´åº¦ï¼ˆç”¨äºç¼©æ”¾ï¼‰

### 2.3 è®¡ç®—æ­¥éª¤è¯¦è§£

```mermaid
flowchart TD
    subgraph æ­¥éª¤1: ç”Ÿæˆ Q, K, V
        X[è¾“å…¥ X<br/>seq_len Ã— hidden_dim]
        X --> WQ[W_Q æŠ•å½±]
        X --> WK[W_K æŠ•å½±]
        X --> WV[W_V æŠ•å½±]
        WQ --> Q[Query<br/>seq_len Ã— d_k]
        WK --> K[Key<br/>seq_len Ã— d_k]
        WV --> V[Value<br/>seq_len Ã— d_v]
    end

    subgraph æ­¥éª¤2: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        Q --> MM[Q Ã— K^T]
        K --> MM
        MM --> SC[Ã· âˆšd_k<br/>ç¼©æ”¾]
        SC --> MASK[+ Mask<br/>å¯é€‰]
        MASK --> SM[Softmax]
        SM --> ATT[æ³¨æ„åŠ›æƒé‡<br/>seq_len Ã— seq_len]
    end

    subgraph æ­¥éª¤3: åŠ æƒæ±‚å’Œ
        ATT --> OUT[Ã— V]
        V --> OUT
        OUT --> O[è¾“å‡º<br/>seq_len Ã— d_v]
    end

    style SC fill:#fff9c4
    style SM fill:#c8e6c9
```

### 2.4 é€æ­¥è®¡ç®—ç¤ºä¾‹

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç®€å•çš„åºåˆ—ï¼Œ3 ä¸ª tokenï¼Œæ¯ä¸ª token çš„éšè—ç»´åº¦æ˜¯ 4ï¼š

```python
import torch
import torch.nn.functional as F

# è¾“å…¥
seq_len = 3
d_k = 4

# å‡è®¾ Q, K, V å·²ç»é€šè¿‡çº¿æ€§æŠ•å½±å¾—åˆ°
Q = torch.tensor([
    [1.0, 0.0, 1.0, 0.0],  # token 0 çš„ query
    [0.0, 1.0, 0.0, 1.0],  # token 1 çš„ query
    [1.0, 1.0, 0.0, 0.0],  # token 2 çš„ query
])

K = torch.tensor([
    [1.0, 0.0, 0.0, 1.0],  # token 0 çš„ key
    [0.0, 1.0, 1.0, 0.0],  # token 1 çš„ key
    [1.0, 1.0, 1.0, 1.0],  # token 2 çš„ key
])

V = torch.tensor([
    [1.0, 2.0, 3.0, 4.0],  # token 0 çš„ value
    [5.0, 6.0, 7.0, 8.0],  # token 1 çš„ value
    [9.0, 10., 11., 12.],  # token 2 çš„ value
])

# æ­¥éª¤ 1: è®¡ç®— Q Ã— K^T
scores = Q @ K.T
print("æ³¨æ„åŠ›åˆ†æ•° (æœªç¼©æ”¾):")
print(scores)
# tensor([[1., 1., 2.],
#         [1., 1., 2.],
#         [1., 1., 3.]])

# æ­¥éª¤ 2: ç¼©æ”¾
d_k = 4
scaled_scores = scores / (d_k ** 0.5)
print("\nç¼©æ”¾åçš„åˆ†æ•°:")
print(scaled_scores)

# æ­¥éª¤ 3: Softmax
attention_weights = F.softmax(scaled_scores, dim=-1)
print("\næ³¨æ„åŠ›æƒé‡:")
print(attention_weights)
# æ¯è¡Œå’Œä¸º 1

# æ­¥éª¤ 4: åŠ æƒæ±‚å’Œ
output = attention_weights @ V
print("\nè¾“å‡º:")
print(output)
```

### 2.5 æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–

æ³¨æ„åŠ›æƒé‡å½¢æˆä¸€ä¸ª `[seq_len, seq_len]` çš„çŸ©é˜µï¼š

```
         Token 0  Token 1  Token 2
Token 0 [  0.30    0.30     0.40  ]  # Token 0 å…³æ³¨è°
Token 1 [  0.30    0.30     0.40  ]  # Token 1 å…³æ³¨è°
Token 2 [  0.20    0.20     0.60  ]  # Token 2 å…³æ³¨è°
```

æ¯ä¸€è¡Œè¡¨ç¤ºä¸€ä¸ª token å¯¹æ‰€æœ‰ token çš„æ³¨æ„åŠ›åˆ†å¸ƒï¼ˆå’Œä¸º 1ï¼‰ã€‚

---

## 3. ç¼©æ”¾å› å­ âˆšd çš„ä½œç”¨

### 3.1 ä¸ºä»€ä¹ˆéœ€è¦ç¼©æ”¾

å½“ $d_k$ è¾ƒå¤§æ—¶ï¼Œ$QK^T$ çš„ç‚¹ç§¯ç»“æœä¼šå˜å¾—å¾ˆå¤§ã€‚è¿™ä¼šå¯¼è‡´ï¼š

1. **Softmax é¥±å’Œ**ï¼šå¤§å€¼ç»è¿‡ softmax åè¶‹è¿‘äº 1ï¼Œå°å€¼è¶‹è¿‘äº 0
2. **æ¢¯åº¦æ¶ˆå¤±**ï¼šsoftmax åœ¨é¥±å’ŒåŒºåŸŸçš„æ¢¯åº¦æ¥è¿‘ 0

```mermaid
graph LR
    subgraph æ— ç¼©æ”¾
        S1[å¤§çš„ç‚¹ç§¯å€¼] --> SM1[Softmax é¥±å’Œ]
        SM1 --> G1[æ¢¯åº¦æ¶ˆå¤±]
    end

    subgraph æœ‰ç¼©æ”¾
        S2[ç¼©æ”¾åçš„ç‚¹ç§¯] --> SM2[Softmax æ­£å¸¸]
        SM2 --> G2[æ¢¯åº¦æ­£å¸¸]
    end

    style G1 fill:#ffcdd2
    style G2 fill:#c8e6c9
```

### 3.2 æ•°å­¦è§£é‡Š

å‡è®¾ Q å’Œ K çš„å…ƒç´ æœä»å‡å€¼ 0ã€æ–¹å·® 1 çš„åˆ†å¸ƒï¼Œé‚£ä¹ˆï¼š

- $Q \cdot K$ çš„å‡å€¼ä¸º 0
- $Q \cdot K$ çš„æ–¹å·®ä¸º $d_k$

é™¤ä»¥ $\sqrt{d_k}$ åï¼Œæ–¹å·®å˜ä¸º 1ï¼Œåˆ†å¸ƒæ›´ç¨³å®šã€‚

---

## 4. å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰

### 4.1 ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´

å•å¤´æ³¨æ„åŠ›åªèƒ½å­¦ä¹ ä¸€ç§"å…³æ³¨æ¨¡å¼"ã€‚å¤šå¤´æ³¨æ„åŠ›è®©æ¨¡å‹åŒæ—¶å­¦ä¹ å¤šç§ä¸åŒçš„å…³ç³»ï¼š

```mermaid
graph TB
    subgraph å¤šå¤´æ³¨æ„åŠ›çš„ä¼˜åŠ¿
        H1[Head 1<br/>å…³æ³¨è¯­æ³•å…³ç³»]
        H2[Head 2<br/>å…³æ³¨è¯­ä¹‰å…³ç³»]
        H3[Head 3<br/>å…³æ³¨ä½ç½®å…³ç³»]
        H4[Head 4<br/>å…³æ³¨å…¶ä»–æ¨¡å¼]
    end
```

### 4.2 å¤šå¤´è®¡ç®—è¿‡ç¨‹

```mermaid
graph TD
    X[è¾“å…¥ X<br/>batch Ã— seq Ã— hidden] --> SPLIT[åˆ†å‰²æˆå¤šä¸ªå¤´]

    subgraph å¹¶è¡Œè®¡ç®—
        SPLIT --> H1[Head 1<br/>Attention]
        SPLIT --> H2[Head 2<br/>Attention]
        SPLIT --> H3[Head 3<br/>Attention]
        SPLIT --> HN[Head N<br/>Attention]
    end

    H1 --> CAT[Concat]
    H2 --> CAT
    H3 --> CAT
    HN --> CAT

    CAT --> WO[W_O æŠ•å½±]
    WO --> O[è¾“å‡º]
```

### 4.3 ä»£ç å®ç°

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, hidden_dim, num_heads):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads

        # Q, K, V æŠ•å½±
        self.q_proj = nn.Linear(hidden_dim, hidden_dim)
        self.k_proj = nn.Linear(hidden_dim, hidden_dim)
        self.v_proj = nn.Linear(hidden_dim, hidden_dim)

        # è¾“å‡ºæŠ•å½±
        self.o_proj = nn.Linear(hidden_dim, hidden_dim)

    def forward(self, x):
        batch_size, seq_len, _ = x.shape

        # æŠ•å½±
        Q = self.q_proj(x)  # [batch, seq, hidden]
        K = self.k_proj(x)
        V = self.v_proj(x)

        # é‡å¡‘ä¸ºå¤šå¤´: [batch, seq, num_heads, head_dim]
        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)
        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)
        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)

        # è½¬ç½®: [batch, num_heads, seq, head_dim]
        Q = Q.transpose(1, 2)
        K = K.transpose(1, 2)
        V = V.transpose(1, 2)

        # æ³¨æ„åŠ›è®¡ç®—
        scores = Q @ K.transpose(-2, -1) / (self.head_dim ** 0.5)
        attn_weights = F.softmax(scores, dim=-1)
        output = attn_weights @ V  # [batch, num_heads, seq, head_dim]

        # åˆå¹¶å¤šå¤´
        output = output.transpose(1, 2)  # [batch, seq, num_heads, head_dim]
        output = output.reshape(batch_size, seq_len, -1)  # [batch, seq, hidden]

        # è¾“å‡ºæŠ•å½±
        output = self.o_proj(output)

        return output
```

### 4.4 å¤´æ•°ä¸ç»´åº¦çš„å…³ç³»

```
hidden_dim = num_heads Ã— head_dim
```

**å¸¸è§é…ç½®**ï¼š

| æ¨¡å‹ | hidden_dim | num_heads | head_dim |
|------|-----------|-----------|----------|
| GPT-2 Small | 768 | 12 | 64 |
| GPT-2 Large | 1280 | 20 | 64 |
| LLaMA-7B | 4096 | 32 | 128 |
| LLaMA-70B | 8192 | 64 | 128 |

---

## 5. Masked Attentionï¼ˆå› æœæ©ç ï¼‰

### 5.1 ä¸ºä»€ä¹ˆéœ€è¦æ©ç 

åœ¨è¯­è¨€æ¨¡å‹ä¸­ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ª token æ—¶**ä¸èƒ½çœ‹åˆ°æœªæ¥çš„ token**ã€‚å› æœæ©ç ç¡®ä¿æ¯ä¸ªä½ç½®åªèƒ½å…³æ³¨å®ƒä¹‹å‰çš„ä½ç½®ã€‚

```mermaid
graph LR
    subgraph æ— æ©ç ï¼ˆåŒå‘æ³¨æ„åŠ›ï¼‰
        A1[token 1] <--> A2[token 2]
        A1 <--> A3[token 3]
        A2 <--> A3
    end

    subgraph æœ‰æ©ç ï¼ˆå•å‘æ³¨æ„åŠ›ï¼‰
        B1[token 1]
        B2[token 2] --> B1
        B3[token 3] --> B1
        B3 --> B2
    end
```

### 5.2 æ©ç çŸ©é˜µ

å› æœæ©ç æ˜¯ä¸€ä¸ªä¸‹ä¸‰è§’çŸ©é˜µï¼š

```python
seq_len = 4
mask = torch.tril(torch.ones(seq_len, seq_len))
print(mask)
# tensor([[1., 0., 0., 0.],
#         [1., 1., 0., 0.],
#         [1., 1., 1., 0.],
#         [1., 1., 1., 1.]])
```

**å¯è§†åŒ–**ï¼š

```
         ä½ç½® 0  ä½ç½® 1  ä½ç½® 2  ä½ç½® 3
ä½ç½® 0  [  1      0       0       0   ]  â†’ åªèƒ½çœ‹è‡ªå·±
ä½ç½® 1  [  1      1       0       0   ]  â†’ å¯çœ‹ 0, 1
ä½ç½® 2  [  1      1       1       0   ]  â†’ å¯çœ‹ 0, 1, 2
ä½ç½® 3  [  1      1       1       1   ]  â†’ å¯çœ‹å…¨éƒ¨
```

### 5.3 åº”ç”¨æ©ç 

åœ¨ softmax ä¹‹å‰åº”ç”¨æ©ç ï¼Œå°†ä¸å…è®¸å…³æ³¨çš„ä½ç½®è®¾ä¸ºè´Ÿæ— ç©·ï¼š

```python
def masked_attention(Q, K, V, mask):
    d_k = Q.shape[-1]
    scores = Q @ K.transpose(-2, -1) / (d_k ** 0.5)

    # åº”ç”¨æ©ç ï¼šå°† mask=0 çš„ä½ç½®è®¾ä¸º -inf
    scores = scores.masked_fill(mask == 0, float('-inf'))

    attn_weights = F.softmax(scores, dim=-1)
    output = attn_weights @ V
    return output
```

**æ©ç åçš„æ³¨æ„åŠ›åˆ†æ•°**ï¼š

```
before softmax:
[[ 0.5   -inf   -inf   -inf]
 [ 0.3    0.7   -inf   -inf]
 [ 0.2    0.4    0.6   -inf]
 [ 0.1    0.3    0.5    0.8]]

after softmax:
[[1.00   0.00   0.00   0.00]  # åªå…³æ³¨ä½ç½® 0
 [0.40   0.60   0.00   0.00]  # å…³æ³¨ä½ç½® 0, 1
 [0.25   0.33   0.42   0.00]  # å…³æ³¨ä½ç½® 0, 1, 2
 [0.15   0.22   0.28   0.35]] # å…³æ³¨å…¨éƒ¨
```

---

## 6. æ³¨æ„åŠ›çš„è®¡ç®—å¤æ‚åº¦

### 6.1 æ—¶é—´å¤æ‚åº¦

æ ¸å¿ƒè®¡ç®— $QK^T$ å’Œ $(\text{softmax})V$ï¼š

- $QK^T$ï¼š$[n, d] \times [d, n] = O(n^2 d)$
- $\text{Attention} \times V$ï¼š$[n, n] \times [n, d] = O(n^2 d)$

**æ€»æ—¶é—´å¤æ‚åº¦**ï¼š$O(n^2 d)$

å…¶ä¸­ $n$ æ˜¯åºåˆ—é•¿åº¦ï¼Œ$d$ æ˜¯ç»´åº¦ã€‚

### 6.2 ç©ºé—´å¤æ‚åº¦

éœ€è¦å­˜å‚¨æ³¨æ„åŠ›æƒé‡çŸ©é˜µï¼š

**ç©ºé—´å¤æ‚åº¦**ï¼š$O(n^2)$

### 6.3 é•¿åºåˆ—çš„æŒ‘æˆ˜

```mermaid
graph LR
    subgraph åºåˆ—é•¿åº¦å½±å“
        L1[n=512] --> C1[è®¡ç®—é‡ 262K]
        L2[n=2048] --> C2[è®¡ç®—é‡ 4.2M]
        L3[n=8192] --> C3[è®¡ç®—é‡ 67M]
        L4[n=32768] --> C4[è®¡ç®—é‡ 1B]
    end
```

å½“åºåˆ—é•¿åº¦å¢åŠ  4 å€ï¼Œè®¡ç®—é‡å¢åŠ  16 å€ï¼è¿™æ˜¯é•¿åºåˆ— LLM é¢ä¸´çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚

### 6.4 ä¼˜åŒ–æ–¹æ³•ç®€ä»‹

| æ–¹æ³• | åŸç† | å¤æ‚åº¦ |
|------|------|--------|
| Flash Attention | IO ä¼˜åŒ–ï¼Œå‡å°‘å†…å­˜è®¿é—® | O(nÂ²) ä½†æ›´å¿« |
| Sparse Attention | ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼ | O(nâˆšn) æˆ– O(n) |
| Linear Attention | æ ¸æ–¹æ³•è¿‘ä¼¼ | O(n) |
| Sliding Window | åªå…³æ³¨å±€éƒ¨çª—å£ | O(nw) |

vLLM ä¸»è¦ä½¿ç”¨ **Flash Attention** ä½œä¸ºæ³¨æ„åŠ›åç«¯ã€‚

---

## 7. Grouped-Query Attention (GQA)

### 7.1 ä¼ ç»Ÿ MHA vs GQA

ä¸ºäº†å‡å°‘ KV Cache çš„å†…å­˜å ç”¨ï¼Œç°ä»£æ¨¡å‹ä½¿ç”¨ GQAï¼š

```mermaid
graph TB
    subgraph MHAï¼ˆMulti-Head Attentionï¼‰
        MQ1[Q Head 1] --> MK1[K Head 1]
        MQ2[Q Head 2] --> MK2[K Head 2]
        MQ3[Q Head 3] --> MK3[K Head 3]
        MQ4[Q Head 4] --> MK4[K Head 4]
    end

    subgraph GQAï¼ˆGrouped-Query Attentionï¼‰
        GQ1[Q Head 1] --> GK1[K Group 1]
        GQ2[Q Head 2] --> GK1
        GQ3[Q Head 3] --> GK2[K Group 2]
        GQ4[Q Head 4] --> GK2
    end
```

### 7.2 GQA çš„ä¼˜åŠ¿

| ç‰¹æ€§ | MHA | GQA |
|------|-----|-----|
| Q heads | N | N |
| K/V heads | N | N/group_size |
| KV Cache å¤§å° | 100% | å‡å°‘åˆ° 1/group_size |
| æ¨¡å‹è´¨é‡ | åŸºå‡† | æ¥è¿‘åŸºå‡† |

**ç¤ºä¾‹**ï¼ˆLLaMA-2-70Bï¼‰ï¼š
- Q heads: 64
- KV heads: 8
- KV Cache å‡å°‘ 8 å€ï¼

---

## 8. æ³¨æ„åŠ›ä¸ KV Cache çš„å…³ç³»

### 8.1 ä¸ºä»€ä¹ˆéœ€è¦ç¼“å­˜ K å’Œ V

åœ¨è‡ªå›å½’ç”Ÿæˆä¸­ï¼Œæ¯ç”Ÿæˆä¸€ä¸ªæ–° tokenï¼Œéƒ½éœ€è¦è®¡ç®—å®ƒä¸æ‰€æœ‰å†å² token çš„æ³¨æ„åŠ›ã€‚

**ä¸ä½¿ç”¨ KV Cache**ï¼šæ¯æ¬¡éƒ½é‡æ–°è®¡ç®—æ‰€æœ‰ token çš„ K å’Œ V
**ä½¿ç”¨ KV Cache**ï¼šç¼“å­˜å†å² token çš„ K å’Œ Vï¼Œåªè®¡ç®—æ–° token çš„

è¿™æ­£æ˜¯ä¸‹ä¸€ç« çš„ä¸»é¢˜ï¼

### 8.2 é¢„è§ˆï¼šKV Cache çš„ä½œç”¨

```mermaid
sequenceDiagram
    participant New as æ–° Token
    participant Cache as KV Cache
    participant ATT as Attention

    Note over Cache: å­˜å‚¨å†å² token çš„ K, V

    New->>ATT: è®¡ç®—æ–° token çš„ Q, K, V
    Cache->>ATT: æä¾›å†å² K, V
    ATT->>ATT: Q_new Ã— [K_cache, K_new]^T
    ATT->>ATT: Attention Ã— [V_cache, V_new]
    ATT->>Cache: å°† K_new, V_new åŠ å…¥ç¼“å­˜
```

---

## 9. æœ¬ç« å°ç»“

### æ ¸å¿ƒå…¬å¼

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$

### å…³é”®æ¦‚å¿µ

| æ¦‚å¿µ | è¯´æ˜ |
|------|------|
| Q/K/V | Queryï¼ˆæŸ¥è¯¢ï¼‰ã€Keyï¼ˆé”®ï¼‰ã€Valueï¼ˆå€¼ï¼‰ |
| ç¼©æ”¾å› å­ | $\sqrt{d_k}$ï¼Œé˜²æ­¢ softmax é¥±å’Œ |
| å¤šå¤´æ³¨æ„åŠ› | å¹¶è¡Œå­¦ä¹ å¤šç§æ³¨æ„åŠ›æ¨¡å¼ |
| å› æœæ©ç  | é˜²æ­¢çœ‹åˆ°æœªæ¥ token |
| GQA | å‡å°‘ KV headsï¼Œé™ä½å†…å­˜å ç”¨ |

### è®¡ç®—å¤æ‚åº¦

- æ—¶é—´å¤æ‚åº¦ï¼š$O(n^2 d)$
- ç©ºé—´å¤æ‚åº¦ï¼š$O(n^2)$
- é•¿åºåˆ—æ˜¯ä¸»è¦æŒ‘æˆ˜

### ä¸ vLLM çš„å…³è”

- KV Cache æ˜¯æ³¨æ„åŠ›ä¼˜åŒ–çš„æ ¸å¿ƒ
- PagedAttention ä¼˜åŒ– K/V çš„å†…å­˜ç®¡ç†
- Flash Attention ä¼˜åŒ–æ³¨æ„åŠ›è®¡ç®—é€Ÿåº¦

---

## æ€è€ƒé¢˜

1. å¦‚æœæ²¡æœ‰ç¼©æ”¾å› å­ $\sqrt{d_k}$ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ
2. ä¸ºä»€ä¹ˆ GQA å¯ä»¥åœ¨å‡å°‘ KV heads çš„åŒæ—¶ä¿æŒæ¨¡å‹è´¨é‡ï¼Ÿ
3. åœ¨å› æœæ©ç ä¸‹ï¼Œä½ç½® 0 çš„ token åªèƒ½å…³æ³¨è‡ªå·±ï¼Œè¿™ä¼šå½±å“æ¨¡å‹æ•ˆæœå—ï¼Ÿ

---

## ä¸‹ä¸€æ­¥

ç†è§£äº†æ³¨æ„åŠ›æœºåˆ¶åï¼Œæˆ‘ä»¬å°†æ·±å…¥å­¦ä¹  KV Cache çš„æ¦‚å¿µå’Œä½œç”¨ï¼š

ğŸ‘‰ [ä¸‹ä¸€ç« ï¼šKV Cache æ¦‚å¿µ](04-kv-cache-concept.md)
