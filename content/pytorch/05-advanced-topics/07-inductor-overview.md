---
title: "TorchInductor æ¦‚è§ˆ"
weight: 7
---

> TorchInductor æ˜¯ PyTorch çš„å®˜æ–¹ç¼–è¯‘å™¨åç«¯,ä¸º torch.compile æä¾›åŠ¨æ€å½¢çŠ¶æ„ŸçŸ¥çš„ä»£ç ç”Ÿæˆèƒ½åŠ›

---

## 1. ä»€ä¹ˆæ˜¯ TorchInductor

TorchInductor æ˜¯ PyTorch 2.x å¼•å…¥çš„**åŠ¨æ€å½¢çŠ¶æ„ŸçŸ¥ç¼–è¯‘å™¨åç«¯**,ä½œä¸º `torch.compile` çš„é»˜è®¤åç«¯,è´Ÿè´£å°† FX å›¾ç¼–è¯‘ä¸ºé«˜æ€§èƒ½çš„æœºå™¨ä»£ç ã€‚

### æ ¸å¿ƒç‰¹æ€§

- **åŠ¨æ€å½¢çŠ¶æ”¯æŒ**: ä½¿ç”¨ç¬¦å·å½¢çŠ¶(Symbolic Shapes)å¤„ç†åŠ¨æ€å°ºå¯¸çš„å¼ é‡
- **è‡ªåŠ¨å†…æ ¸èåˆ**: æ™ºèƒ½èåˆå¤šä¸ªç®—å­ä¸ºå•ä¸€å†…æ ¸,å‡å°‘å†…å­˜è®¿é—®
- **å¤šåç«¯ä»£ç ç”Ÿæˆ**: æ”¯æŒ Tritonã€C++ã€SIMD ç­‰å¤šç§åç«¯
- **è®¾å¤‡æ— å…³**: åŒæ—¶æ”¯æŒ CUDAã€CPUã€ROCmã€MPS ç­‰ç¡¬ä»¶

### åœ¨ torch.compile æ ˆä¸­çš„ä½ç½®

```mermaid
flowchart LR
    subgraph user["ç”¨æˆ·å±‚"]
        A["torch.compile(model)"]
    end

    subgraph dynamo["TorchDynamo å±‚"]
        B["å­—èŠ‚ç åˆ†æ"]
        C["å›¾æ•è·"]
        D["Guard ç”Ÿæˆ"]
    end

    subgraph fx["FX å±‚"]
        E["FX GraphModule"]
        F["ç¬¦å·å½¢çŠ¶æ¨å¯¼"]
    end

    subgraph inductor["TorchInductor å±‚"]
        G["GraphLowering"]
        H["Scheduler"]
        I["Codegen"]
    end

    subgraph runtime["è¿è¡Œæ—¶"]
        J["ç¼–è¯‘åçš„å¯æ‰§è¡Œä»£ç "]
    end

    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    F --> G
    G --> H
    H --> I
    I --> J

    style inductor fill:#e1f5dd,stroke:#4caf50,stroke-width:3px
```

**å·¥ä½œæµç¨‹**:
1. **Dynamo** æ•è· Python å­—èŠ‚ç ,ç”Ÿæˆ FX Graph
2. **FX** å¯¹å›¾è¿›è¡Œç¬¦å·å½¢çŠ¶æ¨å¯¼å’Œä¼˜åŒ–
3. **Inductor** å°† FX Graph ç¼–è¯‘ä¸ºä¼˜åŒ–çš„å†…æ ¸ä»£ç 
4. **Runtime** æ‰§è¡Œç”Ÿæˆçš„ä»£ç 

---

## 2. æ ¸å¿ƒæ¶æ„

### æ•´ä½“æ¶æ„

```mermaid
flowchart TD
    subgraph input["è¾“å…¥"]
        A["FX GraphModule"]
    end

    subgraph lowering["å›¾é™ä½å±‚"]
        B["GraphLowering"]
        C["IR Generation"]
    end

    subgraph scheduling["è°ƒåº¦å±‚"]
        D["Scheduler"]
        E["Fusion Decision"]
        F["Memory Planning"]
    end

    subgraph codegen["ä»£ç ç”Ÿæˆå±‚"]
        G["Triton Codegen"]
        H["C++ Codegen"]
        I["SIMD Codegen"]
    end

    subgraph output["è¾“å‡º"]
        J["CompiledFxGraph"]
    end

    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    F --> G
    F --> H
    F --> I
    G --> J
    H --> J
    I --> J

    style lowering fill:#e3f2fd,stroke:#2196f3,stroke-width:2px
    style scheduling fill:#fff3e0,stroke:#ff9800,stroke-width:2px
    style codegen fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px
```

### æ ¸å¿ƒç»„ä»¶

| ç»„ä»¶ | æ–‡ä»¶ | èŒè´£ |
|------|------|------|
| **GraphLowering** | `graph.py` | FX Graph â†’ IR è½¬æ¢ |
| **IR** | `ir.py` | ä¸­é—´è¡¨ç¤ºå±‚(Pointwiseã€Reduction ç­‰) |
| **Lowering** | `lowering.py` | ç®—å­ â†’ IR æ˜ å°„(1000+ ç®—å­) |
| **Scheduler** | `scheduler.py` | èåˆå†³ç­–ã€å†…å­˜è§„åˆ’ |
| **Triton Codegen** | `codegen/triton.py` | ç”Ÿæˆ Triton GPU å†…æ ¸ |
| **C++ Codegen** | `codegen/cpp.py` | ç”Ÿæˆ CPU C++ å†…æ ¸ |
| **CodeCache** | `codecache.py` | ç¼–è¯‘ç¼“å­˜ç®¡ç† |
| **OutputCode** | `output_code.py` | åŒ…è£…ä¸ºå¯è°ƒç”¨å¯¹è±¡ |

---

## 3. ç›®å½•ç»“æ„è¯¦è§£

```
torch/_inductor/
â”œâ”€â”€ __init__.py                  # å…¬å…± API: compile_fx, aoti_compile_and_package
â”œâ”€â”€ compile_fx.py                # ä¸»ç¼–è¯‘æµç¨‹ (2500+ è¡Œ)
â”œâ”€â”€ compile_fx_async.py          # å¼‚æ­¥ç¼–è¯‘æ”¯æŒ
â”œâ”€â”€ graph.py                     # GraphLowering ç±» (FX â†’ IR)
â”œâ”€â”€ ir.py                        # IR å®šä¹‰ (4700+ è¡Œ)
â”‚   â”œâ”€â”€ IRNode, Loops, Pointwise, Reduction
â”‚   â”œâ”€â”€ View ç±»å‹ (Permute, Expand, Slice...)
â”‚   â””â”€â”€ Layout (FixedLayout, FlexibleLayout)
â”œâ”€â”€ lowering.py                  # ç®—å­ lowering (7400+ è¡Œ)
â”œâ”€â”€ scheduler.py                 # èåˆä¸è°ƒåº¦ (6800+ è¡Œ)
â”œâ”€â”€ select_algorithm.py          # ç®—æ³•é€‰æ‹©ä¸è‡ªåŠ¨è°ƒä¼˜
â”œâ”€â”€ codecache.py                 # ç¼–è¯‘ç¼“å­˜
â”œâ”€â”€ config.py                    # 400+ é…ç½®é€‰é¡¹
â”œâ”€â”€ memory.py                    # å†…å­˜è§„åˆ’
â”œâ”€â”€ dependencies.py              # ä¾èµ–åˆ†æ
â”œâ”€â”€ virtualized.py               # è™šæ‹Ÿç®—å­æ¥å£
â”œâ”€â”€ pattern_matcher.py           # æ¨¡å¼åŒ¹é…ä¼˜åŒ–
â”‚
â”œâ”€â”€ codegen/                     # ä»£ç ç”Ÿæˆåç«¯
â”‚   â”œâ”€â”€ triton.py               # Triton å†…æ ¸ç”Ÿæˆ (5500+ è¡Œ)
â”‚   â”œâ”€â”€ cpp.py                  # C++ ä»£ç ç”Ÿæˆ
â”‚   â”œâ”€â”€ cpp_wrapper_cpu.py      # CPU wrapper
â”‚   â”œâ”€â”€ cpp_wrapper_gpu.py      # GPU wrapper
â”‚   â”œâ”€â”€ simd.py                 # SIMD ä¼˜åŒ–
â”‚   â”œâ”€â”€ cuda/                   # CUDA ç‰¹å®šä»£ç 
â”‚   â”œâ”€â”€ rocm/                   # ROCm åç«¯
â”‚   â”œâ”€â”€ cutlass/                # CUTLASS æ¨¡æ¿
â”‚   â””â”€â”€ halide.py               # Halide åç«¯
â”‚
â”œâ”€â”€ fx_passes/                   # FX å›¾ä¼˜åŒ– passes (39+ æ–‡ä»¶)
â”‚   â”œâ”€â”€ pre_grad.py             # æ¢¯åº¦å‰ä¼˜åŒ–
â”‚   â”œâ”€â”€ post_grad.py            # æ¢¯åº¦åä¼˜åŒ–
â”‚   â”œâ”€â”€ fuse_attention.py       # Attention èåˆ
â”‚   â”œâ”€â”€ mkldnn_fusion.py        # MKLDNN èåˆ
â”‚   â””â”€â”€ quantization.py         # é‡åŒ–ä¼˜åŒ–
â”‚
â”œâ”€â”€ kernel/                      # ç‰¹æ®Šå†…æ ¸æ¨¡æ¿
â”‚   â”œâ”€â”€ mm.py                   # çŸ©é˜µä¹˜æ³•
â”‚   â”œâ”€â”€ conv.py                 # å·ç§¯
â”‚   â””â”€â”€ flex/                   # Flex Attention
â”‚
â””â”€â”€ runtime/                     # è¿è¡Œæ—¶å·¥å…·
    â”œâ”€â”€ triton_heuristics.py    # Triton å¯å‘å¼
    â”œâ”€â”€ hints.py                # å†…æ ¸æç¤º
    â””â”€â”€ autotune_cache.py       # è‡ªåŠ¨è°ƒä¼˜ç¼“å­˜
```

---

## 4. æ ¸å¿ƒç¼–è¯‘æµç¨‹

### compile_fx ä¸»æµç¨‹

```python
# torch/_inductor/compile_fx.py
def compile_fx(
    model: GraphModule,
    example_inputs: List[torch.Tensor],
    *,
    inner_compile=None,
    config_patches=None,
):
    """
    ç¼–è¯‘ FX GraphModule ä¸ºä¼˜åŒ–çš„å¯æ‰§è¡Œä»£ç 

    æµç¨‹:
    1. ç¬¦å·å½¢çŠ¶æ¨å¯¼ (FakeTensorProp)
    2. FX passes ä¼˜åŒ–
    3. GraphLowering (FX â†’ IR)
    4. Scheduler èåˆå†³ç­–
    5. Codegen ç”Ÿæˆä»£ç 
    6. ç¼–è¯‘å¹¶åŒ…è£…
    """
    return compile_fx_inner(
        model, example_inputs,
        cudagraphs=config.triton.cudagraphs,
        ...
    )
```

### ç¼–è¯‘é˜¶æ®µæµç¨‹å›¾

```mermaid
sequenceDiagram
    participant User
    participant CompileFx as compile_fx
    participant FakeProp as FakeTensorProp
    participant Passes as FX Passes
    participant Lowering as GraphLowering
    participant Scheduler
    participant Codegen
    participant Output as OutputCode

    User->>CompileFx: FX GraphModule
    CompileFx->>FakeProp: ç¬¦å·å½¢çŠ¶æ¨å¯¼
    FakeProp-->>CompileFx: å¸¦ç¬¦å·å½¢çŠ¶çš„ Graph

    CompileFx->>Passes: å›¾ä¼˜åŒ–
    Note over Passes: pre_grad, post_grad, <br/>joint_graph passes
    Passes-->>CompileFx: ä¼˜åŒ–åçš„ Graph

    CompileFx->>Lowering: FX â†’ IR è½¬æ¢
    Lowering-->>CompileFx: IR Nodes

    CompileFx->>Scheduler: èåˆä¸è°ƒåº¦
    Scheduler-->>CompileFx: Fusion Groups

    CompileFx->>Codegen: ä»£ç ç”Ÿæˆ
    Note over Codegen: Triton / C++ / SIMD
    Codegen-->>CompileFx: ç”Ÿæˆçš„ä»£ç 

    CompileFx->>Output: ç¼–è¯‘å¹¶åŒ…è£…
    Output-->>User: CompiledFxGraph
```

---

## 5. IR ä¸­é—´è¡¨ç¤º

Inductor ä½¿ç”¨åˆ†å±‚çš„ IR è¡¨ç¤ºè®¡ç®—:

### IR å±‚çº§ç»“æ„

```mermaid
classDiagram
    class IRNode {
        <<abstract>>
    }

    class Loops {
        +ranges: List[Expr]
        +reduction_ranges: List[Expr]
    }

    class Pointwise {
        +inner_fn: Callable
        æ”¯æŒ element-wise æ“ä½œ
    }

    class Reduction {
        +reduction_type: str
        +inner_fn: Callable
        æ”¯æŒå½’çº¦æ“ä½œ(sum, max, ...)
    }

    class BaseView {
        +data: IRNode
        é›¶æ‹·è´è§†å›¾å˜æ¢
    }

    class Buffer {
        +layout: Layout
        +data: IRNode
        å¼ é‡å­˜å‚¨æŠ½è±¡
    }

    IRNode <|-- Loops
    IRNode <|-- BaseView
    IRNode <|-- Buffer
    Loops <|-- Pointwise
    Loops <|-- Reduction
```

**å…³é”® IR ç±»å‹**:
- **Pointwise**: Element-wise æ“ä½œ (å¦‚ `add`, `relu`)
- **Reduction**: å½’çº¦æ“ä½œ (å¦‚ `sum`, `max`, `softmax`)
- **View**: é›¶æ‹·è´å˜æ¢ (å¦‚ `transpose`, `reshape`)
- **Buffer**: å¼ é‡å­˜å‚¨,åŒ…å« Layout ä¿¡æ¯

---

## 6. ç®€å•ç¤ºä¾‹

### ä½¿ç”¨ torch.compile

```python
import torch

# å®šä¹‰æ¨¡å‹
def model(x, y):
    a = x + y
    b = a * 2
    return b.sum()

# ç¼–è¯‘æ¨¡å‹
compiled_model = torch.compile(model, backend="inductor")

# è¿è¡Œ
x = torch.randn(1024, 1024, device="cuda")
y = torch.randn(1024, 1024, device="cuda")

result = compiled_model(x, y)
```

### ç¼–è¯‘è¿‡ç¨‹

1. **Dynamo æ•è·**: æ•è· `model` çš„è®¡ç®—å›¾
2. **ç”Ÿæˆ FX Graph**:
   ```
   graph():
       %x : [num_users=1] = placeholder[target=x]
       %y : [num_users=1] = placeholder[target=y]
       %add : [num_users=1] = call_function[target=operator.add](args = (%x, %y))
       %mul : [num_users=1] = call_function[target=operator.mul](args = (%add, 2))
       %sum : [num_users=1] = call_method[target=sum](args = (%mul,))
       return sum
   ```
3. **Inductor ç¼–è¯‘**:
   - `add` + `mul` èåˆä¸ºå•ä¸€ Pointwise å†…æ ¸
   - `sum` ç¼–è¯‘ä¸º Reduction å†…æ ¸
4. **ç”Ÿæˆ Triton ä»£ç **: ä¸¤ä¸ªä¼˜åŒ–å†…æ ¸
5. **æ‰§è¡Œ**: ç›´æ¥è¿è¡Œç¼–è¯‘åçš„ä»£ç 

---

## 7. é…ç½®ç³»ç»Ÿ

Inductor æä¾› 400+ é…ç½®é€‰é¡¹,é€šè¿‡ `torch._inductor.config` è®¿é—®:

```python
import torch._inductor.config as config

# è°ƒè¯•é€‰é¡¹
config.debug = True                    # æ‰“å°è¯¦ç»†ä¿¡æ¯
config.trace.enabled = True            # è¿½è¸ªç¼–è¯‘è¿‡ç¨‹

# ä¼˜åŒ–é€‰é¡¹
config.epilogue_fusion = True          # å¯ç”¨ epilogue èåˆ
config.max_autotune = True             # æœ€å¤§åŒ–è‡ªåŠ¨è°ƒä¼˜

# ä»£ç ç”Ÿæˆé€‰é¡¹
config.cpp_wrapper = True              # ä½¿ç”¨ C++ wrapper
config.triton.cudagraphs = True        # å¯ç”¨ CUDA Graphs

# ç¼“å­˜é€‰é¡¹
config.fx_graph_cache = True           # FX å›¾ç¼“å­˜
config.autotune_local_cache = True     # è‡ªåŠ¨è°ƒä¼˜ç¼“å­˜
```

### å…³é”®é…ç½®ç±»åˆ«

| ç±»åˆ« | é…ç½®é¡¹ç¤ºä¾‹ | ç”¨é€” |
|------|----------|------|
| **èåˆ** | `epilogue_fusion`, `pattern_matcher` | æ§åˆ¶å†…æ ¸èåˆç­–ç•¥ |
| **è°ƒä¼˜** | `max_autotune`, `coordinate_descent_tuning` | è‡ªåŠ¨è°ƒä¼˜è¡Œä¸º |
| **ä»£ç ç”Ÿæˆ** | `cpp_wrapper`, `triton.unique_kernel_names` | ä»£ç ç”Ÿæˆé€‰é¡¹ |
| **è°ƒè¯•** | `debug`, `trace.enabled`, `output_code` | è°ƒè¯•ä¸è¿½è¸ª |
| **æ€§èƒ½** | `benchmark_kernel`, `epilogue_fusion_first` | æ€§èƒ½ä¼˜åŒ– |

---

## 8. ä¸å…¶ä»–ç¼–è¯‘å™¨å¯¹æ¯”

| ç‰¹æ€§ | TorchInductor | TorchScript | ONNX Runtime | TVM |
|------|---------------|-------------|--------------|-----|
| **åŠ¨æ€å½¢çŠ¶** | âœ… åŸç”Ÿæ”¯æŒ | âŒ éœ€é™æ€å½¢çŠ¶ | âš ï¸ éƒ¨åˆ†æ”¯æŒ | âš ï¸ éƒ¨åˆ†æ”¯æŒ |
| **è‡ªåŠ¨èåˆ** | âœ… æ™ºèƒ½èåˆ | âŒ æ‰‹åŠ¨ä¼˜åŒ– | âœ… æœ‰é™èåˆ | âœ… éœ€æ‰‹åŠ¨è°ƒä¼˜ |
| **Python å…¼å®¹** | âœ… å®Œå…¨å…¼å®¹ | âš ï¸ å—é™å­é›† | âŒ éœ€å¯¼å‡º | âŒ éœ€å¯¼å‡º |
| **å¤šåç«¯** | âœ… Triton/C++/SIMD | âŒ ä»…è§£é‡Šæ‰§è¡Œ | âœ… å¤šåç«¯ | âœ… å¤šåç«¯ |
| **è°ƒè¯•æ€§** | âœ… å¯è¯»ç”Ÿæˆä»£ç  | âš ï¸ å­—èŠ‚ç éš¾è¯» | âš ï¸ ä¸­ç­‰ | âŒ è¾ƒéš¾ |
| **ä¸Šæ‰‹éš¾åº¦** | ğŸŸ¢ ä½(è‡ªåŠ¨) | ğŸŸ¡ ä¸­ç­‰ | ğŸŸ¡ ä¸­ç­‰ | ğŸ”´ é«˜ |

---

## 9. ä¸‹ä¸€æ­¥

- **[ç¼–è¯‘ç®¡çº¿æµç¨‹](./01-compile-pipeline.md)**: æ·±å…¥ `compile_fx` çš„å®Œæ•´æµç¨‹
- **[IR ä¸ Lowering](./02-ir-lowering.md)**: è¯¦è§£ä¸­é—´è¡¨ç¤ºå’Œç®—å­é™ä½
- **[è°ƒåº¦å™¨ä¸èåˆ](./03-scheduler-fusion.md)**: èåˆç­–ç•¥ä¸å†…å­˜ä¼˜åŒ–
- **[Triton ä»£ç ç”Ÿæˆ](./04-triton-codegen.md)**: GPU å†…æ ¸ç”Ÿæˆç»†èŠ‚
- **[C++ ä»£ç ç”Ÿæˆ](./05-cpp-codegen.md)**: CPU å†…æ ¸ç”Ÿæˆç»†èŠ‚
- **[è°ƒè¯•æŒ‡å—](./06-inductor-debug.md)**: è°ƒè¯•æŠ€å·§ä¸å¸¸è§é—®é¢˜

---

## 10. å…³é”®æ–‡ä»¶é€ŸæŸ¥

| åŠŸèƒ½ | æ–‡ä»¶è·¯å¾„ | è¯´æ˜ |
|------|---------|------|
| ç¼–è¯‘å…¥å£ | `torch/_inductor/compile_fx.py` | `compile_fx()`, `compile_fx_inner()` |
| å›¾é™ä½ | `torch/_inductor/graph.py` | `GraphLowering` ç±» |
| IR å®šä¹‰ | `torch/_inductor/ir.py` | IRNode, Loops, Pointwise, Reduction |
| ç®—å­æ˜ å°„ | `torch/_inductor/lowering.py` | `lowerings` å­—å…¸,1000+ ç®—å­ |
| èåˆè°ƒåº¦ | `torch/_inductor/scheduler.py` | `Scheduler`, èåˆé€»è¾‘ |
| Triton ç”Ÿæˆ | `torch/_inductor/codegen/triton.py` | `TritonKernel` ç±» |
| C++ ç”Ÿæˆ | `torch/_inductor/codegen/cpp.py` | C++ codegen |
| é…ç½® | `torch/_inductor/config.py` | æ‰€æœ‰é…ç½®é€‰é¡¹ |

---

> **æç¤º**: Inductor æ˜¯ PyTorch 2.x çš„æ ¸å¿ƒç»„ä»¶,ç†è§£å…¶æ¶æ„æ˜¯æ·±å…¥ torch.compile çš„å…³é”®ã€‚å»ºè®®ä»ç¼–è¯‘æµç¨‹å¼€å§‹,é€æ­¥æ·±å…¥ IRã€è°ƒåº¦å’Œä»£ç ç”Ÿæˆç»†èŠ‚ã€‚
