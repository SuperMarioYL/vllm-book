---
title: "Triton ä»£ç ç”Ÿæˆ"
weight: 8
---

> æ·±å…¥è§£æ Inductor å¦‚ä½•ç”Ÿæˆé«˜æ€§èƒ½ Triton GPU å†…æ ¸

---

## 1. Triton åç«¯æ¦‚è¿°

**Triton** æ˜¯ä¸€ä¸ª GPU ç¼–ç¨‹è¯­è¨€,æä¾›ç±» Python è¯­æ³•ç¼–å†™é«˜æ€§èƒ½ CUDA å†…æ ¸ã€‚Inductor ä½¿ç”¨ Triton ä½œä¸ºä¸»è¦çš„ GPU ä»£ç ç”Ÿæˆåç«¯ã€‚

### Triton vs CUDA

| ç‰¹æ€§ | Triton | CUDA C++ |
|------|--------|----------|
| **è¯­æ³•** | Python-like | C++ |
| **å†…å­˜ç®¡ç†** | è‡ªåŠ¨åˆ†å— | æ‰‹åŠ¨ç®¡ç† |
| **å¹¶è¡Œæ¨¡å‹** | Block-level | Thread-level |
| **ä¸Šæ‰‹éš¾åº¦** | ğŸŸ¢ ä½ | ğŸ”´ é«˜ |
| **æ€§èƒ½** | ğŸŸ¡ æ¥è¿‘æ‰‹å†™ CUDA | ğŸŸ¢ æœ€ä¼˜ |

---

## 2. TritonKernel ç±»

### æ ¸å¿ƒç»“æ„

```python
# torch/_inductor/codegen/triton.py
class TritonKernel:
    """Triton å†…æ ¸ç”Ÿæˆå™¨"""

    def __init__(self, *groups, index_dtype, mutations, pid_cache):
        self.args = KernelArgs()         # å†…æ ¸å‚æ•°
        self.loads = []                  # åŠ è½½æ“ä½œåˆ—è¡¨
        self.stores = []                 # å­˜å‚¨æ“ä½œåˆ—è¡¨
        self.compute = []                # è®¡ç®—æ“ä½œåˆ—è¡¨
        self.indexing_code = []          # ç´¢å¼•è®¡ç®—ä»£ç 
        self.suffix = []                 # åç¼€ä»£ç (å¦‚æ–­è¨€)

    def codegen_kernel(self, name):
        """ç”Ÿæˆå®Œæ•´çš„ Triton å†…æ ¸"""
        # 1. ç”Ÿæˆç´¢å¼•ä»£ç 
        index_code = self.indexing()

        # 2. ç”Ÿæˆè®¡ç®—ä»£ç 
        compute_code = self.codegen_body()

        # 3. ç»„è£…å†…æ ¸
        kernel_code = f"""
@triton.jit
def {name}({self.args.signature()}):
{indent(index_code)}
{indent(compute_code)}
"""
        return kernel_code
```

---

## 3. ä»£ç ç”Ÿæˆæµç¨‹

### ä¸»æµç¨‹

```mermaid
sequenceDiagram
    participant Scheduler
    participant TritonScheduling
    participant TritonKernel
    participant Output

    Scheduler->>TritonScheduling: FusedSchedulerNode
    TritonScheduling->>TritonKernel: åˆ›å»º Kernel

    loop æ¯ä¸ª Node
        TritonScheduling->>TritonKernel: codegen_node()
    end

    TritonKernel->>TritonKernel: indexing()
    Note over TritonKernel: ç”Ÿæˆç´¢å¼•ä»£ç 

    TritonKernel->>TritonKernel: codegen_body()
    Note over TritonKernel: ç”Ÿæˆè®¡ç®—ä»£ç 

    TritonKernel->>TritonKernel: store_output()
    Note over TritonKernel: ç”Ÿæˆå­˜å‚¨ä»£ç 

    TritonKernel->>Output: å®Œæ•´ Triton ä»£ç 
```

---

## 4. ç´¢å¼•ç”Ÿæˆ (indexing)

### Program ID è®¡ç®—

```python
def indexing(self):
    """ç”Ÿæˆç´¢å¼•è®¡ç®—ä»£ç """
    code = []

    # 1. è·å– program_id
    for i, range_expr in enumerate(self.ranges):
        code.append(f"pid_{i} = tl.program_id({i})")

    # 2. è®¡ç®—å—åç§»
    for i, (range_expr, block_size) in enumerate(
        zip(self.ranges, self.block_sizes)
    ):
        code.append(
            f"offset_{i} = pid_{i} * {block_size} + tl.arange(0, {block_size})"
        )

    # 3. ç”Ÿæˆ mask
    for i, range_expr in enumerate(self.ranges):
        code.append(f"mask_{i} = offset_{i} < {range_expr}")

    return "\n".join(code)
```

**ç”Ÿæˆä»£ç ç¤ºä¾‹**:
```python
# 2D ç´¢å¼•
pid_0 = tl.program_id(0)
pid_1 = tl.program_id(1)

offset_0 = pid_0 * XBLOCK + tl.arange(0, XBLOCK)
offset_1 = pid_1 * YBLOCK + tl.arange(0, YBLOCK)

mask_0 = offset_0 < xnumel
mask_1 = offset_1 < ynumel
```

---

## 5. åŠ è½½ä»£ç ç”Ÿæˆ

### load() æ“ä½œ

```python
def load(self, name, index):
    """ç”Ÿæˆ tl.load ä»£ç """
    # 1. è®¡ç®—å†…å­˜åœ°å€
    ptr = f"{name}_ptr + {self.codegen_indexing(index)}"

    # 2. ç”Ÿæˆ mask
    mask = self.codegen_mask(index)

    # 3. ç”Ÿæˆ load ä»£ç 
    load_code = f"tl.load({ptr}, mask={mask})"

    # 4. æ·»åŠ åˆ° loads åˆ—è¡¨
    tmp_var = f"tmp{len(self.loads)}"
    self.loads.append(f"{tmp_var} = {load_code}")

    return tmp_var
```

**ç”Ÿæˆä»£ç ç¤ºä¾‹**:
```python
# åŠ è½½ x[i, j]
tmp0 = tl.load(x_ptr + offset_0 * stride_x0 + offset_1 * stride_x1,
               mask=mask_0 & mask_1)
```

---

## 6. è®¡ç®—ä»£ç ç”Ÿæˆ

### Pointwise æ“ä½œ

```python
def codegen_pointwise(self, node: Pointwise):
    """ç”Ÿæˆ Pointwise è®¡ç®—ä»£ç """
    # 1. åŠ è½½è¾“å…¥
    inputs = []
    for input_name in node.get_read_names():
        tmp = self.load(input_name, index)
        inputs.append(tmp)

    # 2. ç”Ÿæˆè®¡ç®—
    result = self.codegen_inner_fn(node.inner_fn, inputs)

    # 3. å­˜å‚¨ç»“æœ
    self.store(node.get_name(), index, result)
```

**ç¤ºä¾‹ - èåˆçš„ Pointwise**:
```python
# inner_fn: relu((x + 1) * 2)

# ç”Ÿæˆä»£ç 
tmp0 = tl.load(x_ptr + offset, mask=mask)
tmp1 = tmp0 + 1.0
tmp2 = tmp1 * 2.0
tmp3 = tl.where(tmp2 > 0, tmp2, 0.0)  # relu
tl.store(out_ptr + offset, tmp3, mask=mask)
```

### Reduction æ“ä½œ

```python
def codegen_reduction(self, node: Reduction):
    """ç”Ÿæˆ Reduction ä»£ç """
    reduction_type = node.reduction_type

    # 1. åˆå§‹åŒ–ç´¯åŠ å™¨
    if reduction_type == "sum":
        acc_init = "0.0"
    elif reduction_type == "max":
        acc_init = "float('-inf')"

    code = [f"acc = {acc_init}"]

    # 2. Reduction å¾ªç¯
    code.append(f"for roffset in range(0, {reduction_range}, RBLOCK):")

    # 3. åŠ è½½æ•°æ®
    load_code = self.load_reduction(node, "roffset")
    code.append(f"    val = {load_code}")

    # 4. ç´¯åŠ 
    if reduction_type == "sum":
        code.append("    acc += val")
    elif reduction_type == "max":
        code.append("    acc = tl.maximum(acc, val)")

    # 5. å­˜å‚¨ç»“æœ
    code.append(f"tl.store(out_ptr + offset, acc, mask=mask)")

    return "\n".join(code)
```

**ç”Ÿæˆä»£ç ç¤ºä¾‹ - Sum Reduction**:
```python
# sum(x, dim=1)
acc = 0.0
for roffset in range(0, reduction_numel, RBLOCK):
    ridx = roffset + tl.arange(0, RBLOCK)
    rmask = ridx < reduction_numel
    val = tl.load(x_ptr + offset_0 * stride_x0 + ridx * stride_x1,
                  mask=mask_0 & rmask)
    acc += val

tl.store(out_ptr + offset_0, acc, mask=mask_0)
```

---

## 7. è‡ªåŠ¨è°ƒä¼˜ (Autotuning)

### Triton é…ç½®ç©ºé—´

```python
# torch/_inductor/select_algorithm.py
class TritonChoice:
    """Triton è‡ªåŠ¨è°ƒä¼˜é€‰æ‹©"""

    def __init__(self, configs):
        self.configs = configs  # é…ç½®åˆ—è¡¨

    def autotune(self, example_inputs):
        """è‡ªåŠ¨è°ƒä¼˜é€‰æ‹©æœ€ä¼˜é…ç½®"""
        best_config = None
        best_time = float("inf")

        for config in self.configs:
            # 1. ä½¿ç”¨è¯¥é…ç½®ç¼–è¯‘å†…æ ¸
            kernel = compile_with_config(config)

            # 2. åŸºå‡†æµ‹è¯•
            time = benchmark_kernel(kernel, example_inputs)

            # 3. æ›´æ–°æœ€ä¼˜é…ç½®
            if time < best_time:
                best_time = time
                best_config = config

        return best_config
```

### é…ç½®å‚æ•°

```python
# Block sizes é…ç½®
configs = [
    triton.Config(
        {"XBLOCK": 256, "YBLOCK": 64}, num_warps=4, num_stages=2
    ),
    triton.Config(
        {"XBLOCK": 512, "YBLOCK": 32}, num_warps=8, num_stages=3
    ),
    triton.Config(
        {"XBLOCK": 1024, "YBLOCK": 16}, num_warps=16, num_stages=4
    ),
]
```

**è°ƒä¼˜å‚æ•°**:
- `XBLOCK`, `YBLOCK`: Block å¤§å°
- `num_warps`: Warp æ•°é‡(æ¯ä¸ª SM ä¸Šçš„å¹¶è¡Œåº¦)
- `num_stages`: Pipeline é˜¶æ®µ(è½¯ä»¶æµæ°´çº¿)

---

## 8. å®Œæ•´ Kernel ç¤ºä¾‹

### èåˆ Add + ReLU

**è¾“å…¥ IR**:
```python
# y = relu(x + 1)
pointwise = Pointwise.create(
    inner_fn=lambda idx: ops.relu(ops.add(x[idx], 1.0)),
    ranges=[sympy.Symbol("s0")],
)
```

**ç”Ÿæˆçš„ Triton ä»£ç **:
```python
@triton.jit
def fused_add_relu_kernel(
    x_ptr,
    out_ptr,
    xnumel,
    XBLOCK: tl.constexpr,
):
    # ç´¢å¼•è®¡ç®—
    pid = tl.program_id(0)
    xoffset = pid * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)
    xmask = xindex < xnumel

    # åŠ è½½
    tmp0 = tl.load(x_ptr + xindex, mask=xmask)

    # è®¡ç®—
    tmp1 = tmp0 + 1.0
    tmp2 = tl.where(tmp1 > 0, tmp1, 0.0)  # relu

    # å­˜å‚¨
    tl.store(out_ptr + xindex, tmp2, mask=xmask)


# å¯åŠ¨é…ç½®
grid = lambda meta: (triton.cdiv(xnumel, meta["XBLOCK"]),)
fused_add_relu_kernel[grid](x, out, xnumel, XBLOCK=256)
```

---

## 9. Reduction Tree

### Online Softmax

```python
# Softmax ä½¿ç”¨ Reduction Tree ä¼˜åŒ–
@triton.jit
def softmax_kernel(x_ptr, out_ptr, ...):
    # Stage 1: è®¡ç®— max (ä½¿ç”¨ tree reduction)
    max_val = float("-inf")
    for i in range(0, N, BLOCK):
        val = tl.load(x_ptr + i)
        max_val = tl.maximum(max_val, val)

    # Warp-level reduction
    max_val = tl.max(max_val, axis=0)

    # Stage 2: è®¡ç®— sum(exp(x - max))
    sum_exp = 0.0
    for i in range(0, N, BLOCK):
        val = tl.load(x_ptr + i)
        exp_val = tl.exp(val - max_val)
        sum_exp += exp_val

    sum_exp = tl.sum(sum_exp, axis=0)

    # Stage 3: å½’ä¸€åŒ–
    for i in range(0, N, BLOCK):
        val = tl.load(x_ptr + i)
        result = tl.exp(val - max_val) / sum_exp
        tl.store(out_ptr + i, result)
```

---

## 10. TMA æ”¯æŒ

### Tensor Memory Accelerator

```python
# Hopper GPU (H100) çš„ TMA æ”¯æŒ
@triton.jit
def matmul_tma_kernel(a_ptr, b_ptr, c_ptr, ...):
    # ä½¿ç”¨ TMA åŠ è½½
    a_block = tl.experimental.tma.load(a_desc, [pid_m, pid_k])
    b_block = tl.experimental.tma.load(b_desc, [pid_k, pid_n])

    # çŸ©é˜µä¹˜æ³•
    acc = tl.dot(a_block, b_block)

    # TMA å­˜å‚¨
    tl.experimental.tma.store(c_desc, acc, [pid_m, pid_n])
```

---

## 11. è°ƒè¯•ç”Ÿæˆä»£ç 

### æŸ¥çœ‹ç”Ÿæˆçš„ Triton ä»£ç 

```python
import torch._inductor.config as config

# å¯ç”¨ä»£ç è¾“å‡º
config.debug = True
config.trace.enabled = True

# ç¼–è¯‘æ¨¡å‹
model = torch.compile(my_model, backend="inductor")

# ç”Ÿæˆçš„ä»£ç ä¿å­˜åœ¨:
# /tmp/torchinductor_<user>/<hash>/kernel_*.py
```

### æ‰‹åŠ¨è¿è¡Œ Triton å†…æ ¸

```python
import triton

# åŠ è½½ç”Ÿæˆçš„å†…æ ¸
from /tmp/torchinductor_xxx.kernel_0 import fused_kernel

# å‡†å¤‡è¾“å…¥
x = torch.randn(1024, device="cuda")
out = torch.empty(1024, device="cuda")

# æ‰‹åŠ¨è°ƒç”¨
grid = (triton.cdiv(1024, 256),)
fused_kernel[grid](x, out, 1024, XBLOCK=256)
```

---

## 12. æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### Block Size é€‰æ‹©

```python
# å° Tensor: ä½¿ç”¨å° Block
# å¤§ Tensor: ä½¿ç”¨å¤§ Block

# å¯å‘å¼
if numel < 1024:
    XBLOCK = 64
elif numel < 1024 * 1024:
    XBLOCK = 256
else:
    XBLOCK = 1024
```

### Memory Coalescing

```python
# å¥½çš„è®¿é—®æ¨¡å¼(è¿ç»­)
for i in range(0, N, BLOCK):
    idx = i + tl.arange(0, BLOCK)  # è¿ç»­ç´¢å¼•
    val = tl.load(ptr + idx)

# å·®çš„è®¿é—®æ¨¡å¼(è·¨æ­¥)
for i in range(0, N, BLOCK):
    idx = i * stride + tl.arange(0, BLOCK)  # stride >> 1
    val = tl.load(ptr + idx)
```

---

## 13. ä¸‹ä¸€æ­¥

- **[C++ ä»£ç ç”Ÿæˆ](./05-cpp-codegen.md)**: CPU åç«¯çš„ä»£ç ç”Ÿæˆ
- **[Inductor è°ƒè¯•](./06-inductor-debug.md)**: è°ƒè¯•æŠ€å·§ä¸å¸¸è§é—®é¢˜

---

## 14. æ€»ç»“

| ç»„ä»¶ | èŒè´£ | å…³é”®æ–‡ä»¶ |
|------|------|---------|
| **TritonKernel** | ä»£ç ç”Ÿæˆæ ¸å¿ƒ | `codegen/triton.py` |
| **indexing** | ç´¢å¼•è®¡ç®— | `TritonKernel.indexing()` |
| **codegen_body** | è®¡ç®—é€»è¾‘ | `TritonKernel.codegen_body()` |
| **Autotuning** | é…ç½®æœç´¢ | `select_algorithm.py` |
| **TritonChoice** | é…ç½®ç®¡ç† | `select_algorithm.py` |

Triton åç«¯æ˜¯ Inductor GPU æ€§èƒ½çš„å…³é”®,é€šè¿‡è‡ªåŠ¨åˆ†å—ã€èåˆå’Œè°ƒä¼˜,ç”Ÿæˆæ¥è¿‘æ‰‹å†™ CUDA çš„é«˜æ€§èƒ½å†…æ ¸ã€‚
