---
title: "vLLM æ•´ä½“æ¶æ„æ¦‚è§ˆ"
weight: 3
---


> æœ¬ç« å°†å¸¦ä½ äº†è§£ vLLM çš„æ•´ä½“æ¶æ„è®¾è®¡ï¼ŒåŒ…æ‹¬æ ¸å¿ƒç»„ä»¶ã€æ•°æ®æµç¨‹å’Œä»£ç ç›®å½•ç»“æ„ã€‚

---

## å¼•è¨€

ç»è¿‡å‰ä¸¤ç« çš„å­¦ä¹ ï¼Œæˆ‘ä»¬å·²ç»äº†è§£äº† LLM æ¨ç†é¢ä¸´çš„æŒ‘æˆ˜ä»¥åŠ vLLM çš„æ ¸å¿ƒåˆ›æ–°ç†å¿µã€‚æœ¬ç« å°†ä»ç³»ç»Ÿæ¶æ„çš„è§’åº¦ï¼Œå…¨é¢ä»‹ç» vLLM çš„è®¾è®¡ã€‚

ç†è§£æ¶æ„æ˜¯æ·±å…¥å­¦ä¹ çš„åŸºç¡€ã€‚å½“ä½ åç»­é˜…è¯»ä»£ç æˆ–è°ƒè¯•é—®é¢˜æ—¶ï¼Œè¿™å¼ "åœ°å›¾"å°†å¸®åŠ©ä½ å¿«é€Ÿå®šä½ã€‚

---

## 1. ç³»ç»Ÿæ¶æ„å…¨æ™¯å›¾

### 1.1 é«˜å±‚æ¶æ„

vLLM é‡‡ç”¨åˆ†å±‚æ¶æ„è®¾è®¡ï¼Œä»ä¸Šåˆ°ä¸‹åˆ†ä¸ºå››å±‚ï¼š

```mermaid
graph TD
    subgraph user_interface["ç”¨æˆ·æ¥å£å±‚"]
        A1["Python API<br/>LLM ç±»"]
        A2["CLI<br/>vllm serve"]
        A3["OpenAI API<br/>HTTP Server"]
        A4[gRPC Server]
    end

    subgraph engine_layer["å¼•æ“å±‚"]
        B1["LLMEngine<br/>åŒæ­¥å¼•æ“"]
        B2["AsyncLLM<br/>å¼‚æ­¥å¼•æ“"]
        B3["InputProcessor<br/>è¾“å…¥å¤„ç†"]
        B4["OutputProcessor<br/>è¾“å‡ºå¤„ç†"]
    end

    subgraph core_layer["æ ¸å¿ƒå±‚"]
        C1["EngineCore<br/>æ ¸å¿ƒé€»è¾‘"]
        C2["Scheduler<br/>è°ƒåº¦å™¨"]
        C3["KVCacheManager<br/>ç¼“å­˜ç®¡ç†"]
        C4["BlockPool<br/>å†…å­˜å—æ± "]
    end

    subgraph execution_layer["æ‰§è¡Œå±‚"]
        D1["ModelExecutor<br/>æ‰§è¡Œå™¨"]
        D2["GPUModelRunner<br/>æ¨¡å‹è¿è¡Œå™¨"]
        D3["Worker<br/>å·¥ä½œè¿›ç¨‹"]
        D4["Attention Backend<br/>æ³¨æ„åŠ›åç«¯"]
    end

    A1 --> B1
    A2 --> B2
    A3 --> B2
    A4 --> B2
    B1 --> B3
    B1 --> B4
    B2 --> B3
    B2 --> B4
    B3 --> C1
    B4 --> C1
    C1 --> C2
    C1 --> D1
    C2 --> C3
    C3 --> C4
    D1 --> D2
    D2 --> D3
    D3 --> D4

    style A1 fill:#e3f2fd
    style A2 fill:#e3f2fd
    style A3 fill:#e3f2fd
    style B1 fill:#fff3e0
    style B2 fill:#fff3e0
    style C1 fill:#e8f5e9
    style C2 fill:#e8f5e9
    style D1 fill:#fce4ec
    style D2 fill:#fce4ec
```

**å„å±‚èŒè´£**ï¼š

| å±‚çº§ | èŒè´£ | å…³é”®ç»„ä»¶ |
|------|------|---------|
| ç”¨æˆ·æ¥å£å±‚ | æä¾›å¤šç§è®¿é—®æ–¹å¼ | LLMã€CLIã€OpenAI API |
| å¼•æ“å±‚ | åè°ƒè¾“å…¥è¾“å‡ºå¤„ç† | LLMEngineã€AsyncLLM |
| æ ¸å¿ƒå±‚ | è°ƒåº¦ä¸å†…å­˜ç®¡ç† | Schedulerã€KVCacheManager |
| æ‰§è¡Œå±‚ | æ¨¡å‹è®¡ç®—ä¸é‡‡æ · | ModelExecutorã€ModelRunner |

### 1.2 ç»„ä»¶äº¤äº’å…³ç³»

è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªæ›´è¯¦ç»†çš„æµç¨‹å›¾å±•ç¤ºç»„ä»¶ä¹‹é—´çš„äº¤äº’ï¼š

```mermaid
flowchart TB
    subgraph user_request["ç”¨æˆ·è¯·æ±‚"]
        U["ç”¨æˆ·"] -->|generate/chat| API["API å…¥å£"]
    end

    subgraph engine_process["å¼•æ“å¤„ç†"]
        API --> IP["InputProcessor<br/>Tokenization<br/>Prompt å¤„ç†"]
        IP --> EC["EngineCore<br/>æ ¸å¿ƒé€»è¾‘"]
        EC --> OP["OutputProcessor<br/>Detokenization<br/>ç»“æœå°è£…"]
        OP --> U
    end

    subgraph core_scheduling["æ ¸å¿ƒè°ƒåº¦"]
        EC <--> SCH["Scheduler<br/>è¯·æ±‚è°ƒåº¦<br/>èµ„æºåˆ†é…"]
        SCH <--> KVM["KVCacheManager<br/>ç¼“å­˜åˆ†é…<br/>å‰ç¼€ç¼“å­˜"]
        KVM <--> BP["BlockPool<br/>å—ç®¡ç†<br/>LRU é©±é€"]
    end

    subgraph model_execution["æ¨¡å‹æ‰§è¡Œ"]
        EC <--> EX["ModelExecutor<br/>æ‰§è¡Œåè°ƒ"]
        EX --> MR["GPUModelRunner<br/>è¾“å…¥å‡†å¤‡<br/>æ¨¡å‹å‰å‘"]
        MR --> W["Worker<br/>GPU è®¡ç®—"]
        W --> ATT["Attention<br/>PagedAttention"]
        W --> SAM["Sampler<br/>Token é‡‡æ ·"]
    end

    style EC fill:#c8e6c9
    style SCH fill:#bbdefb
    style KVM fill:#bbdefb
```

---

## 2. æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 2.1 ç”¨æˆ·æ¥å£å±‚

vLLM æä¾›å¤šç§ä½¿ç”¨æ–¹å¼ï¼Œæ»¡è¶³ä¸åŒåœºæ™¯éœ€æ±‚ã€‚

#### LLM ç±»ï¼ˆPython APIï¼‰

**æ–‡ä»¶ä½ç½®**ï¼š`vllm/entrypoints/llm.py`

è¿™æ˜¯æœ€ç›´æ¥çš„ä½¿ç”¨æ–¹å¼ï¼Œé€‚åˆæ‰¹é‡å¤„ç†åœºæ™¯ï¼š

```python
from vllm import LLM, SamplingParams

llm = LLM(model="meta-llama/Llama-2-7b-hf")

sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    max_tokens=100
)

prompts = ["Hello, my name is", "The capital of France is"]
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(output.outputs[0].text)
```

#### CLI å‘½ä»¤

**æ–‡ä»¶ä½ç½®**ï¼š`vllm/entrypoints/cli/main.py`

é€‚åˆå¿«é€Ÿå¯åŠ¨æœåŠ¡ï¼š

```bash
# å¯åŠ¨ OpenAI å…¼å®¹çš„ API æœåŠ¡
vllm serve meta-llama/Llama-2-7b-hf --port 8000

vllm bench --model meta-llama/Llama-2-7b-hf
```

#### OpenAI å…¼å®¹ API

**æ–‡ä»¶ä½ç½®**ï¼š`vllm/entrypoints/openai/`

æä¾›ä¸ OpenAI API å…¼å®¹çš„ HTTP æ¥å£ï¼š

```python
import openai

client = openai.OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="token-abc123"  # vLLM ä¸éªŒè¯ API key
)

response = client.chat.completions.create(
    model="meta-llama/Llama-2-7b-hf",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

### 2.2 å¼•æ“å±‚

#### LLMEngine

**æ–‡ä»¶ä½ç½®**ï¼š`vllm/v1/engine/llm_engine.py`

LLMEngine æ˜¯åŒæ­¥æ¨¡å¼çš„æ ¸å¿ƒåè°ƒå™¨ï¼š

```mermaid
classDiagram
    class LLMEngine {
        +vllm_config: VllmConfig
        +input_processor: InputProcessor
        +output_processor: OutputProcessor
        +engine_core: EngineCoreClient
        +add_request(request_id, prompt, params)
        +step() EngineCoreOutputs
        +get_output() List~RequestOutput~
    }

    class InputProcessor {
        +tokenizer: Tokenizer
        +process_inputs(prompt) ProcessedInputs
    }

    class OutputProcessor {
        +detokenizer: Detokenizer
        +process_outputs(outputs) List~RequestOutput~
    }

    LLMEngine --> InputProcessor
    LLMEngine --> OutputProcessor
    LLMEngine --> EngineCoreClient
```

**æ ¸å¿ƒèŒè´£**ï¼š
- æ¥æ”¶ç”¨æˆ·è¯·æ±‚ï¼Œé€šè¿‡ InputProcessor å¤„ç†
- å°†è¯·æ±‚å‘é€ç»™ EngineCore æ‰§è¡Œ
- é€šè¿‡ OutputProcessor å¤„ç†è¾“å‡ºç»“æœ

#### AsyncLLM

**æ–‡ä»¶ä½ç½®**ï¼š`vllm/v1/engine/async_llm.py`

AsyncLLM æ˜¯å¼‚æ­¥æ¨¡å¼çš„å¼•æ“ï¼Œæ”¯æŒæµå¼è¾“å‡ºå’Œé«˜å¹¶å‘ï¼š

```python
# AsyncLLM çš„å…¸å‹ä½¿ç”¨åœºæ™¯
async for output in engine.generate(prompt, params):
    # æµå¼è¾“å‡ºæ¯ä¸ª token
    print(output.outputs[0].text, end="", flush=True)
```

### 2.3 æ ¸å¿ƒå±‚

#### EngineCore

**æ–‡ä»¶ä½ç½®**ï¼š`vllm/v1/engine/core.py`

EngineCore æ˜¯æ•´ä¸ªç³»ç»Ÿçš„"å¤§è„‘"ï¼ŒåŒ…å«æ ¸å¿ƒçš„è°ƒåº¦å’Œæ‰§è¡Œé€»è¾‘ï¼š

```mermaid
classDiagram
    class EngineCore {
        +scheduler: Scheduler
        +model_executor: GPUExecutor
        +kv_cache_config: KVCacheConfig
        +step() EngineCoreOutputs
        +add_request(request: Request)
        +abort_requests(request_ids)
    }

    class Scheduler {
        +waiting: RequestQueue
        +running: List~Request~
        +kv_cache_manager: KVCacheManager
        +schedule() SchedulerOutput
        +update_from_output(output)
    }

    class GPUExecutor {
        +model_runner: GPUModelRunner
        +execute_model(scheduler_output)
        +sample_tokens(logits)
    }

    EngineCore --> Scheduler
    EngineCore --> GPUExecutor
```

**EngineCore.step() æ–¹æ³•æ˜¯æ ¸å¿ƒå¾ªç¯**ï¼š

```mermaid
flowchart TD
    A["å¼€å§‹ step"] --> B["Scheduler.schedule<br/>å†³å®šå“ªäº›è¯·æ±‚æ‰§è¡Œ"]
    B --> C{"æœ‰è¯·æ±‚éœ€è¦æ‰§è¡Œ?"}
    C -->|å¦| D["è¿”å›ç©ºè¾“å‡º"]
    C -->|æ˜¯| E["ModelExecutor.execute_model<br/>æ‰§è¡Œå‰å‘ä¼ æ’­"]
    E --> F["è·å– logits"]
    F --> G["Scheduler.get_grammar_bitmask<br/>è·å–è¯­æ³•çº¦æŸ"]
    G --> H["ModelExecutor.sample_tokens<br/>é‡‡æ ·ç”Ÿæˆ token"]
    H --> I["Scheduler.update_from_output<br/>æ›´æ–°è¯·æ±‚çŠ¶æ€"]
    I --> J["æ£€æŸ¥å®Œæˆæ¡ä»¶"]
    J --> K["æ„å»º EngineCoreOutputs"]
    K --> L["è¿”å›è¾“å‡º"]

    style B fill:#bbdefb
    style E fill:#c8e6c9
    style H fill:#fff9c4
```

#### Schedulerï¼ˆè°ƒåº¦å™¨ï¼‰

**æ–‡ä»¶ä½ç½®**ï¼š`vllm/v1/core/sched/scheduler.py`

Scheduler è´Ÿè´£å†³å®šæ¯ä¸ª step æ‰§è¡Œå“ªäº›è¯·æ±‚ï¼š

```mermaid
classDiagram
    class Scheduler {
        +waiting: RequestQueue
        +running: List~Request~
        +kv_cache_manager: KVCacheManager
        +max_num_running_reqs: int
        +max_num_scheduled_tokens: int
        +schedule() SchedulerOutput
        +update_from_output(output, sampled_tokens)
        +add_request(request)
        +finish_requests(request_ids)
    }

    class RequestQueue {
        +queue: Deque~Request~
        +policy: SchedulingPolicy
        +append(request)
        +popleft() Request
        +peek() Request
    }

    class KVCacheManager {
        +allocate_slots(request, num_tokens)
        +free(request)
        +get_computed_blocks(request)
    }

    Scheduler --> RequestQueue
    Scheduler --> KVCacheManager
```

**è°ƒåº¦æµç¨‹ç®€è¿°**ï¼š

1. **å¤„ç† running è¯·æ±‚**ï¼š
   - è®¡ç®—æ¯ä¸ªè¯·æ±‚éœ€è¦çš„æ–° token æ•°
   - å°è¯•åˆ†é… KV Cache
   - å†…å­˜ä¸è¶³æ—¶æ‰§è¡ŒæŠ¢å 

2. **å¤„ç† waiting è¯·æ±‚**ï¼š
   - æŒ‰ä¼˜å…ˆçº§ä»é˜Ÿåˆ—å–å‡ºè¯·æ±‚
   - æ£€æŸ¥èµ„æºæ˜¯å¦è¶³å¤Ÿ
   - åˆ†é…èµ„æºå¹¶ç§»å…¥ running

3. **è¿”å› SchedulerOutput**ï¼š
   - åŒ…å«éœ€è¦æ‰§è¡Œçš„è¯·æ±‚ä¿¡æ¯
   - ä¼ é€’ç»™ ModelExecutor æ‰§è¡Œ

#### KVCacheManagerï¼ˆKV Cache ç®¡ç†å™¨ï¼‰

**æ–‡ä»¶ä½ç½®**ï¼š`vllm/v1/core/kv_cache_manager.py`

KVCacheManager ç®¡ç† KV Cache çš„åˆ†é…å’Œé‡Šæ”¾ï¼š

```mermaid
classDiagram
    class KVCacheManager {
        +coordinator: KVCacheCoordinator
        +block_pool: BlockPool
        +enable_caching: bool
        +get_computed_blocks(request) Tuple
        +allocate_slots(request, num_tokens) List~int~
        +free(request)
    }

    class BlockPool {
        +blocks: List~KVCacheBlock~
        +free_block_queue: FreeKVCacheBlockQueue
        +cached_block_hash_to_block: Dict
        +get_free_block() KVCacheBlock
        +free_block(block)
    }

    class KVCacheBlock {
        +block_id: int
        +ref_cnt: int
        +block_hash: Optional~BlockHash~
    }

    KVCacheManager --> BlockPool
    BlockPool --> KVCacheBlock
```

### 2.4 æ‰§è¡Œå±‚

#### GPUModelRunner

**æ–‡ä»¶ä½ç½®**ï¼š`vllm/v1/worker/gpu_model_runner.py`

GPUModelRunner è´Ÿè´£å‡†å¤‡è¾“å…¥æ•°æ®å¹¶æ‰§è¡Œæ¨¡å‹å‰å‘ä¼ æ’­ï¼š

```mermaid
flowchart TD
    subgraph execute_model["GPUModelRunner.execute_model"]
        A["æ¥æ”¶ SchedulerOutput"] --> B["å‡†å¤‡è¾“å…¥ Tensors<br/>input_ids, positions"]
        B --> C["æ„å»º AttentionMetadata<br/>block_tables, slot_mapping"]
        C --> D["æ¨¡å‹å‰å‘ä¼ æ’­<br/>model.forward"]
        D --> E["è·å– hidden_states"]
        E --> F["LM Head è®¡ç®—<br/>è·å– logits"]
        F --> G["è¿”å› logits"]
    end

    subgraph sample_tokens["GPUModelRunner.sample_tokens"]
        H["æ¥æ”¶ logits"] --> I["åº”ç”¨é‡‡æ ·å‚æ•°<br/>temperature, top_p"]
        I --> J["Sampler.forward<br/>é‡‡æ ·é€»è¾‘"]
        J --> K["è¿”å› sampled_token_ids"]
    end

    G --> H
```

**å…³é”®æ•°æ®ç»“æ„**ï¼š

| æ•°æ® | è¯´æ˜ | æ¥æº |
|------|------|------|
| input_ids | è¾“å…¥ token IDs | SchedulerOutput |
| positions | ä½ç½®ç¼–ç ç´¢å¼• | è®¡ç®—å¾—åˆ° |
| block_tables | å—è¡¨æ˜ å°„ | KVCacheManager |
| slot_mapping | æ§½ä½æ˜ å°„ | KVCacheManager |
| kv_caches | KV Cache å¼ é‡ | GPU æ˜¾å­˜ |

#### Attention Backend

**æ–‡ä»¶ä½ç½®**ï¼š`vllm/v1/attention/backends/`

vLLM æ”¯æŒå¤šç§æ³¨æ„åŠ›å®ç°åç«¯ï¼š

```mermaid
graph TD
    A[Attention Backend æ¥å£] --> B[Flash Attention V2]
    A --> C[Flash Attention V3]
    A --> D[Flash Infer]
    A --> E[XFormers]

    style B fill:#c8e6c9
    style C fill:#c8e6c9
```

**Flash Attention** æ˜¯é»˜è®¤åç«¯ï¼Œæä¾›é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—å’Œ PagedAttention æ”¯æŒã€‚

---

## 3. æ•°æ®æµå®Œæ•´è¿½è¸ª

è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªå…·ä½“çš„ä¾‹å­è¿½è¸ªæ•°æ®åœ¨ç³»ç»Ÿä¸­çš„å®Œæ•´æµç¨‹ï¼š

### 3.1 å®Œæ•´è¯·æ±‚å¤„ç†æ—¶åºå›¾

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant LLM as LLM ç±»
    participant IP as InputProcessor
    participant EC as EngineCore
    participant SCH as Scheduler
    participant KVM as KVCacheManager
    participant EX as ModelExecutor
    participant MR as GPUModelRunner
    participant OP as OutputProcessor

    User->>LLM: generate("Hello, world", params)
    LLM->>IP: process_inputs("Hello, world")
    IP-->>LLM: ProcessedInputs(token_ids=[...])

    LLM->>EC: add_request(request)
    EC->>SCH: add_request(request)
    Note over SCH: è¯·æ±‚åŠ å…¥ waiting é˜Ÿåˆ—

    loop ç›´åˆ°å®Œæˆ
        LLM->>EC: step()

        EC->>SCH: schedule()
        SCH->>KVM: allocate_slots(request, num_tokens)
        KVM-->>SCH: [slot_ids]
        SCH-->>EC: SchedulerOutput

        EC->>EX: execute_model(scheduler_output)
        EX->>MR: execute_model(...)
        MR-->>EX: logits
        EX-->>EC: logits

        EC->>EX: sample_tokens(logits)
        EX->>MR: sample(logits)
        MR-->>EX: sampled_token_ids
        EX-->>EC: sampled_token_ids

        EC->>SCH: update_from_output(output, tokens)
        Note over SCH: æ›´æ–°è¯·æ±‚çŠ¶æ€<br/>æ£€æŸ¥å®Œæˆæ¡ä»¶

        EC-->>LLM: EngineCoreOutputs
    end

    LLM->>OP: process_outputs(outputs)
    OP-->>LLM: RequestOutput

    LLM-->>User: RequestOutput(text="...")
```

### 3.2 æ•°æ®ç»“æ„å˜åŒ–è¿½è¸ª

| é˜¶æ®µ | è¾“å…¥æ•°æ® | è¾“å‡ºæ•°æ® | å¤„ç†ç»„ä»¶ |
|------|---------|---------|---------|
| ç”¨æˆ·è¾“å…¥ | `"Hello, world"` | - | - |
| Tokenization | å­—ç¬¦ä¸² | `token_ids=[15496, 11, 995]` | InputProcessor |
| è¯·æ±‚åˆ›å»º | token_ids | `Request` å¯¹è±¡ | EngineCore |
| è°ƒåº¦ | Request | `SchedulerOutput` | Scheduler |
| ç¼“å­˜åˆ†é… | Request | `slot_mapping, block_tables` | KVCacheManager |
| æ¨¡å‹æ‰§è¡Œ | Tensors | `logits` | GPUModelRunner |
| é‡‡æ · | logits | `token_id=318` | Sampler |
| çŠ¶æ€æ›´æ–° | token_id | æ›´æ–° Request | Scheduler |
| è¾“å‡ºå¤„ç† | token_ids | `"I am..."` | OutputProcessor |

---

## 4. ä»£ç ç›®å½•ç»“æ„è¯¦è§£

### 4.1 ç›®å½•æ ‘æ¦‚è§ˆ

```
vllm/
â”œâ”€â”€ entrypoints/                  # ç”¨æˆ·æ¥å£å±‚
â”‚   â”œâ”€â”€ llm.py                    # LLM ç±»ï¼ˆPython APIï¼‰
â”‚   â”œâ”€â”€ cli/                      # CLI å‘½ä»¤
â”‚   â”‚   â””â”€â”€ main.py               # CLI å…¥å£
â”‚   â”œâ”€â”€ openai/                   # OpenAI å…¼å®¹ API
â”‚   â”‚   â”œâ”€â”€ api_server.py         # HTTP æœåŠ¡å™¨
â”‚   â”‚   â””â”€â”€ serving_*.py          # å„ç§ serving å®ç°
â”‚   â””â”€â”€ serve/                    # serve ç›¸å…³
â”‚
â”œâ”€â”€ v1/                           # V1 æ¶æ„ï¼ˆæ–°ç‰ˆæœ¬ï¼‰
â”‚   â”œâ”€â”€ engine/                   # å¼•æ“å±‚
â”‚   â”‚   â”œâ”€â”€ llm_engine.py         # LLMEngine
â”‚   â”‚   â”œâ”€â”€ async_llm.py          # AsyncLLM
â”‚   â”‚   â”œâ”€â”€ core.py               # EngineCore
â”‚   â”‚   â”œâ”€â”€ core_client.py        # æ ¸å¿ƒå®¢æˆ·ç«¯
â”‚   â”‚   â”œâ”€â”€ input_processor.py    # è¾“å…¥å¤„ç†
â”‚   â”‚   â”œâ”€â”€ output_processor.py   # è¾“å‡ºå¤„ç†
â”‚   â”‚   â””â”€â”€ detokenizer.py        # è§£ç å™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                     # æ ¸å¿ƒå±‚
â”‚   â”‚   â”œâ”€â”€ sched/                # è°ƒåº¦ç›¸å…³
â”‚   â”‚   â”‚   â”œâ”€â”€ scheduler.py      # Scheduler
â”‚   â”‚   â”‚   â”œâ”€â”€ request_queue.py  # è¯·æ±‚é˜Ÿåˆ—
â”‚   â”‚   â”‚   â””â”€â”€ output.py         # è°ƒåº¦è¾“å‡º
â”‚   â”‚   â”œâ”€â”€ kv_cache_manager.py   # KV Cache ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ block_pool.py         # å†…å­˜å—æ± 
â”‚   â”‚   â””â”€â”€ kv_cache_utils.py     # ç¼“å­˜å·¥å…·
â”‚   â”‚
â”‚   â”œâ”€â”€ worker/                   # æ‰§è¡Œå±‚
â”‚   â”‚   â”œâ”€â”€ gpu_model_runner.py   # GPU æ¨¡å‹è¿è¡Œå™¨
â”‚   â”‚   â”œâ”€â”€ gpu_worker.py         # GPU å·¥ä½œè¿›ç¨‹
â”‚   â”‚   â””â”€â”€ block_table.py        # å—è¡¨ç®¡ç†
â”‚   â”‚
â”‚   â”œâ”€â”€ attention/                # æ³¨æ„åŠ›å®ç°
â”‚   â”‚   â”œâ”€â”€ backends/             # åç«¯å®ç°
â”‚   â”‚   â”‚   â””â”€â”€ flash_attn.py     # Flash Attention
â”‚   â”‚   â””â”€â”€ ops/                  # åº•å±‚æ“ä½œ
â”‚   â”‚       â””â”€â”€ paged_attn.py     # PagedAttention
â”‚   â”‚
â”‚   â”œâ”€â”€ sample/                   # é‡‡æ ·
â”‚   â”‚   â””â”€â”€ sampler.py            # Sampler
â”‚   â”‚
â”‚   â”œâ”€â”€ request.py                # Request æ•°æ®ç»“æ„
â”‚   â””â”€â”€ outputs.py                # è¾“å‡ºæ•°æ®ç»“æ„
â”‚
â”œâ”€â”€ config/                       # é…ç½®
â”‚   â””â”€â”€ vllm.py                   # VllmConfig
â”‚
â”œâ”€â”€ model_executor/               # æ¨¡å‹æ‰§è¡Œå™¨
â”‚   â”œâ”€â”€ models/                   # æ¨¡å‹å®ç°
â”‚   â””â”€â”€ layers/                   # å±‚å®ç°
â”‚
â”œâ”€â”€ sampling_params.py            # SamplingParams
â”‚
â””â”€â”€ csrc/                         # C++/CUDA ä»£ç 
    â””â”€â”€ attention/                # æ³¨æ„åŠ› CUDA å†…æ ¸
        â”œâ”€â”€ paged_attention_v1.cu
        â””â”€â”€ paged_attention_v2.cu
```

### 4.2 å…³é”®æ–‡ä»¶ç´¢å¼•

| åŠŸèƒ½ç±»åˆ« | æ–‡ä»¶è·¯å¾„ | å…³é”®ç±»/å‡½æ•° |
|---------|---------|------------|
| **å…¥å£** | | |
| Python API | `vllm/entrypoints/llm.py` | `LLM`, `generate()` |
| CLI | `vllm/entrypoints/cli/main.py` | `main()` |
| **å¼•æ“** | | |
| åŒæ­¥å¼•æ“ | `vllm/v1/engine/llm_engine.py` | `LLMEngine` |
| å¼‚æ­¥å¼•æ“ | `vllm/v1/engine/async_llm.py` | `AsyncLLM` |
| æ ¸å¿ƒé€»è¾‘ | `vllm/v1/engine/core.py` | `EngineCore`, `step()` |
| **è°ƒåº¦** | | |
| è°ƒåº¦å™¨ | `vllm/v1/core/sched/scheduler.py` | `Scheduler`, `schedule()` |
| è¯·æ±‚é˜Ÿåˆ— | `vllm/v1/core/sched/request_queue.py` | `RequestQueue` |
| **å†…å­˜ç®¡ç†** | | |
| KV Cache | `vllm/v1/core/kv_cache_manager.py` | `KVCacheManager` |
| å—æ±  | `vllm/v1/core/block_pool.py` | `BlockPool` |
| **æ‰§è¡Œ** | | |
| æ¨¡å‹è¿è¡Œ | `vllm/v1/worker/gpu_model_runner.py` | `GPUModelRunner` |
| Worker | `vllm/v1/worker/gpu_worker.py` | `GPUWorker` |
| **æ³¨æ„åŠ›** | | |
| PagedAttention | `vllm/v1/attention/ops/paged_attn.py` | `PagedAttention` |
| Flash Attention | `vllm/v1/attention/backends/flash_attn.py` | `FlashAttentionBackend` |
| **æ•°æ®ç»“æ„** | | |
| è¯·æ±‚ | `vllm/v1/request.py` | `Request`, `RequestStatus` |
| é‡‡æ ·å‚æ•° | `vllm/sampling_params.py` | `SamplingParams` |

---

## 5. é…ç½®ç³»ç»Ÿ

### 5.1 VllmConfig

vLLM ä½¿ç”¨ç»Ÿä¸€çš„é…ç½®ç³»ç»Ÿï¼Œä¸»è¦é…ç½®åŒ…æ‹¬ï¼š

```mermaid
classDiagram
    class VllmConfig {
        +model_config: ModelConfig
        +cache_config: CacheConfig
        +parallel_config: ParallelConfig
        +scheduler_config: SchedulerConfig
        +speculative_config: SpeculativeConfig
    }

    class ModelConfig {
        +model: str
        +dtype: str
        +max_model_len: int
    }

    class CacheConfig {
        +block_size: int
        +num_gpu_blocks: int
        +enable_prefix_caching: bool
    }

    class SchedulerConfig {
        +max_num_seqs: int
        +max_num_batched_tokens: int
    }

    VllmConfig --> ModelConfig
    VllmConfig --> CacheConfig
    VllmConfig --> SchedulerConfig
```

### 5.2 å¸¸ç”¨é…ç½®å‚æ•°

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `--model` | æ¨¡å‹è·¯å¾„æˆ–åç§° | å¿…å¡« |
| `--dtype` | æ•°æ®ç²¾åº¦ | auto |
| `--max-model-len` | æœ€å¤§åºåˆ—é•¿åº¦ | æ¨¡å‹é»˜è®¤ |
| `--gpu-memory-utilization` | GPU æ˜¾å­˜åˆ©ç”¨ç‡ | 0.9 |
| `--max-num-seqs` | æœ€å¤§å¹¶å‘è¯·æ±‚æ•° | 256 |
| `--block-size` | KV Cache å—å¤§å° | 16 |
| `--enable-prefix-caching` | å¯ç”¨å‰ç¼€ç¼“å­˜ | False |
| `--tensor-parallel-size` | å¼ é‡å¹¶è¡Œå¤§å° | 1 |

---

## 6. V1 vs æ—§ç‰ˆæ¶æ„

vLLM å½“å‰ä¸»è¦ä½¿ç”¨ V1 æ¶æ„ï¼Œç›¸æ¯”æ—§ç‰ˆæœ‰ä»¥ä¸‹æ”¹è¿›ï¼š

| ç‰¹æ€§ | æ—§ç‰ˆ | V1 |
|------|------|-----|
| è°ƒåº¦å™¨ | BlockSpaceManager | KVCacheManager |
| æ‰§è¡Œæµç¨‹ | åŒæ­¥ä¸ºä¸» | å¼‚æ­¥ä¼˜åŒ– |
| å†…å­˜ç®¡ç† | åŸºç¡€ PagedAttention | æ›´ç»†ç²’åº¦çš„å—ç®¡ç† |
| å‰ç¼€ç¼“å­˜ | æœ‰é™æ”¯æŒ | å®Œæ•´æ”¯æŒ |
| ä»£ç ç»„ç»‡ | åˆ†æ•£ | æ¨¡å—åŒ– |

æœ¬æ–‡æ¡£ç³»åˆ—ä¸»è¦åŸºäº **V1 æ¶æ„**è¿›è¡Œè®²è§£ã€‚

---

## 7. æœ¬ç« å°ç»“

### æ¶æ„å±‚æ¬¡

1. **ç”¨æˆ·æ¥å£å±‚**ï¼šæä¾› Python APIã€CLIã€OpenAI API ç­‰å¤šç§è®¿é—®æ–¹å¼
2. **å¼•æ“å±‚**ï¼šLLMEngine/AsyncLLM åè°ƒè¾“å…¥è¾“å‡ºå¤„ç†
3. **æ ¸å¿ƒå±‚**ï¼šScheduler å’Œ KVCacheManager è´Ÿè´£è°ƒåº¦å’Œå†…å­˜ç®¡ç†
4. **æ‰§è¡Œå±‚**ï¼šGPUModelRunner æ‰§è¡Œæ¨¡å‹è®¡ç®—

### å…³é”®ç»„ä»¶

- **EngineCore**ï¼šç³»ç»Ÿ"å¤§è„‘"ï¼ŒåŒ…å« step() æ ¸å¿ƒå¾ªç¯
- **Scheduler**ï¼šå†³å®šå“ªäº›è¯·æ±‚åœ¨æ¯ä¸ª step æ‰§è¡Œ
- **KVCacheManager**ï¼šç®¡ç† KV Cache çš„åˆ†é…å’Œé‡Šæ”¾
- **GPUModelRunner**ï¼šå‡†å¤‡è¾“å…¥å¹¶æ‰§è¡Œæ¨¡å‹å‰å‘ä¼ æ’­

### æ•°æ®æµç¨‹

```
ç”¨æˆ·è¾“å…¥ â†’ Tokenization â†’ è¯·æ±‚è°ƒåº¦ â†’ ç¼“å­˜åˆ†é…
    â†’ æ¨¡å‹æ‰§è¡Œ â†’ é‡‡æ · â†’ çŠ¶æ€æ›´æ–° â†’ Detokenization â†’ ç”¨æˆ·è¾“å‡º
```

### ä»£ç å®šä½

- å…¥å£ï¼š`vllm/entrypoints/`
- å¼•æ“ï¼š`vllm/v1/engine/`
- è°ƒåº¦ï¼š`vllm/v1/core/sched/`
- æ‰§è¡Œï¼š`vllm/v1/worker/`
- æ³¨æ„åŠ›ï¼š`vllm/v1/attention/`

---

## æ€è€ƒé¢˜

1. ä¸ºä»€ä¹ˆ vLLM è¦å°† EngineCore å’Œ LLMEngine åˆ†å¼€è®¾è®¡ï¼Ÿ
2. Scheduler å’Œ KVCacheManager ä¹‹é—´æ˜¯å¦‚ä½•åä½œçš„ï¼Ÿ
3. å¦‚æœä½ è¦æ·»åŠ ä¸€ä¸ªæ–°çš„ç”¨æˆ·æ¥å£ï¼ˆæ¯”å¦‚ WebSocketï¼‰ï¼Œéœ€è¦ä¿®æ”¹å“ªäº›ç»„ä»¶ï¼Ÿ

---

## ä¸‹ä¸€æ­¥

æ¶æ„æ¦‚è§ˆå·²ç»å®Œæˆï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†è¿›å…¥æ·±åº¦å­¦ä¹ åŸºç¡€éƒ¨åˆ†ï¼Œä¸ºç†è§£æ ¸å¿ƒç®—æ³•æ‰“ä¸‹ç†è®ºåŸºç¡€ï¼š

ğŸ‘‰ [ä¸‹ä¸€ç« ï¼šç¥ç»ç½‘ç»œåŸºç¡€](../02-dl-fundamentals/01-neural-network-basics.md)

---

## é™„ï¼šå¿«é€Ÿå‚è€ƒå¡ç‰‡

### è¯·æ±‚å¤„ç†æµç¨‹

```
User â†’ LLM.generate() â†’ InputProcessor â†’ EngineCore
     â†’ Scheduler.schedule() â†’ KVCacheManager.allocate_slots()
     â†’ GPUModelRunner.execute_model() â†’ Sampler
     â†’ Scheduler.update_from_output() â†’ OutputProcessor â†’ User
```

### æ ¸å¿ƒæ–‡ä»¶é€ŸæŸ¥

```
è°ƒåº¦é€»è¾‘    â†’ vllm/v1/core/sched/scheduler.py
ç¼“å­˜ç®¡ç†    â†’ vllm/v1/core/kv_cache_manager.py
æ¨¡å‹æ‰§è¡Œ    â†’ vllm/v1/worker/gpu_model_runner.py
æ ¸å¿ƒå¾ªç¯    â†’ vllm/v1/engine/core.py
```
