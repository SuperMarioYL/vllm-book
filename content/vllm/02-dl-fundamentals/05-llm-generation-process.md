---
title: "LLM ç”Ÿæˆè¿‡ç¨‹"
weight: 5
---


> æœ¬ç« å°†è¯¦ç»†ä»‹ç» LLM æ–‡æœ¬ç”Ÿæˆçš„å®Œæ•´æµç¨‹ï¼ŒåŒ…æ‹¬ Prefillã€Decode ä¸¤ä¸ªé˜¶æ®µä»¥åŠå„ç§é‡‡æ ·ç­–ç•¥ã€‚

---

## å¼•è¨€

LLM ç”Ÿæˆæ–‡æœ¬æ˜¯ä¸€ä¸ªå¤æ‚çš„è¿‡ç¨‹ï¼Œæ¶‰åŠ tokenizationã€æ¨¡å‹å‰å‘ä¼ æ’­ã€é‡‡æ ·ç­‰å¤šä¸ªç¯èŠ‚ã€‚ç†è§£è¿™ä¸ªè¿‡ç¨‹å¯¹äºç†è§£ vLLM çš„ä¼˜åŒ–ç­–ç•¥è‡³å…³é‡è¦ã€‚

---

## 1. ç”Ÿæˆæµç¨‹æ¦‚è§ˆ

### 1.1 å®Œæ•´æµç¨‹å›¾

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant Tok as Tokenizer
    participant Model as LLM
    participant Sampler as é‡‡æ ·å™¨
    participant DeTok as Detokenizer

    User->>Tok: "Hello, world"
    Tok->>Model: [15496, 11, 995]

    rect rgb(200, 230, 200)
        Note over Model: Prefill é˜¶æ®µ
        Model->>Model: å¤„ç†æ‰€æœ‰è¾“å…¥ tokens
        Model->>Model: åˆå§‹åŒ– KV Cache
        Model->>Sampler: logits
        Sampler->>Model: ç¬¬ä¸€ä¸ªè¾“å‡º token
    end

    rect rgb(200, 200, 230)
        Note over Model: Decode é˜¶æ®µ
        loop ç›´åˆ°åœæ­¢æ¡ä»¶
            Model->>Model: å¤„ç† 1 ä¸ªæ–° token
            Model->>Model: æ›´æ–° KV Cache
            Model->>Sampler: logits
            Sampler->>Model: ä¸‹ä¸€ä¸ª token
        end
    end

    Model->>DeTok: [318, 716, 257, ...]
    DeTok->>User: "I am a language model..."
```

### 1.2 ä¸¤é˜¶æ®µæ¨¡å‹

LLM ç”Ÿæˆåˆ†ä¸ºä¸¤ä¸ªæˆªç„¶ä¸åŒçš„é˜¶æ®µï¼š

| é˜¶æ®µ | Prefillï¼ˆé¢„å¡«å……ï¼‰ | Decodeï¼ˆè§£ç ï¼‰ |
|------|------------------|----------------|
| å¤„ç†å†…å®¹ | æ•´ä¸ªè¾“å…¥ prompt | æ–°ç”Ÿæˆçš„ token |
| æ¯æ¬¡å¤„ç† | N ä¸ª tokens | 1 ä¸ª token |
| KV Cache | åˆå§‹åŒ– | å¢é‡æ›´æ–° |
| è®¡ç®—ç‰¹æ€§ | è®¡ç®—å¯†é›†å‹ | å†…å­˜å¯†é›†å‹ |
| GPU åˆ©ç”¨ç‡ | é«˜ | ä½ |

---

## 2. Prefill é˜¶æ®µè¯¦è§£

### 2.1 è¾“å…¥å¤„ç†ï¼šTokenization

ç¬¬ä¸€æ­¥æ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸º token IDsï¼š

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

text = "Hello, how are you?"
tokens = tokenizer.encode(text)
print(tokens)  # [1, 15043, 29892, 920, 526, 366, 29973]
print(tokenizer.convert_ids_to_tokens(tokens))
```

### 2.2 å¹¶è¡Œè®¡ç®—æ‰€æœ‰ Token

åœ¨ Prefill é˜¶æ®µï¼Œæ‰€æœ‰è¾“å…¥ token å¯ä»¥å¹¶è¡Œå¤„ç†ï¼š

```mermaid
flowchart TD
    subgraph prefill_parallel["Prefill å¹¶è¡Œå¤„ç†"]
        I["è¾“å…¥: token_ids<br/>[1, 15043, 29892, 920, 526, 366]"]
        E["Embedding Layer<br/>å¹¶è¡ŒæŸ¥è¡¨"]
        PE["Position Encoding<br/>æ·»åŠ ä½ç½®ä¿¡æ¯"]

        subgraph transformer_layers["Transformer Layers"]
            L1[Layer 1]
            L2[Layer 2]
            LN[Layer N]
        end

        LH[LM Head]
        O["Logits<br/>[seq_len, vocab_size]"]

        I --> E --> PE --> L1 --> L2 --> LN --> LH --> O
    end

    style E fill:#e3f2fd
    style L1 fill:#c8e6c9
    style L2 fill:#c8e6c9
    style LN fill:#c8e6c9
```

### 2.3 KV Cache åˆå§‹åŒ–ä¸å¡«å……

Prefill æœŸé—´ï¼Œè®¡ç®—å¹¶å­˜å‚¨æ‰€æœ‰è¾“å…¥ token çš„ Kã€Vï¼š

```python
def prefill(model, input_ids, kv_cache):
    """
    input_ids: [batch_size, seq_len]
    """
    batch_size, seq_len = input_ids.shape

    # Embedding
    hidden_states = model.embed_tokens(input_ids)  # [batch, seq, hidden]

    # éå†æ¯ä¸€å±‚
    for layer_idx, layer in enumerate(model.layers):
        # è®¡ç®— Q, K, V
        q = layer.q_proj(hidden_states)
        k = layer.k_proj(hidden_states)
        v = layer.v_proj(hidden_states)

        # å­˜å…¥ KV Cache
        kv_cache.update(layer_idx, k, v)

        # è‡ªæ³¨æ„åŠ›è®¡ç®—
        # ... (ä½¿ç”¨å®Œæ•´çš„ K, Vï¼Œåº”ç”¨å› æœæ©ç )

        # FFN
        # ...

    # LM Head
    logits = model.lm_head(hidden_states)

    # åªè¿”å›æœ€åä¸€ä¸ªä½ç½®çš„ logitsï¼ˆç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ª tokenï¼‰
    return logits[:, -1, :]  # [batch, vocab_size]
```

### 2.4 ç”Ÿæˆç¬¬ä¸€ä¸ª Token

ä½¿ç”¨æœ€åä¸€ä¸ªä½ç½®çš„ logits ç”Ÿæˆç¬¬ä¸€ä¸ªè¾“å‡º tokenï¼š

```python
def generate_first_token(logits, sampling_params):
    """
    logits: [batch_size, vocab_size]
    """
    # åº”ç”¨é‡‡æ ·ç­–ç•¥
    next_token = sample(logits, sampling_params)  # [batch_size, 1]
    return next_token
```

---

## 3. Decode é˜¶æ®µè¯¦è§£

### 3.1 å• Token å¢é‡è®¡ç®—

Decode é˜¶æ®µæ¯æ¬¡åªå¤„ç†ä¸€ä¸ªæ–° tokenï¼š

```mermaid
flowchart LR
    subgraph decode_incremental["Decode å¢é‡è®¡ç®—"]
        NT["æ–° token"]
        E[Embedding]
        Q["è®¡ç®— Q_new"]
        KV["è®¡ç®— K_new, V_new"]
        Cache[("è¯»å– KV Cache")]
        ATT["Attention<br/>Q_new x [K_cache; K_new]"]
        Update["æ›´æ–° KV Cache"]
        FFN[FFN]
        LM[LM Head]
        O[Logits]

        NT --> E --> Q
        E --> KV
        Cache --> ATT
        KV --> ATT
        Q --> ATT
        ATT --> FFN --> LM --> O
        KV --> Update --> Cache
    end
```

### 3.2 å¦‚ä½•åˆ©ç”¨ KV Cache

```python
def decode_step(model, new_token_id, kv_cache, position):
    """
    new_token_id: [batch_size, 1]
    position: å½“å‰ä½ç½®ç´¢å¼•
    """
    # Embedding
    hidden_states = model.embed_tokens(new_token_id)  # [batch, 1, hidden]

    # éå†æ¯ä¸€å±‚
    for layer_idx, layer in enumerate(model.layers):
        # åªè®¡ç®—æ–° token çš„ Q, K, V
        q_new = layer.q_proj(hidden_states)  # [batch, 1, hidden]
        k_new = layer.k_proj(hidden_states)
        v_new = layer.v_proj(hidden_states)

        # ä»ç¼“å­˜è·å–å†å² K, V
        k_cache, v_cache = kv_cache.get(layer_idx)

        # åˆå¹¶ï¼š[k_cache, k_new] å’Œ [v_cache, v_new]
        k_full = torch.cat([k_cache, k_new], dim=2)
        v_full = torch.cat([v_cache, v_new], dim=2)

        # æ›´æ–°ç¼“å­˜
        kv_cache.update(layer_idx, k_new, v_new)

        # æ³¨æ„åŠ›è®¡ç®—ï¼šQ_new (1ä¸ª) ä¸ K_full (N+1ä¸ª)
        # scores: [batch, heads, 1, N+1]
        scores = (q_new @ k_full.transpose(-2, -1)) / sqrt(head_dim)

        # æ— éœ€å› æœæ©ç ï¼ˆæ–° token å¯ä»¥çœ‹åˆ°æ‰€æœ‰å†å²ï¼‰
        attn_weights = F.softmax(scores, dim=-1)

        # åŠ æƒæ±‚å’Œ
        attn_output = attn_weights @ v_full  # [batch, heads, 1, head_dim]

        # ... FFN ç­‰

    # LM Head
    logits = model.lm_head(hidden_states)  # [batch, 1, vocab_size]

    return logits.squeeze(1)  # [batch, vocab_size]
```

### 3.3 Decode å¾ªç¯

```python
def decode_loop(model, first_token, kv_cache, max_tokens, stop_token_id):
    """å®Œæ•´çš„ decode å¾ªç¯"""
    generated_tokens = [first_token]
    current_token = first_token
    position = kv_cache.current_len

    for step in range(max_tokens):
        # æ‰§è¡Œä¸€æ­¥ decode
        logits = decode_step(model, current_token, kv_cache, position)

        # é‡‡æ ·ä¸‹ä¸€ä¸ª token
        next_token = sample(logits, sampling_params)

        # æ£€æŸ¥åœæ­¢æ¡ä»¶
        if next_token == stop_token_id:
            break

        generated_tokens.append(next_token)
        current_token = next_token
        position += 1

    return generated_tokens
```

---

## 4. é‡‡æ ·ç­–ç•¥è¯¦è§£

### 4.1 ä» Logits åˆ°æ¦‚ç‡åˆ†å¸ƒ

æ¨¡å‹è¾“å‡ºçš„æ˜¯ logitsï¼ˆæœªå½’ä¸€åŒ–çš„åˆ†æ•°ï¼‰ï¼Œéœ€è¦è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼š

```python
# logits: [vocab_size]

probs = F.softmax(logits, dim=-1)
```

### 4.2 Greedy Decodingï¼ˆè´ªå©ªè§£ç ï¼‰

æœ€ç®€å•çš„ç­–ç•¥ï¼šæ¯æ¬¡é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ tokenã€‚

```python
def greedy_decode(logits):
    return torch.argmax(logits, dim=-1)
```

**ç‰¹ç‚¹**ï¼š
- ç¡®å®šæ€§ï¼ˆç›¸åŒè¾“å…¥æ€»æ˜¯ç›¸åŒè¾“å‡ºï¼‰
- å¯èƒ½é™·å…¥é‡å¤
- ä¸é€‚åˆåˆ›æ„ç”Ÿæˆ

### 4.3 Temperatureï¼ˆæ¸©åº¦ï¼‰

Temperature æ§åˆ¶æ¦‚ç‡åˆ†å¸ƒçš„"å°–é”"ç¨‹åº¦ï¼š

```python
def apply_temperature(logits, temperature):
    return logits / temperature
```

```mermaid
graph LR
    subgraph temperature_effect["Temperature æ•ˆæœ"]
        T1["T=0.1<br/>éå¸¸å°–é”<br/>å‡ ä¹æ˜¯ Greedy"]
        T2["T=1.0<br/>åŸå§‹åˆ†å¸ƒ"]
        T3["T=2.0<br/>æ›´å¹³æ»‘<br/>æ›´éšæœº"]
    end
```

| Temperature | æ•ˆæœ | é€‚ç”¨åœºæ™¯ |
|-------------|------|---------|
| < 1.0 | æ›´ç¡®å®šï¼Œåå‘é«˜æ¦‚ç‡ | äº‹å®æ€§å›ç­” |
| = 1.0 | åŸå§‹åˆ†å¸ƒ | ä¸€èˆ¬åœºæ™¯ |
| > 1.0 | æ›´éšæœºï¼Œæ›´å¤šæ · | åˆ›æ„å†™ä½œ |

### 4.4 Top-k Sampling

åªä»æ¦‚ç‡æœ€é«˜çš„ k ä¸ª token ä¸­é‡‡æ ·ï¼š

```python
def top_k_sampling(logits, k):
    # æ‰¾åˆ° top-k çš„å€¼å’Œç´¢å¼•
    top_k_logits, top_k_indices = torch.topk(logits, k)

    # å°†å…¶ä»–ä½ç½®è®¾ä¸º -inf
    filtered_logits = torch.full_like(logits, float('-inf'))
    filtered_logits.scatter_(-1, top_k_indices, top_k_logits)

    # é‡æ–°è®¡ç®—æ¦‚ç‡å¹¶é‡‡æ ·
    probs = F.softmax(filtered_logits, dim=-1)
    return torch.multinomial(probs, num_samples=1)
```

**ç¤ºä¾‹**ï¼ˆk=3ï¼‰ï¼š

```
åŸå§‹æ¦‚ç‡: [0.40, 0.30, 0.15, 0.10, 0.05]
Top-3:    [0.40, 0.30, 0.15, 0.00, 0.00]
å½’ä¸€åŒ–å: [0.47, 0.35, 0.18, 0.00, 0.00]
```

### 4.5 Top-p (Nucleus) Sampling

é€‰æ‹©ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° p çš„æœ€å° token é›†åˆï¼š

```python
def top_p_sampling(logits, p):
    # æ’åº
    sorted_logits, sorted_indices = torch.sort(logits, descending=True)
    probs = F.softmax(sorted_logits, dim=-1)

    # è®¡ç®—ç´¯ç§¯æ¦‚ç‡
    cumsum_probs = torch.cumsum(probs, dim=-1)

    # æ‰¾åˆ°ç´¯ç§¯æ¦‚ç‡ > p çš„ä½ç½®
    sorted_indices_to_remove = cumsum_probs > p
    # ä¿ç•™ç¬¬ä¸€ä¸ªè¶…è¿‡é˜ˆå€¼çš„
    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
    sorted_indices_to_remove[..., 0] = False

    # è¿‡æ»¤
    sorted_logits[sorted_indices_to_remove] = float('-inf')

    # é‡‡æ ·
    probs = F.softmax(sorted_logits, dim=-1)
    return torch.multinomial(probs, num_samples=1)
```

**ç¤ºä¾‹**ï¼ˆp=0.9ï¼‰ï¼š

```
æ’åºåæ¦‚ç‡: [0.40, 0.30, 0.15, 0.10, 0.05]
ç´¯ç§¯æ¦‚ç‡:   [0.40, 0.70, 0.85, 0.95, 1.00]
                                â†‘ è¶…è¿‡ 0.9
ä¿ç•™:       [0.40, 0.30, 0.15, 0.10]  ç´¯ç§¯ = 0.95
```

### 4.6 é‡‡æ ·ç­–ç•¥å¯¹æ¯”

```mermaid
graph TD
    subgraph sampling_strategy["é‡‡æ ·ç­–ç•¥é€‰æ‹©"]
        G["Greedy<br/>ç¡®å®šæ€§ã€å¯èƒ½é‡å¤"]
        TK["Top-k<br/>å›ºå®šæ•°é‡çš„å€™é€‰"]
        TP["Top-p<br/>åŠ¨æ€æ•°é‡çš„å€™é€‰"]
        T["Temperature<br/>æ§åˆ¶éšæœºç¨‹åº¦"]

        G --> |é€‚åˆ| F["äº‹å®é—®ç­”"]
        TK --> |é€‚åˆ| C1["é€šç”¨å¯¹è¯"]
        TP --> |é€‚åˆ| C2["åˆ›æ„å†™ä½œ"]
        T --> |é…åˆ| TK
        T --> |é…åˆ| TP
    end
```

### 4.7 å¸¸ç”¨å‚æ•°ç»„åˆ

| åœºæ™¯ | Temperature | Top-p | Top-k |
|------|------------|-------|-------|
| ä»£ç ç”Ÿæˆ | 0.1-0.3 | - | - |
| äº‹å®é—®ç­” | 0.0-0.5 | 0.9 | - |
| é€šç”¨å¯¹è¯ | 0.7-0.9 | 0.9 | 40 |
| åˆ›æ„å†™ä½œ | 1.0-1.2 | 0.95 | 50 |
| è„‘æš´åˆ›æ„ | 1.5-2.0 | 0.98 | 100 |

---

## 5. åœæ­¢æ¡ä»¶

### 5.1 å¸¸è§åœæ­¢æ¡ä»¶

```python
def check_stop_condition(token_id, generated_tokens, params):
    # 1. ç”Ÿæˆäº† EOS token
    if token_id == params.eos_token_id:
        return True, "EOS"

    # 2. è¾¾åˆ°æœ€å¤§é•¿åº¦
    if len(generated_tokens) >= params.max_tokens:
        return True, "MAX_LENGTH"

    # 3. é‡åˆ°åœæ­¢å­—ç¬¦ä¸²
    text = tokenizer.decode(generated_tokens)
    for stop_str in params.stop_strings:
        if stop_str in text:
            return True, "STOP_STRING"

    return False, None
```

### 5.2 vLLM ä¸­çš„åœæ­¢æ¡ä»¶

```python
# vllm/sampling_params.py
class SamplingParams:
    max_tokens: int = 16           # æœ€å¤§ç”Ÿæˆ token æ•°
    stop: List[str] = []           # åœæ­¢å­—ç¬¦ä¸²
    stop_token_ids: List[int] = [] # åœæ­¢ token ID
    include_stop_str_in_output: bool = False
    ignore_eos: bool = False       # æ˜¯å¦å¿½ç•¥ EOS
```

---

## 6. è®¡ç®—ç‰¹æ€§å¯¹æ¯”

### 6.1 Prefill vs Decode

```mermaid
graph LR
    subgraph prefill_stage["Prefill"]
        P1["å¤„ç† N ä¸ª tokens"]
        P2["è®¡ç®—é‡: O(NÂ² Ã— d)"]
        P3["å†…å­˜è®¿é—®: O(N Ã— d)"]
        P4["è®¡ç®—å¯†åº¦: é«˜"]
    end

    subgraph decode_stage["Decode"]
        D1["å¤„ç† 1 ä¸ª token"]
        D2["è®¡ç®—é‡: O(N Ã— d)"]
        D3["å†…å­˜è®¿é—®: O(N Ã— d)"]
        D4["è®¡ç®—å¯†åº¦: ä½"]
    end
```

| ç‰¹æ€§ | Prefill | Decode |
|------|---------|--------|
| æ¯æ¬¡å¤„ç† tokens | N | 1 |
| Attention è®¡ç®— | Q[N] Ã— K[N]áµ€ | Q[1] Ã— K[N]áµ€ |
| è®¡ç®—é‡ | O(NÂ²d) | O(Nd) |
| å†…å­˜è¯»å– | æ¨¡å‹æƒé‡ | æ¨¡å‹æƒé‡ + KV Cache |
| è®¡ç®—/è®¿å­˜æ¯” | é«˜ | ä½ |
| GPU åˆ©ç”¨ç‡ | 50-80% | 10-30% |
| ç“¶é¢ˆ | è®¡ç®— | å†…å­˜å¸¦å®½ |

### 6.2 GPU åˆ©ç”¨ç‡å¯è§†åŒ–

```mermaid
gantt
    title GPU åˆ©ç”¨ç‡æ—¶é—´çº¿
    dateFormat X
    axisFormat %s

    section GPU è®¡ç®—
    Prefill (é«˜åˆ©ç”¨ç‡) :done, p, 0, 20
    Decode Step 1 (ä½åˆ©ç”¨ç‡) :crit, d1, 20, 25
    Decode Step 2 (ä½åˆ©ç”¨ç‡) :crit, d2, 25, 30
    Decode Step 3 (ä½åˆ©ç”¨ç‡) :crit, d3, 30, 35
    ...æ›´å¤š decode steps :crit, dn, 35, 80
```

### 6.3 æ‰¹å¤„ç†çš„é‡è¦æ€§

å•ç‹¬å¤„ç†ä¸€ä¸ª decode step æ—¶ï¼ŒGPU å¤§éƒ¨åˆ†æ—¶é—´åœ¨ç­‰å¾…æ•°æ®ä¼ è¾“ã€‚é€šè¿‡æ‰¹å¤„ç†å¤šä¸ªè¯·æ±‚ï¼Œå¯ä»¥æé«˜ GPU åˆ©ç”¨ç‡ï¼š

```python
# å•è¯·æ±‚
def decode_single(request):
    read_weights()      # 14GB
    process_1_token()   # å¾ˆå°çš„è®¡ç®—é‡
    # GPU å¤§éƒ¨åˆ†æ—¶é—´ç©ºé—²

def decode_batch(requests, batch_size=32):
    read_weights()      # 14GBï¼ˆåªè¯»ä¸€æ¬¡ï¼‰
    process_32_tokens() # 32 å€çš„è®¡ç®—é‡
    # GPU åˆ©ç”¨ç‡æé«˜ 32 å€
```

---

## 7. å®Œæ•´ç”Ÿæˆç¤ºä¾‹

### 7.1 ä»£ç ç¤ºä¾‹

```python
def generate(model, tokenizer, prompt, max_tokens=100, temperature=0.8, top_p=0.9):
    # 1. Tokenization
    input_ids = tokenizer.encode(prompt, return_tensors='pt').cuda()

    # 2. åˆå§‹åŒ– KV Cache
    kv_cache = KVCache(model.config)
    kv_cache.allocate(batch_size=1)

    # 3. Prefill é˜¶æ®µ
    logits = prefill(model, input_ids, kv_cache)

    # 4. é‡‡æ ·ç¬¬ä¸€ä¸ª token
    sampling_params = SamplingParams(temperature=temperature, top_p=top_p)
    first_token = sample(logits, sampling_params)
    generated_tokens = [first_token.item()]

    # 5. Decode å¾ªç¯
    current_token = first_token
    for _ in range(max_tokens - 1):
        # Decode ä¸€æ­¥
        logits = decode_step(model, current_token, kv_cache)

        # é‡‡æ ·
        next_token = sample(logits, sampling_params)

        # æ£€æŸ¥åœæ­¢æ¡ä»¶
        if next_token.item() == tokenizer.eos_token_id:
            break

        generated_tokens.append(next_token.item())
        current_token = next_token

    # 6. Detokenization
    output_text = tokenizer.decode(generated_tokens)
    return output_text

output = generate(model, tokenizer, "Once upon a time", max_tokens=50)
print(output)
```

### 7.2 æ—¶åºå›¾

```mermaid
sequenceDiagram
    participant T as Tokenizer
    participant P as Prefill
    participant D as Decode
    participant S as Sampler
    participant C as KV Cache

    Note over T,C: è¾“å…¥: "Hello"

    T->>P: token_ids = [1, 15043]
    P->>C: åˆå§‹åŒ–ç¼“å­˜
    P->>C: å­˜å‚¨ K[0:2], V[0:2]
    P->>S: logits
    S->>D: token_id = 318 ("I")

    loop Decode å¾ªç¯
        D->>C: è¯»å– K[0:n], V[0:n]
        D->>C: å†™å…¥ K[n], V[n]
        D->>S: logits
        S->>D: next_token
    end

    Note over T,C: è¾“å‡º: "I am fine"
```

---

## 8. æœ¬ç« å°ç»“

### ç”Ÿæˆæµç¨‹

1. **Tokenization**ï¼šæ–‡æœ¬ â†’ Token IDs
2. **Prefill**ï¼šå¹¶è¡Œå¤„ç†è¾“å…¥ï¼Œåˆå§‹åŒ– KV Cache
3. **Decode**ï¼šé€ä¸ªç”Ÿæˆ tokenï¼Œå¢é‡æ›´æ–° KV Cache
4. **Sampling**ï¼šä» logits é‡‡æ · token
5. **Detokenization**ï¼šToken IDs â†’ æ–‡æœ¬

### ä¸¤é˜¶æ®µç‰¹æ€§

| é˜¶æ®µ | Prefill | Decode |
|------|---------|--------|
| å¹¶è¡Œåº¦ | é«˜ | ä½ï¼ˆæ¯æ¬¡ 1 tokenï¼‰ |
| è®¡ç®—å¯†åº¦ | é«˜ | ä½ |
| ç“¶é¢ˆ | è®¡ç®— | å†…å­˜å¸¦å®½ |
| ä¼˜åŒ–é‡ç‚¹ | å¹¶è¡Œè®¡ç®— | æ‰¹å¤„ç† |

### é‡‡æ ·ç­–ç•¥

- **Greedy**ï¼šç¡®å®šæ€§ï¼Œå–æœ€å¤§æ¦‚ç‡
- **Temperature**ï¼šæ§åˆ¶éšæœºç¨‹åº¦
- **Top-k**ï¼šé™åˆ¶å€™é€‰æ•°é‡
- **Top-p**ï¼šåŠ¨æ€é™åˆ¶ç´¯ç§¯æ¦‚ç‡

### ä¸ vLLM çš„å…³è”

- **Continuous Batching**ï¼šåŠ¨æ€ç»„åˆ Prefill å’Œ Decode
- **Chunked Prefill**ï¼šåˆ†å—å¤„ç†é•¿è¾“å…¥
- **é‡‡æ ·ä¼˜åŒ–**ï¼šæ‰¹é‡é‡‡æ ·æé«˜æ•ˆç‡

---

## æ€è€ƒé¢˜

1. ä¸ºä»€ä¹ˆ Decode é˜¶æ®µä¸èƒ½åƒ Prefill é‚£æ ·å¹¶è¡Œå¤„ç†å¤šä¸ª tokenï¼Ÿ
2. å¦‚æœä½¿ç”¨ temperature=0ï¼Œç»“æœä¼šå’Œ greedy decoding ä¸€æ ·å—ï¼Ÿ
3. vLLM çš„ Continuous Batching å¦‚ä½•åŒæ—¶å¤„ç† Prefill å’Œ Decode è¯·æ±‚ï¼Ÿ

---

## ä¸‹ä¸€æ­¥

æ·±åº¦å­¦ä¹ åŸºç¡€éƒ¨åˆ†å·²ç»å®Œæˆï¼æ¥ä¸‹æ¥æˆ‘ä»¬å°†è¿›å…¥æ ¸å¿ƒæ¨¡å—è¯¦è§£ï¼Œé¦–å…ˆä»‹ç» vLLM çš„æ ¸å¿ƒåˆ›æ–°â€”â€”PagedAttentionï¼š

ğŸ‘‰ [ä¸‹ä¸€ç« ï¼šPagedAttention åˆ†é¡µæ³¨æ„åŠ›](../03-core-modules/01-paged-attention.md)
