---
title: "PagedAttention åˆ†é¡µæ³¨æ„åŠ›"
weight: 1
---


> æœ¬ç« å°†è¯¦ç»†ä»‹ç» vLLM çš„æ ¸å¿ƒåˆ›æ–°â€”â€”PagedAttentionï¼ŒåŒ…æ‹¬è®¾è®¡æ€æƒ³ã€æ•°æ®ç»“æ„å’Œå®ç°åŸç†ã€‚

---

## å¼•è¨€

PagedAttention æ˜¯ vLLM æœ€é‡è¦çš„åˆ›æ–°ï¼Œå®ƒå€Ÿé‰´äº†æ“ä½œç³»ç»Ÿè™šæ‹Ÿå†…å­˜ç®¡ç†çš„æ€æƒ³ï¼Œé©å‘½æ€§åœ°è§£å†³äº† KV Cache çš„æ˜¾å­˜æµªè´¹é—®é¢˜ã€‚æœ¬ç« å°†æ·±å…¥å‰–æå…¶è®¾è®¡åŸç†å’Œå®ç°ç»†èŠ‚ã€‚

---

## 1. ä¼ ç»Ÿ KV Cache çš„é—®é¢˜å›é¡¾

### 1.1 è¿ç»­å†…å­˜åˆ†é…çš„è¦æ±‚

ä¼ ç»Ÿæ–¹æ¡ˆè¦æ±‚æ¯ä¸ªè¯·æ±‚çš„ KV Cache å­˜å‚¨åœ¨**è¿ç»­çš„å†…å­˜ç©ºé—´**ä¸­ï¼š

```
ä¼ ç»Ÿ KV Cache å¸ƒå±€:
+----------------------------------------------------------+
| Request A çš„ KV Cache (é¢„åˆ†é… max_seq_len)                 |
| [K0,V0][K1,V1][K2,V2]...[Kn,Vn][   ç©ºé—²é¢„ç•™ç©ºé—´   ]        |
+----------------------------------------------------------+
| Request B çš„ KV Cache (é¢„åˆ†é… max_seq_len)                 |
| [K0,V0][K1,V1]...[Km,Vm][      ç©ºé—²é¢„ç•™ç©ºé—´      ]         |
+----------------------------------------------------------+
```

### 1.2 æ˜¾å­˜ç¢ç‰‡åŒ–å›¾è§£

å½“å¤šä¸ªè¯·æ±‚å¹¶å‘æ—¶ï¼Œæ˜¾å­˜ç¢ç‰‡åŒ–é—®é¢˜ä¸¥é‡ï¼š

```mermaid
graph TB
    subgraph time_t1["æ—¶é—´ T1 - ä¸‰ä¸ªè¯·æ±‚å¼€å§‹"]
        M1["Request A<br/>é¢„åˆ†é… 2GB<br/>å®é™…ç”¨ 0.1GB"]
        M2["Request B<br/>é¢„åˆ†é… 2GB<br/>å®é™…ç”¨ 0.2GB"]
        M3["Request C<br/>é¢„åˆ†é… 2GB<br/>å®é™…ç”¨ 0.1GB"]
        M4["ç©ºé—² 2GB"]
    end

    subgraph time_t2["æ—¶é—´ T2 - Request B å®Œæˆ"]
        N1["Request A<br/>é¢„åˆ†é… 2GB<br/>å®é™…ç”¨ 0.5GB"]
        N2["ç©ºæ´ 2GB<br/>å¤–éƒ¨ç¢ç‰‡"]
        N3["Request C<br/>é¢„åˆ†é… 2GB<br/>å®é™…ç”¨ 0.3GB"]
        N4["ç©ºé—² 2GB"]
    end

    subgraph time_t3["æ—¶é—´ T3 - æ–°è¯·æ±‚ D åˆ°æ¥"]
        O1["Request A<br/>2GB"]
        O2["ç©ºæ´ 2GB"]
        O3["Request C<br/>2GB"]
        O4["ç©ºé—² 2GB"]
        O5["Request D éœ€è¦ 3GB<br/>å¤±è´¥!"]
    end

    style N2 fill:#ffcdd2
    style O2 fill:#ffcdd2
    style O5 fill:#ffcdd2
```

### 1.3 é‡åŒ–æµªè´¹

| é—®é¢˜ç±»å‹ | è¯´æ˜ | æµªè´¹æ¯”ä¾‹ |
|---------|------|---------|
| å†…éƒ¨ç¢ç‰‡ | é¢„åˆ†é… >> å®é™…ä½¿ç”¨ | 40-60% |
| å¤–éƒ¨ç¢ç‰‡ | ç©ºæ´æ— æ³•åˆ©ç”¨ | 20-30% |
| **æ€»è®¡** | **ç»¼åˆæµªè´¹** | **60-80%** |

---

## 2. PagedAttention æ ¸å¿ƒæ€æƒ³

### 2.1 çµæ„Ÿæ¥æºï¼šæ“ä½œç³»ç»Ÿè™šæ‹Ÿå†…å­˜

æ“ä½œç³»ç»Ÿå¦‚ä½•ç®¡ç†å†…å­˜ï¼Ÿ

```mermaid
graph LR
    subgraph virtual_memory["è™šæ‹Ÿå†…å­˜"]
        VA["è™šæ‹Ÿåœ°å€ç©ºé—´<br/>ç¨‹åºçœ‹åˆ°çš„è¿ç»­ç©ºé—´"]
    end

    subgraph page_table["é¡µè¡¨"]
        PT["Page Table<br/>è™šæ‹Ÿé¡µ â†’ ç‰©ç†é¡µ"]
    end

    subgraph physical_memory["ç‰©ç†å†…å­˜"]
        PA["ç‰©ç†å†…å­˜<br/>å®é™…ä¸è¿ç»­çš„é¡µ"]
    end

    VA --> PT --> PA
```

**å…³é”®ç‰¹æ€§**ï¼š
1. ç¨‹åºçœ‹åˆ°è¿ç»­çš„åœ°å€ç©ºé—´
2. ç‰©ç†å†…å­˜å¯ä»¥ä¸è¿ç»­
3. æŒ‰éœ€åˆ†é…ï¼ˆç”¨åˆ°æ‰åˆ†é…ï¼‰
4. é¡µé¢å¯ä»¥å…±äº«

### 2.2 PagedAttention çš„ç±»æ¯”

å°†æ“ä½œç³»ç»Ÿçš„æ€æƒ³åº”ç”¨åˆ° KV Cache ç®¡ç†ï¼š

| æ“ä½œç³»ç»Ÿæ¦‚å¿µ | PagedAttention å¯¹åº” |
|-------------|-------------------|
| é¡µï¼ˆPageï¼‰ | Blockï¼ˆå—ï¼‰ |
| é¡µè¡¨ï¼ˆPage Tableï¼‰ | Block Tableï¼ˆå—è¡¨ï¼‰ |
| è™šæ‹Ÿåœ°å€ | é€»è¾‘å—ç´¢å¼• |
| ç‰©ç†åœ°å€ | ç‰©ç†å— ID |
| é¡µå¸§ | KV Cache å— |

### 2.3 æ ¸å¿ƒæ”¹è¿›

```mermaid
graph LR
    subgraph traditional["ä¼ ç»Ÿæ–¹æ¡ˆ"]
        T1["é¢„åˆ†é…è¿ç»­ç©ºé—´"]
        T2["å¤§é‡æµªè´¹"]
        T1 --> T2
    end

    subgraph paged_attention["PagedAttention"]
        P1["æŒ‰éœ€åˆ†é…"]
        P2["åˆ†å—å­˜å‚¨"]
        P3["éè¿ç»­"]
        P4["é«˜åˆ©ç”¨ç‡"]
        P1 --> P2 --> P3 --> P4
    end

    style T2 fill:#ffcdd2
    style P4 fill:#c8e6c9
```

---

## 3. å…³é”®æ•°æ®ç»“æ„è¯¦è§£

### 3.1 Blockï¼ˆå—ï¼‰

Block æ˜¯ KV Cache çš„åŸºæœ¬å­˜å‚¨å•å…ƒï¼š

```python
# æ¦‚å¿µå®šä¹‰
class KVCacheBlock:
    block_id: int              # ç‰©ç†å— ID
    ref_cnt: int               # å¼•ç”¨è®¡æ•°ï¼ˆæ”¯æŒå…±äº«ï¼‰
    block_hash: Optional[int]  # ç”¨äºå‰ç¼€ç¼“å­˜åŒ¹é…
```

**Block çš„ç‰¹ç‚¹**ï¼š
- **å›ºå®šå¤§å°**ï¼šæ¯ä¸ª block å­˜å‚¨å›ºå®šæ•°é‡çš„ tokenï¼ˆå¦‚ 16 ä¸ªï¼‰
- **ç‹¬ç«‹åˆ†é…**ï¼šä¸éœ€è¦è¿ç»­
- **å¯å¤ç”¨**ï¼šé‡Šæ”¾åå¯åˆ†é…ç»™å…¶ä»–è¯·æ±‚

### 3.2 Block çš„å­˜å‚¨å†…å®¹

æ¯ä¸ª Block å­˜å‚¨è‹¥å¹² token çš„ K å’Œ Vï¼š

```
Block ç»“æ„ (block_size = 16):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Token 0:  K[layers, heads, head_dim]            â”‚
â”‚           V[layers, heads, head_dim]            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Token 1:  K[layers, heads, head_dim]            â”‚
â”‚           V[layers, heads, head_dim]            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ...                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Token 15: K[layers, heads, head_dim]            â”‚
â”‚           V[layers, heads, head_dim]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å®é™…å­˜å‚¨å½¢çŠ¶**ï¼š

```python
# å•ä¸ª Block çš„ KV Cache å½¢çŠ¶
k_block = torch.zeros(num_layers, num_heads, block_size, head_dim)
v_block = torch.zeros(num_layers, num_heads, block_size, head_dim)

kv_cache = torch.zeros(num_blocks, 2, num_layers, num_heads, block_size, head_dim)
```

### 3.3 Block Tableï¼ˆå—è¡¨ï¼‰

Block Table è®°å½•é€»è¾‘å—åˆ°ç‰©ç†å—çš„æ˜ å°„ï¼š

```mermaid
classDiagram
    class BlockTable {
        +block_ids: List[int]
        +num_blocks: int
        +append(block_id)
        +get_physical_block(logical_idx) int
    }
```

**ç¤ºä¾‹**ï¼š

```
Request A çš„ Block Table:
é€»è¾‘å—ç´¢å¼•:  0    1    2    3
            â†“    â†“    â†“    â†“
ç‰©ç†å— ID:  [5]  [2]  [8]  [12]

è§£é‡Š:
- é€»è¾‘å— 0 â†’ ç‰©ç†å— 5
- é€»è¾‘å— 1 â†’ ç‰©ç†å— 2
- é€»è¾‘å— 2 â†’ ç‰©ç†å— 8
- é€»è¾‘å— 3 â†’ ç‰©ç†å— 12
```

### 3.4 Slot Mappingï¼ˆæ§½ä½æ˜ å°„ï¼‰

Slot Mapping å°† token ä½ç½®æ˜ å°„åˆ°å…·ä½“çš„ç¼“å­˜æ§½ä½ï¼š

```python
def get_slot_mapping(token_position, block_size, block_table):
    """
    token_position: token åœ¨åºåˆ—ä¸­çš„ä½ç½®ï¼ˆå¦‚ 35ï¼‰
    block_size: æ¯ä¸ª block çš„ token æ•°ï¼ˆå¦‚ 16ï¼‰
    block_table: å—è¡¨
    """
    logical_block_idx = token_position // block_size  # 35 // 16 = 2
    block_offset = token_position % block_size        # 35 % 16 = 3

    physical_block_id = block_table[logical_block_idx]  # å‡è®¾æ˜¯ 8
    slot_id = physical_block_id * block_size + block_offset  # 8 * 16 + 3 = 131

    return slot_id
```

**å›¾è§£**ï¼š

```mermaid
graph LR
    subgraph token_pos["Token ä½ç½® 35"]
        T[token_position = 35]
    end

    subgraph calculation["è®¡ç®—"]
        LB["é€»è¾‘å— = 35 // 16 = 2"]
        OFF["åç§» = 35 % 16 = 3"]
        PB["ç‰©ç†å— = block_table[2] = 8"]
        SLOT["slot = 8 Ã— 16 + 3 = 131"]
    end

    T --> LB --> PB
    T --> OFF --> SLOT
    PB --> SLOT
```

---

## 4. å†…å­˜ç®¡ç†ä¼˜åŠ¿

### 4.1 å‡å°‘æ˜¾å­˜ç¢ç‰‡

```mermaid
graph TB
    subgraph traditional_approach["ä¼ ç»Ÿæ–¹æ¡ˆ"]
        A1["Request A: 2GB é¢„åˆ†é…<br/>å®é™… 0.1GB"]
        A2["Request B: 2GB é¢„åˆ†é…<br/>å®é™… 0.2GB"]
        A3["Request C: 2GB é¢„åˆ†é…<br/>å®é™… 0.1GB"]
        A4["ç©ºæ´å’Œç¢ç‰‡"]
    end

    subgraph paged_attention_approach["PagedAttention"]
        B1["Block Pool<br/>ç»Ÿä¸€ç®¡ç†æ‰€æœ‰å—"]
        B2["Request A: 2 blocks"]
        B3["Request B: 4 blocks"]
        B4["Request C: 2 blocks"]
        B5["ç©ºé—²å—å¯ç«‹å³å¤ç”¨"]
    end

    style A4 fill:#ffcdd2
    style B5 fill:#c8e6c9
```

### 4.2 æŒ‰éœ€åˆ†é…

```mermaid
sequenceDiagram
    participant R as Request
    participant S as Scheduler
    participant BP as BlockPool

    R->>S: å¼€å§‹ç”Ÿæˆï¼ˆ0 tokensï¼‰
    S->>BP: åˆ†é… 1 ä¸ª block
    BP-->>S: Block 5

    loop æ¯ 16 ä¸ª token
        R->>S: éœ€è¦æ–°ç©ºé—´
        S->>BP: åˆ†é… 1 ä¸ª block
        BP-->>S: Block N
    end

    R->>S: ç”Ÿæˆå®Œæˆ
    S->>BP: é‡Šæ”¾æ‰€æœ‰ blocks
    Note over BP: å—ç«‹å³å¯ç”¨äºå…¶ä»–è¯·æ±‚
```

### 4.3 æ”¯æŒ Copy-on-Write

å½“å¤šä¸ªè¯·æ±‚å…±äº«ç›¸åŒå‰ç¼€æ—¶ï¼Œå¯ä»¥å…±äº« Blockï¼š

```mermaid
graph TB
    subgraph shared_scenario["å…±äº«åœºæ™¯"]
        P["å…±åŒå‰ç¼€<br/>'System prompt...'"]
    end

    subgraph shared_blocks["å…±äº«çš„ Blocks"]
        B1[Block 0]
        B2[Block 1]
        B3[Block 2]
    end

    subgraph request_a["Request A"]
        A["ç»§ç»­ç”Ÿæˆ A çš„å†…å®¹"]
        AB[Block 10]
    end

    subgraph request_b["Request B"]
        B["ç»§ç»­ç”Ÿæˆ B çš„å†…å®¹"]
        BB[Block 15]
    end

    P --> B1
    P --> B2
    P --> B3
    B3 --> AB
    B3 --> BB

    style B1 fill:#c8e6c9
    style B2 fill:#c8e6c9
    style B3 fill:#c8e6c9
```

**å¼•ç”¨è®¡æ•°**ï¼š
- Block 0, 1, 2 çš„ ref_cnt = 2ï¼ˆè¢«ä¸¤ä¸ªè¯·æ±‚å…±äº«ï¼‰
- åªæœ‰å½“ ref_cnt = 0 æ—¶æ‰çœŸæ­£é‡Šæ”¾

### 4.4 æ”¯æŒå‰ç¼€ç¼“å­˜

ç›¸åŒå‰ç¼€çš„è¯·æ±‚å¯ä»¥ç›´æ¥å¤ç”¨å·²è®¡ç®—çš„ KV Cacheï¼š

```python
# å‰ç¼€ç¼“å­˜ç¤ºä¾‹
request_1 = "ä½ å¥½ï¼Œè¯·é—®" + "å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
request_2 = "ä½ å¥½ï¼Œè¯·é—®" + "ä»Šå¤©æ˜ŸæœŸå‡ ï¼Ÿ"

```

---

## 5. PagedAttention è®¡ç®—æµç¨‹

### 5.1 å†™å…¥ KV Cache

```mermaid
sequenceDiagram
    participant M as Model
    participant PA as PagedAttention
    participant BT as Block Table
    participant KVC as KV Cache Memory

    M->>PA: æ–° token çš„ K, V
    PA->>BT: æŸ¥è¯¢ slot_mapping
    BT-->>PA: slot_id = 131
    PA->>KVC: kv_cache[131] = (K, V)
```

### 5.2 è¯»å–å¹¶è®¡ç®— Attention

```mermaid
flowchart TD
    subgraph input["è¾“å…¥"]
        Q["Query: æ–° token çš„ Q"]
        BT["Block Table: ç‰©ç†å—åˆ—è¡¨"]
    end

    subgraph paged_attn_compute["PagedAttention è®¡ç®—"]
        FETCH["ä»å„ä¸ªç‰©ç†å—è·å– K, V"]
        COMPUTE["è®¡ç®— Attention<br/>Q Ã— K^T / âˆšd"]
        SOFTMAX[Softmax]
        WEIGHTED["åŠ æƒæ±‚å’Œ V"]
    end

    subgraph output["è¾“å‡º"]
        O["Attention è¾“å‡º"]
    end

    Q --> COMPUTE
    BT --> FETCH
    FETCH --> COMPUTE
    COMPUTE --> SOFTMAX
    SOFTMAX --> WEIGHTED
    WEIGHTED --> O
```

### 5.3 ä»£ç å®ç°æ¦‚è§ˆ

```python
# vllm/v1/attention/ops/paged_attn.py (ç®€åŒ–ç‰ˆ)

class PagedAttention:
    @staticmethod
    def write_to_paged_cache(
        key: torch.Tensor,           # [num_tokens, num_heads, head_dim]
        value: torch.Tensor,         # [num_tokens, num_heads, head_dim]
        key_cache: torch.Tensor,     # [num_blocks, block_size, num_heads, head_dim]
        value_cache: torch.Tensor,   # [num_blocks, block_size, num_heads, head_dim]
        slot_mapping: torch.Tensor,  # [num_tokens]
    ):
        """å°†æ–°çš„ K, V å†™å…¥ç¼“å­˜"""
        # ä½¿ç”¨ slot_mapping ç¡®å®šå†™å…¥ä½ç½®
        # slot_mapping[i] å‘Šè¯‰æˆ‘ä»¬ token i åº”è¯¥å†™å…¥å“ªä¸ªæ§½ä½
        pass

    @staticmethod
    def forward(
        query: torch.Tensor,         # [num_tokens, num_heads, head_dim]
        key_cache: torch.Tensor,     # KV Cache
        value_cache: torch.Tensor,
        block_tables: torch.Tensor,  # [batch, max_blocks] å—è¡¨
        context_lens: torch.Tensor,  # [batch] æ¯ä¸ªè¯·æ±‚çš„ä¸Šä¸‹æ–‡é•¿åº¦
        ...
    ) -> torch.Tensor:
        """æ‰§è¡Œ PagedAttention è®¡ç®—"""
        # 1. æ ¹æ® block_tables å®šä½ K, V
        # 2. è®¡ç®— Attention
        # 3. è¿”å›è¾“å‡º
        pass
```

---

## 6. å—çš„åŠ¨æ€ç®¡ç†

### 6.1 å—çš„ç”Ÿå‘½å‘¨æœŸ

```mermaid
stateDiagram-v2
    [*] --> Free: åˆå§‹åŒ–

    Free --> Allocated: åˆ†é…ç»™è¯·æ±‚
    Allocated --> Free: è¯·æ±‚å®Œæˆ<br/>ref_cnt=0

    Allocated --> Cached: å¯ç”¨å‰ç¼€ç¼“å­˜
    Cached --> Allocated: ç¼“å­˜å‘½ä¸­<br/>ref_cnt++

    Cached --> Free: LRU é©±é€<br/>æˆ–ç¼“å­˜å¤±æ•ˆ

    note right of Allocated : ref_cnt >= 1
    note right of Cached : ä¿ç•™ä»¥å¤‡å¤ç”¨
```

### 6.2 å—åˆ†é…æµç¨‹

```python
def allocate_blocks_for_request(request, kv_cache_manager):
    """ä¸ºè¯·æ±‚åˆ†é…æ‰€éœ€çš„ blocks"""
    num_tokens = len(request.prompt_tokens) + request.num_generated_tokens
    num_blocks_needed = (num_tokens + block_size - 1) // block_size

    blocks = []
    for i in range(num_blocks_needed):
        # å°è¯•è·å–ç©ºé—²å—
        block = kv_cache_manager.get_free_block()
        if block is None:
            # æ²¡æœ‰ç©ºé—²å—ï¼Œè§¦å‘é©±é€æˆ–è¿”å›å¤±è´¥
            return None
        blocks.append(block)

    # æ›´æ–°è¯·æ±‚çš„å—è¡¨
    request.block_table = blocks
    return blocks
```

### 6.3 å—å¢é•¿è¿‡ç¨‹

```mermaid
gantt
    title è¯·æ±‚çš„å—åˆ†é…æ—¶é—´çº¿
    dateFormat X
    axisFormat %s

    section å—åˆ†é…
    Block 0 (prefill) :done, b0, 0, 16
    Block 1 (prefill ç»­) :done, b1, 16, 32
    Block 2 (decode) :active, b2, 32, 48
    Block 3 (decode ç»­) :active, b3, 48, 64
```

---

## 7. CUDA å†…æ ¸å®ç°

### 7.1 æ–‡ä»¶ä½ç½®

- Python æ¥å£ï¼š`vllm/v1/attention/ops/paged_attn.py`
- CUDA å†…æ ¸ï¼š`csrc/attention/paged_attention_v1.cu`ã€`paged_attention_v2.cu`

### 7.2 V1 vs V2 å†…æ ¸

| ç‰¹æ€§ | V1 | V2 |
|------|----|----|
| é€‚ç”¨åœºæ™¯ | çŸ­åºåˆ— | é•¿åºåˆ— |
| åˆ†å—ç­–ç•¥ | ç®€å• | ä¸¤çº§åˆ†å— |
| æ€§èƒ½ | ä¸­ç­‰ | æ›´ä¼˜ |

### 7.3 å†…æ ¸å‚æ•°

```cpp
// paged_attention_v2.cu (ç®€åŒ–)
template<typename T, int BLOCK_SIZE, int NUM_THREADS>
__global__ void paged_attention_v2_kernel(
    T* __restrict__ out,              // è¾“å‡º
    const T* __restrict__ q,          // Query
    const T* __restrict__ k_cache,    // Key Cache
    const T* __restrict__ v_cache,    // Value Cache
    const int* __restrict__ block_tables,  // å—è¡¨
    const int* __restrict__ context_lens,  // ä¸Šä¸‹æ–‡é•¿åº¦
    const float scale,                // ç¼©æ”¾å› å­
    ...
) {
    // 1. ç¡®å®šå½“å‰çº¿ç¨‹å¤„ç†çš„ query
    // 2. éå† block_table ä¸­çš„æ‰€æœ‰å—
    // 3. è®¡ç®— Attention åˆ†æ•°
    // 4. Softmax å’ŒåŠ æƒæ±‚å’Œ
}
```

---

## 8. æ€§èƒ½å¯¹æ¯”

### 8.1 æ˜¾å­˜æ•ˆç‡

| æ–¹æ¡ˆ | æ˜¾å­˜åˆ©ç”¨ç‡ | æœ€å¤§å¹¶å‘ |
|------|-----------|---------|
| ä¼ ç»Ÿé¢„åˆ†é… | 20-40% | ä½ |
| PagedAttention | 96%+ | é«˜ 2-4 å€ |

### 8.2 ååé‡æå‡

```mermaid
graph LR
    subgraph throughput_compare["ååé‡å¯¹æ¯”"]
        T1["ä¼ ç»Ÿæ–¹æ¡ˆ<br/>1x åŸºå‡†"]
        T2["PagedAttention<br/>2-4x æå‡"]
    end

    style T2 fill:#c8e6c9
```

### 8.3 ç¢ç‰‡ç‡

```
ä¼ ç»Ÿæ–¹æ¡ˆ:
- å†…éƒ¨ç¢ç‰‡: 50-70%
- å¤–éƒ¨ç¢ç‰‡: 10-20%
- æ€»ç¢ç‰‡: 60-80%

PagedAttention:
- å†…éƒ¨ç¢ç‰‡: < 4% (æœ€åä¸€ä¸ªå—)
- å¤–éƒ¨ç¢ç‰‡: 0% (å›ºå®šå¤§å°å—)
- æ€»ç¢ç‰‡: < 4%
```

---

## 9. æœ¬ç« å°ç»“

### æ ¸å¿ƒåˆ›æ–°

1. **åˆ†å—å­˜å‚¨**ï¼šå°† KV Cache åˆ†æˆå›ºå®šå¤§å°çš„ Block
2. **éè¿ç»­åˆ†é…**ï¼šBlock å¯ä»¥åˆ†æ•£åœ¨æ˜¾å­˜ä»»æ„ä½ç½®
3. **æŒ‰éœ€åˆ†é…**ï¼šç”Ÿæˆæ–° token æ—¶æ‰åˆ†é…æ–° Block
4. **å—è¡¨æ˜ å°„**ï¼šé€šè¿‡ Block Table ç®¡ç†é€»è¾‘åˆ°ç‰©ç†çš„æ˜ å°„

### å…³é”®æ•°æ®ç»“æ„

| ç»“æ„ | ä½œç”¨ |
|------|------|
| Block | KV Cache çš„åŸºæœ¬å­˜å‚¨å•å…ƒ |
| Block Table | é€»è¾‘å— â†’ ç‰©ç†å—æ˜ å°„ |
| Slot Mapping | Token ä½ç½® â†’ ç¼“å­˜æ§½ä½ |
| BlockPool | ç®¡ç†æ‰€æœ‰ç©ºé—²å— |

### ä¼˜åŠ¿æ€»ç»“

- **æ˜¾å­˜æ•ˆç‡**ï¼šä» 20-40% æå‡åˆ° 96%+
- **å‡å°‘ç¢ç‰‡**ï¼šä» 60-80% é™åˆ° 4% ä»¥ä¸‹
- **æ”¯æŒå…±äº«**ï¼šå¤šè¯·æ±‚å¯å…±äº«ç›¸åŒå‰ç¼€çš„ Block
- **æŒ‰éœ€å¢é•¿**ï¼šä¸éœ€è¦é¢„åˆ†é…æœ€å¤§é•¿åº¦

### ä»£ç ä½ç½®

| åŠŸèƒ½ | æ–‡ä»¶ |
|------|------|
| Python æ¥å£ | `vllm/v1/attention/ops/paged_attn.py` |
| CUDA å†…æ ¸ | `csrc/attention/paged_attention_v2.cu` |
| å—ç®¡ç† | `vllm/v1/core/block_pool.py` |
| å—è¡¨ | `vllm/v1/worker/block_table.py` |

---

## æ€è€ƒé¢˜

1. ä¸ºä»€ä¹ˆé€‰æ‹©å›ºå®šå¤§å°çš„ Block è€Œä¸æ˜¯å¯å˜å¤§å°ï¼Ÿ
2. å‰ç¼€ç¼“å­˜å’Œ Copy-on-Write æœ‰ä»€ä¹ˆåŒºåˆ«å’Œè”ç³»ï¼Ÿ
3. å¦‚æœ block_size è®¾ç½®å¾—å¤ªå¤§æˆ–å¤ªå°ï¼Œä¼šæœ‰ä»€ä¹ˆå½±å“ï¼Ÿ

---

## ä¸‹ä¸€æ­¥

äº†è§£äº† PagedAttention çš„åŸç†åï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹ KV Cache Manager æ˜¯å¦‚ä½•ç®¡ç†è¿™äº› Block çš„ï¼š

ğŸ‘‰ [ä¸‹ä¸€ç« ï¼šKV Cache ç®¡ç†å™¨](02-kv-cache-manager.md)
