# Transformer æ¶æ„è¯¦è§£

> æœ¬ç« å°†è¯¦ç»†ä»‹ç» Transformer æ¶æ„ï¼Œè¿™æ˜¯ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„åŸºç¡€ã€‚

---

## å¼•è¨€

2017 å¹´ï¼ŒGoogle å‘è¡¨äº†åˆ’æ—¶ä»£çš„è®ºæ–‡ã€ŠAttention Is All You Needã€‹ï¼Œæå‡ºäº† Transformer æ¶æ„ã€‚è¿™ä¸ªæ¶æ„å½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œæˆä¸ºäº† GPTã€BERTã€LLaMA ç­‰ç°ä»£ LLM çš„åŸºç¡€ã€‚

ç†è§£ Transformer æ¶æ„æ˜¯ç†è§£ vLLM ä¼˜åŒ–åŸç†çš„å…³é”®ã€‚

---

## 1. Transformer çš„è¯ç”ŸèƒŒæ™¯

### 1.1 RNN/LSTM çš„å±€é™

åœ¨ Transformer ä¹‹å‰ï¼Œåºåˆ—å»ºæ¨¡ä¸»è¦ä¾èµ– RNNï¼ˆå¾ªç¯ç¥ç»ç½‘ç»œï¼‰å’Œ LSTMï¼ˆé•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼‰ï¼š

```mermaid
graph LR
    subgraph RNN çš„é¡ºåºå¤„ç†
        X1[xâ‚] --> H1[hâ‚] --> H2[hâ‚‚] --> H3[hâ‚ƒ] --> H4[hâ‚„]
        X2[xâ‚‚] --> H2
        X3[xâ‚ƒ] --> H3
        X4[xâ‚„] --> H4
    end
```

**RNN çš„é—®é¢˜**ï¼š

| é—®é¢˜ | è¯´æ˜ |
|------|------|
| **é¡ºåºä¾èµ–** | å¿…é¡»æŒ‰é¡ºåºå¤„ç†ï¼Œæ— æ³•å¹¶è¡Œ |
| **é•¿è·ç¦»ä¾èµ–** | éš¾ä»¥æ•è·é•¿åºåˆ—ä¸­çš„è¿œè·ç¦»å…³ç³» |
| **æ¢¯åº¦é—®é¢˜** | é•¿åºåˆ—è®­ç»ƒæ—¶æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸ |
| **è®­ç»ƒæ…¢** | æ— æ³•å……åˆ†åˆ©ç”¨ GPU å¹¶è¡Œèƒ½åŠ› |

### 1.2 Attention çš„çªç ´

Transformer çš„æ ¸å¿ƒåˆ›æ–°æ˜¯**è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰**ï¼š

- å¯ä»¥ç›´æ¥å»ºç«‹åºåˆ—ä¸­ä»»æ„ä¸¤ä¸ªä½ç½®ä¹‹é—´çš„å…³ç³»
- æ‰€æœ‰ä½ç½®å¯ä»¥å¹¶è¡Œè®¡ç®—
- æ²¡æœ‰é¡ºåºä¾èµ–

```mermaid
graph TB
    subgraph Self-Attention
        X1[xâ‚] <--> X2[xâ‚‚]
        X1 <--> X3[xâ‚ƒ]
        X1 <--> X4[xâ‚„]
        X2 <--> X3
        X2 <--> X4
        X3 <--> X4
    end
```

---

## 2. Transformer æ•´ä½“æ¶æ„

### 2.1 åŸå§‹ Encoder-Decoder ç»“æ„

åŸå§‹ Transformer åŒ…å« Encoder å’Œ Decoder ä¸¤éƒ¨åˆ†ï¼š

```mermaid
graph TB
    subgraph è¾“å…¥
        I[æºåºåˆ—<br/>ä¾‹: è‹±æ–‡å¥å­]
    end

    subgraph Encoder
        E1[Embedding + ä½ç½®ç¼–ç ]
        E2[Multi-Head Attention]
        E3[Feed Forward]
        E4[Ã— N å±‚]
        E1 --> E2 --> E3
        E3 -.-> E4
    end

    subgraph Decoder
        D1[Embedding + ä½ç½®ç¼–ç ]
        D2[Masked Multi-Head Attention]
        D3[Cross Attention]
        D4[Feed Forward]
        D5[Ã— N å±‚]
        D1 --> D2 --> D3 --> D4
        D4 -.-> D5
    end

    subgraph è¾“å‡º
        O[ç›®æ ‡åºåˆ—<br/>ä¾‹: ä¸­æ–‡ç¿»è¯‘]
    end

    I --> E1
    E4 --> D3
    D5 --> O
```

**åº”ç”¨åœºæ™¯**ï¼š
- æœºå™¨ç¿»è¯‘ï¼ˆè‹±â†’ä¸­ï¼‰
- æ–‡æœ¬æ‘˜è¦
- BERTï¼ˆä»… Encoderï¼‰
- T5ï¼ˆå®Œæ•´ Encoder-Decoderï¼‰

### 2.2 Decoder-Only æ¶æ„ï¼ˆç°ä»£ LLMï¼‰

ç°ä»£å¤§è¯­è¨€æ¨¡å‹ï¼ˆGPT ç³»åˆ—ã€LLaMAã€Qwen ç­‰ï¼‰éƒ½é‡‡ç”¨ **Decoder-Only** æ¶æ„ï¼š

```mermaid
graph TD
    subgraph Decoder-Only æ¶æ„
        I[è¾“å…¥ tokens] --> EMB[Embedding Layer]
        EMB --> PE[+ ä½ç½®ç¼–ç ]
        PE --> B1[Transformer Block 1]
        B1 --> B2[Transformer Block 2]
        B2 --> B3[...]
        B3 --> BN[Transformer Block N]
        BN --> LN[Layer Norm]
        LN --> LM[LM Head<br/>Linear: hidden â†’ vocab]
        LM --> O[è¾“å‡º logits]
    end

    style EMB fill:#e3f2fd
    style B1 fill:#c8e6c9
    style B2 fill:#c8e6c9
    style BN fill:#c8e6c9
    style LM fill:#fff9c4
```

**ä¸ºä»€ä¹ˆ Decoder-Only æˆä¸ºä¸»æµï¼Ÿ**

| ä¼˜åŠ¿ | è¯´æ˜ |
|------|------|
| **ç»Ÿä¸€æ¶æ„** | é¢„è®­ç»ƒå’Œä¸‹æ¸¸ä»»åŠ¡ä½¿ç”¨ç›¸åŒæ¶æ„ |
| **è‡ªå›å½’ç”Ÿæˆ** | å¤©ç„¶é€‚åˆæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ |
| **æ‰©å±•æ€§** | å‚æ•°é‡æ‰©å±•æ•ˆæœå¥½ |
| **ç®€å•é«˜æ•ˆ** | æ¶æ„ç®€å•ï¼Œè®­ç»ƒæ¨ç†æ›´é«˜æ•ˆ |

### 2.3 å•å±‚ Transformer Block ç»“æ„

æ¯ä¸ª Transformer Block åŒ…å«ä»¥ä¸‹ç»„ä»¶ï¼š

```mermaid
graph TD
    subgraph Transformer Block
        I[è¾“å…¥ X] --> LN1[Layer Norm 1]
        LN1 --> ATT[Multi-Head<br/>Self-Attention]
        ATT --> ADD1[+]
        I --> ADD1
        ADD1 --> LN2[Layer Norm 2]
        LN2 --> FFN[Feed Forward<br/>Network]
        FFN --> ADD2[+]
        ADD1 --> ADD2
        ADD2 --> O[è¾“å‡º]
    end

    style ATT fill:#bbdefb
    style FFN fill:#c8e6c9
```

**å…³é”®ç»„ä»¶**ï¼š

1. **Layer Normalization**ï¼šå½’ä¸€åŒ–ï¼Œç¨³å®šè®­ç»ƒ
2. **Multi-Head Self-Attention**ï¼šæ•è·åºåˆ—å†…çš„å…³ç³»
3. **Feed Forward Network (FFN)**ï¼šéçº¿æ€§å˜æ¢
4. **æ®‹å·®è¿æ¥**ï¼šç¼“è§£æ¢¯åº¦æ¶ˆå¤±ï¼Œå¸®åŠ©ä¿¡æ¯æµåŠ¨

---

## 3. Embedding å±‚

### 3.1 Token Embedding

Token Embedding å°†ç¦»æ•£çš„ token ID æ˜ å°„ä¸ºè¿ç»­çš„å‘é‡ï¼š

```python
import torch.nn as nn

class TokenEmbedding(nn.Module):
    def __init__(self, vocab_size, hidden_dim):
        super().__init__()
        # åˆ›å»ºåµŒå…¥çŸ©é˜µ: [vocab_size, hidden_dim]
        self.embedding = nn.Embedding(vocab_size, hidden_dim)

    def forward(self, token_ids):
        # token_ids: [batch_size, seq_len]
        # è¿”å›: [batch_size, seq_len, hidden_dim]
        return self.embedding(token_ids)

# ç¤ºä¾‹
vocab_size = 32000
hidden_dim = 4096
embedding = TokenEmbedding(vocab_size, hidden_dim)

# è¾“å…¥ token IDs
token_ids = torch.tensor([[1, 234, 567], [89, 10, 1112]])  # [2, 3]
# è¾“å‡ºåµŒå…¥å‘é‡
vectors = embedding(token_ids)  # [2, 3, 4096]
```

### 3.2 Embedding çŸ©é˜µçš„å‚æ•°é‡

```
å‚æ•°é‡ = vocab_size Ã— hidden_dim
```

**ç¤ºä¾‹**ï¼ˆLLaMA-2-7Bï¼‰ï¼š
```
å‚æ•°é‡ = 32000 Ã— 4096 = 131,072,000 â‰ˆ 131M
```

å  7B æ¨¡å‹æ€»å‚æ•°çš„çº¦ **1.9%**ã€‚

---

## 4. ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰

### 4.1 ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ä¿¡æ¯

Self-Attention æœ¬èº«ä¸åŒ…å«ä½ç½®ä¿¡æ¯â€”â€”å®ƒåªçœ‹ token ä¹‹é—´çš„å…³ç³»ï¼Œä¸çŸ¥é“å®ƒä»¬çš„é¡ºåºã€‚

```
# è¿™ä¸¤ä¸ªåºåˆ—çš„ Attention è®¡ç®—ç»“æœç›¸åŒï¼ˆå¦‚æœæ²¡æœ‰ä½ç½®ç¼–ç ï¼‰
"çŒ« è¿½ ç‹—"
"ç‹— è¿½ çŒ«"
```

ä½ç½®ç¼–ç ä¸ºæ¯ä¸ªä½ç½®æ·»åŠ ç‹¬ç‰¹çš„ä¿¡æ¯ï¼Œè®©æ¨¡å‹çŸ¥é“ token çš„é¡ºåºã€‚

### 4.2 æ­£å¼¦ä½ç½®ç¼–ç 

åŸå§‹ Transformer ä½¿ç”¨æ­£å¼¦/ä½™å¼¦å‡½æ•°ï¼š

```
PE(pos, 2i) = sin(pos / 10000^(2i/d))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d))
```

å…¶ä¸­ï¼š
- `pos`ï¼šä½ç½®ç´¢å¼•
- `i`ï¼šç»´åº¦ç´¢å¼•
- `d`ï¼šæ€»ç»´åº¦æ•°

```python
import numpy as np

def sinusoidal_position_encoding(max_len, hidden_dim):
    position = np.arange(max_len)[:, np.newaxis]
    div_term = np.exp(np.arange(0, hidden_dim, 2) * -(np.log(10000.0) / hidden_dim))

    pe = np.zeros((max_len, hidden_dim))
    pe[:, 0::2] = np.sin(position * div_term)
    pe[:, 1::2] = np.cos(position * div_term)
    return pe

# ç”Ÿæˆä½ç½®ç¼–ç 
pe = sinusoidal_position_encoding(512, 4096)
# Shape: [512, 4096]
```

### 4.3 RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰

ç°ä»£ LLMï¼ˆå¦‚ LLaMAã€Qwenï¼‰ä½¿ç”¨ **RoPEï¼ˆRotary Position Embeddingï¼‰**ï¼š

```mermaid
graph LR
    subgraph RoPE åŸç†
        Q[Query å‘é‡] --> R1[æ—‹è½¬çŸ©é˜µ<br/>R(pos)]
        R1 --> RQ[æ—‹è½¬åçš„ Query]

        K[Key å‘é‡] --> R2[æ—‹è½¬çŸ©é˜µ<br/>R(pos)]
        R2 --> RK[æ—‹è½¬åçš„ Key]
    end
```

**RoPE çš„ä¼˜åŠ¿**ï¼š
- ç›¸å¯¹ä½ç½®ä¿¡æ¯è‡ªç„¶ç¼–ç 
- æ”¯æŒä»»æ„é•¿åº¦å¤–æ¨
- è®¡ç®—é«˜æ•ˆ

```python
# RoPE çš„æ ¸å¿ƒæ€æƒ³ï¼ˆç®€åŒ–ï¼‰
def rotate_half(x):
    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]
    return torch.cat([-x2, x1], dim=-1)

def apply_rope(q, k, cos, sin):
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed
```

---

## 5. Multi-Head Attention

è¿™æ˜¯ Transformer çš„æ ¸å¿ƒç»„ä»¶ï¼Œè¯¦ç»†åŸç†å°†åœ¨ä¸‹ä¸€ç« ä»‹ç»ã€‚è¿™é‡Œç»™å‡ºç»“æ„æ¦‚è§ˆï¼š

```mermaid
graph TD
    subgraph Multi-Head Attention
        I[è¾“å…¥ X] --> WQ[W_Q]
        I --> WK[W_K]
        I --> WV[W_V]

        WQ --> Q[Query]
        WK --> K[Key]
        WV --> V[Value]

        Q --> SPLIT1[Split Heads]
        K --> SPLIT2[Split Heads]
        V --> SPLIT3[Split Heads]

        SPLIT1 --> H1[Head 1]
        SPLIT1 --> H2[Head 2]
        SPLIT1 --> HN[Head N]

        SPLIT2 --> H1
        SPLIT2 --> H2
        SPLIT2 --> HN

        SPLIT3 --> H1
        SPLIT3 --> H2
        SPLIT3 --> HN

        H1 --> CAT[Concat]
        H2 --> CAT
        HN --> CAT

        CAT --> WO[W_O]
        WO --> O[è¾“å‡º]
    end
```

**å‚æ•°é‡**ï¼š

```
Q, K, V æŠ•å½±: 3 Ã— hidden_dim Ã— hidden_dim
è¾“å‡ºæŠ•å½±: hidden_dim Ã— hidden_dim
æ€»è®¡: 4 Ã— hidden_dimÂ²
```

**ç¤ºä¾‹**ï¼ˆhidden_dim = 4096ï¼‰ï¼š
```
å‚æ•°é‡ = 4 Ã— 4096Â² = 67,108,864 â‰ˆ 67M
```

---

## 6. Feed Forward Network (FFN)

### 6.1 åŸºæœ¬ç»“æ„

FFN æ˜¯ä¸€ä¸ªç®€å•çš„ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œï¼š

```mermaid
graph LR
    I[è¾“å…¥<br/>hidden_dim] --> L1[Linear 1<br/>hidden â†’ intermediate]
    L1 --> ACT[æ¿€æ´»å‡½æ•°<br/>GELU/SiLU]
    ACT --> L2[Linear 2<br/>intermediate â†’ hidden]
    L2 --> O[è¾“å‡º<br/>hidden_dim]
```

```python
class FeedForward(nn.Module):
    def __init__(self, hidden_dim, intermediate_dim):
        super().__init__()
        self.up_proj = nn.Linear(hidden_dim, intermediate_dim)
        self.down_proj = nn.Linear(intermediate_dim, hidden_dim)
        self.activation = nn.GELU()

    def forward(self, x):
        # x: [batch, seq_len, hidden_dim]
        x = self.up_proj(x)       # [batch, seq_len, intermediate_dim]
        x = self.activation(x)     # [batch, seq_len, intermediate_dim]
        x = self.down_proj(x)      # [batch, seq_len, hidden_dim]
        return x
```

### 6.2 SwiGLU å˜ä½“

LLaMA ç­‰æ¨¡å‹ä½¿ç”¨ **SwiGLU** æ¿€æ´»å‡½æ•°ï¼š

```mermaid
graph LR
    I[è¾“å…¥] --> G[Gate Proj]
    I --> U[Up Proj]
    G --> SILU[SiLU æ¿€æ´»]
    SILU --> MUL[Ã—]
    U --> MUL
    MUL --> D[Down Proj]
    D --> O[è¾“å‡º]
```

```python
class SwiGLUFeedForward(nn.Module):
    def __init__(self, hidden_dim, intermediate_dim):
        super().__init__()
        self.gate_proj = nn.Linear(hidden_dim, intermediate_dim)
        self.up_proj = nn.Linear(hidden_dim, intermediate_dim)
        self.down_proj = nn.Linear(intermediate_dim, hidden_dim)

    def forward(self, x):
        gate = torch.nn.functional.silu(self.gate_proj(x))
        up = self.up_proj(x)
        return self.down_proj(gate * up)
```

### 6.3 FFN å‚æ•°é‡

**æ ‡å‡† FFN**ï¼š
```
å‚æ•°é‡ = 2 Ã— hidden_dim Ã— intermediate_dim
```

**SwiGLU FFN**ï¼ˆæœ‰ä¸‰ä¸ªæŠ•å½±çŸ©é˜µï¼‰ï¼š
```
å‚æ•°é‡ = 3 Ã— hidden_dim Ã— intermediate_dim
```

**ç¤ºä¾‹**ï¼ˆLLaMA-7Bï¼Œhidden=4096ï¼Œintermediate=11008ï¼‰ï¼š
```
å‚æ•°é‡ = 3 Ã— 4096 Ã— 11008 = 135,266,304 â‰ˆ 135M
```

---

## 7. Layer Normalization

### 7.1 ä¸ºä»€ä¹ˆéœ€è¦å½’ä¸€åŒ–

æ·±å±‚ç½‘ç»œä¸­ï¼Œæ¯å±‚è¾“å‡ºçš„åˆ†å¸ƒä¼šå‘ç”Ÿå˜åŒ–ï¼ˆInternal Covariate Shiftï¼‰ï¼Œå¯¼è‡´ï¼š
- è®­ç»ƒä¸ç¨³å®š
- éœ€è¦è¾ƒå°çš„å­¦ä¹ ç‡
- æ”¶æ•›æ…¢

Layer Normalization å°†æ¯å±‚è¾“å‡ºå½’ä¸€åŒ–åˆ°å‡å€¼ 0ã€æ–¹å·® 1 çš„åˆ†å¸ƒã€‚

### 7.2 è®¡ç®—å…¬å¼

```
LayerNorm(x) = Î³ Ã— (x - Î¼) / âˆš(ÏƒÂ² + Îµ) + Î²
```

å…¶ä¸­ï¼š
- `Î¼`ï¼šå‡å€¼
- `ÏƒÂ²`ï¼šæ–¹å·®
- `Îµ`ï¼šé˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°
- `Î³, Î²`ï¼šå¯å­¦ä¹ çš„ç¼©æ”¾å’Œåç§»å‚æ•°

```python
class LayerNorm(nn.Module):
    def __init__(self, hidden_dim, eps=1e-5):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_dim))
        self.bias = nn.Parameter(torch.zeros(hidden_dim))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        return self.weight * (x - mean) / torch.sqrt(var + self.eps) + self.bias
```

### 7.3 RMSNorm

LLaMA ç­‰æ¨¡å‹ä½¿ç”¨ **RMSNorm**ï¼Œå»æ‰äº†å‡å€¼ä¸­å¿ƒåŒ–ï¼š

```
RMSNorm(x) = Î³ Ã— x / âˆš(mean(xÂ²) + Îµ)
```

**ä¼˜åŠ¿**ï¼šè®¡ç®—æ›´ç®€å•ï¼Œæ•ˆæœç›¸å½“ã€‚

```python
class RMSNorm(nn.Module):
    def __init__(self, hidden_dim, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_dim))
        self.eps = eps

    def forward(self, x):
        rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)
        return self.weight * x / rms
```

### 7.4 Pre-Norm vs Post-Norm

```mermaid
graph TB
    subgraph Post-Norm
        I1[è¾“å…¥] --> ATT1[Attention]
        ATT1 --> ADD1[+]
        I1 --> ADD1
        ADD1 --> LN1[LayerNorm]
    end

    subgraph Pre-Normï¼ˆç°ä»£ LLM å¸¸ç”¨ï¼‰
        I2[è¾“å…¥] --> LN2[LayerNorm]
        LN2 --> ATT2[Attention]
        ATT2 --> ADD2[+]
        I2 --> ADD2
    end

    style LN2 fill:#c8e6c9
```

**Pre-Norm ä¼˜åŠ¿**ï¼š
- è®­ç»ƒæ›´ç¨³å®š
- å…è®¸æ›´æ·±çš„ç½‘ç»œ
- æ›´å®¹æ˜“æ”¶æ•›

---

## 8. æ®‹å·®è¿æ¥

### 8.1 ä»€ä¹ˆæ˜¯æ®‹å·®è¿æ¥

æ®‹å·®è¿æ¥è®©ä¿¡æ¯å¯ä»¥"è·³è¿‡"æŸäº›å±‚ç›´æ¥ä¼ é€’ï¼š

```
output = x + Layer(x)
```

### 8.2 ä¸ºä»€ä¹ˆæ®‹å·®è¿æ¥é‡è¦

```mermaid
graph LR
    subgraph æ— æ®‹å·®
        X1[x] --> L1[Layer 1] --> L2[Layer 2] --> L3[Layer 3] --> Y1[y]
    end

    subgraph æœ‰æ®‹å·®
        X2[x] --> LA[Layer 1] --> LB[Layer 2] --> LC[Layer 3] --> Y2[y]
        X2 --> Y2
        LA --> LB
        LB --> LC
    end
```

**ä¼˜åŠ¿**ï¼š
- ç¼“è§£æ¢¯åº¦æ¶ˆå¤±
- å…è®¸è®­ç»ƒæ›´æ·±çš„ç½‘ç»œ
- ä¿¡æ¯ç›´æ¥ä¼ é€’ä¸ä¼šä¸¢å¤±

---

## 9. å®Œæ•´ Transformer Block ä»£ç 

```python
import torch
import torch.nn as nn

class TransformerBlock(nn.Module):
    def __init__(self, hidden_dim, num_heads, intermediate_dim):
        super().__init__()
        self.norm1 = RMSNorm(hidden_dim)
        self.attention = MultiHeadAttention(hidden_dim, num_heads)
        self.norm2 = RMSNorm(hidden_dim)
        self.ffn = SwiGLUFeedForward(hidden_dim, intermediate_dim)

    def forward(self, x, attention_mask=None):
        # Pre-Norm + Attention + æ®‹å·®
        residual = x
        x = self.norm1(x)
        x = self.attention(x, attention_mask)
        x = residual + x

        # Pre-Norm + FFN + æ®‹å·®
        residual = x
        x = self.norm2(x)
        x = self.ffn(x)
        x = residual + x

        return x
```

---

## 10. å‚æ•°é‡è®¡ç®—å®æˆ˜

### 10.1 LLaMA-2-7B å‚æ•°åˆ†å¸ƒ

| ç»„ä»¶ | å…¬å¼ | å‚æ•°é‡ |
|------|------|--------|
| Embedding | vocab Ã— hidden | 32000 Ã— 4096 = 131M |
| æ¯å±‚ Attention Q | hidden Ã— hidden | 4096Â² = 16.8M |
| æ¯å±‚ Attention K | hidden Ã— (hidden/n_heads Ã— n_kv_heads) | 4096 Ã— 4096 = 16.8M |
| æ¯å±‚ Attention V | hidden Ã— (hidden/n_heads Ã— n_kv_heads) | 4096 Ã— 4096 = 16.8M |
| æ¯å±‚ Attention O | hidden Ã— hidden | 4096Â² = 16.8M |
| æ¯å±‚ FFN gate | hidden Ã— intermediate | 4096 Ã— 11008 = 45.1M |
| æ¯å±‚ FFN up | hidden Ã— intermediate | 4096 Ã— 11008 = 45.1M |
| æ¯å±‚ FFN down | intermediate Ã— hidden | 11008 Ã— 4096 = 45.1M |
| æ¯å±‚ Norm | 2 Ã— hidden | 2 Ã— 4096 = 8K |
| LM Head | hidden Ã— vocab | 4096 Ã— 32000 = 131M |

**æ¯å±‚æ€»è®¡**ï¼šçº¦ 202M å‚æ•°
**32 å±‚æ€»è®¡**ï¼š32 Ã— 202M = 6.46B
**åŠ ä¸Š Embedding å’Œ LM Head**ï¼šçº¦ 6.7B

### 10.2 å‚æ•°åˆ†å¸ƒé¥¼å›¾

```mermaid
pie title LLaMA-7B å‚æ•°åˆ†å¸ƒ
    "Attention (Q/K/V/O)" : 32
    "FFN" : 65
    "Embedding + LM Head" : 2
    "Norm" : 1
```

**å…³é”®è§‚å¯Ÿ**ï¼š
- **FFN å æ¯”æœ€å¤§**ï¼ˆçº¦ 65%ï¼‰
- **Attention å…¶æ¬¡**ï¼ˆçº¦ 32%ï¼‰
- **Embedding å æ¯”å¾ˆå°**ï¼ˆçº¦ 2%ï¼‰

è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆ vLLM ä¸»è¦ä¼˜åŒ– Attention å’Œå†…å­˜ç®¡ç†ï¼Œè€Œä¸æ˜¯ FFNã€‚

---

## 11. æœ¬ç« å°ç»“

### æ¶æ„è¦ç‚¹

1. **Decoder-Only æ¶æ„**ï¼šç°ä»£ LLM çš„ä¸»æµé€‰æ‹©
2. **Transformer Block**ï¼šAttention + FFN + Norm + æ®‹å·®
3. **ä½ç½®ç¼–ç **ï¼šRoPE æ˜¯ç°ä»£æ ‡å‡†

### å…³é”®ç»„ä»¶

| ç»„ä»¶ | ä½œç”¨ | ç°ä»£å®ç° |
|------|------|---------|
| Embedding | Token â†’ Vector | ç›´æ¥æŸ¥è¡¨ |
| ä½ç½®ç¼–ç  | æ³¨å…¥ä½ç½®ä¿¡æ¯ | RoPE |
| Self-Attention | æ•è·åºåˆ—å…³ç³» | Multi-Head |
| FFN | éçº¿æ€§å˜æ¢ | SwiGLU |
| Layer Norm | ç¨³å®šè®­ç»ƒ | RMSNorm |
| æ®‹å·®è¿æ¥ | ä¿¡æ¯ç›´ä¼  | Pre-Norm |

### å‚æ•°åˆ†å¸ƒ

- FFN å ä¸»å¯¼ï¼ˆçº¦ 65%ï¼‰
- Attention çº¦ 32%
- Embedding çº¦ 2%

### ä¸ vLLM çš„å…³è”

- Attention è®¡ç®—æ˜¯ KV Cache ä¼˜åŒ–çš„æ ¸å¿ƒ
- å‚æ•°åˆ†å¸ƒå½±å“æ˜¾å­˜ä½¿ç”¨å’Œä¼˜åŒ–ç­–ç•¥
- ä½ç½®ç¼–ç å½±å“åºåˆ—é•¿åº¦æ”¯æŒ

---

## æ€è€ƒé¢˜

1. ä¸ºä»€ä¹ˆ Decoder-Only æ¶æ„åœ¨ LLM ä¸­æ¯” Encoder-Decoder æ›´æµè¡Œï¼Ÿ
2. RoPE ç›¸æ¯”æ­£å¼¦ä½ç½®ç¼–ç æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ
3. ä¸ºä»€ä¹ˆ FFN çš„å‚æ•°é‡æ¯” Attention å¤šï¼Œä½† vLLM ä¸»è¦ä¼˜åŒ– Attentionï¼Ÿ

---

## ä¸‹ä¸€æ­¥

Transformer æ¶æ„ä»‹ç»å®Œæ¯•ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†æ·±å…¥å­¦ä¹ å…¶æ ¸å¿ƒâ€”â€”æ³¨æ„åŠ›æœºåˆ¶ï¼š

ğŸ‘‰ [ä¸‹ä¸€ç« ï¼šæ³¨æ„åŠ›æœºåˆ¶åŸç†](03-attention-mechanism.md)
