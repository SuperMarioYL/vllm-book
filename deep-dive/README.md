# vLLM åŸç†æ·±åº¦è§£æ

> é¢å‘æ·±åº¦å­¦ä¹ åˆå­¦è€…çš„ vLLM æŠ€æœ¯æ–‡æ¡£

---

## æ–‡æ¡£ç®€ä»‹

æœ¬æ–‡æ¡£ç³»åˆ—æ—¨åœ¨å¸®åŠ©æ·±åº¦å­¦ä¹ åˆå­¦è€…æ·±å…¥ç†è§£ vLLM â€”â€” ä¸€ä¸ªé«˜æ€§èƒ½çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†å’ŒæœåŠ¡æ¡†æ¶ã€‚æˆ‘ä»¬å°†ä»æœ€åŸºç¡€çš„æ¦‚å¿µå‡ºå‘ï¼Œé€æ­¥æ·±å…¥åˆ°æ ¸å¿ƒç®—æ³•å’Œä»£ç å®ç°ï¼Œè®©ä½ ä¸ä»…çŸ¥å…¶ç„¶ï¼Œæ›´çŸ¥å…¶æ‰€ä»¥ç„¶ã€‚

### é€‚ç”¨è¯»è€…

- **æ·±åº¦å­¦ä¹ åˆå­¦è€…**ï¼šäº†è§£åŸºæœ¬çš„ Python ç¼–ç¨‹ï¼Œå¯¹æœºå™¨å­¦ä¹ æœ‰åˆæ­¥è®¤è¯†
- **LLM åº”ç”¨å¼€å‘è€…**ï¼šå¸Œæœ›äº†è§£æ¨ç†æ¡†æ¶åº•å±‚åŸç†
- **ç³»ç»Ÿå·¥ç¨‹å¸ˆ**ï¼šè´Ÿè´£éƒ¨ç½²å’Œä¼˜åŒ– LLM æœåŠ¡
- **ç ”ç©¶äººå‘˜**ï¼šç ”ç©¶ LLM æ¨ç†ä¼˜åŒ–æŠ€æœ¯

### ä½ å°†å­¦åˆ°

- å¤§è¯­è¨€æ¨¡å‹æ¨ç†é¢ä¸´çš„æ ¸å¿ƒæŒ‘æˆ˜
- Transformer æ¶æ„å’Œæ³¨æ„åŠ›æœºåˆ¶çš„å·¥ä½œåŸç†
- vLLM çš„æ ¸å¿ƒåˆ›æ–°ï¼šPagedAttention å’Œè¿ç»­æ‰¹å¤„ç†
- ä»å…¥å£åˆ°è¾“å‡ºçš„å®Œæ•´ä»£ç æ‰§è¡Œé“¾è·¯
- å¦‚ä½•è°ƒè¯•å’Œåˆ†æ vLLM ä»£ç 

---

## æ–‡æ¡£ç»“æ„æ€»è§ˆ

```
docs/deep-dive/
â”œâ”€â”€ README.md                              # æœ¬æ–‡ä»¶ - å¯¼èˆªä¸å­¦ä¹ æŒ‡å—
â”‚
â”œâ”€â”€ 01-introduction/                       # ç¬¬ä¸€éƒ¨åˆ†ï¼šå…¥é—¨ç¯‡
â”‚   â”œâ”€â”€ 01-why-vllm.md                     # ä¸ºä»€ä¹ˆéœ€è¦ vLLM
â”‚   â”œâ”€â”€ 02-llm-inference-challenges.md     # LLM æ¨ç†é¢ä¸´çš„æŒ‘æˆ˜
â”‚   â””â”€â”€ 03-vllm-overview.md                # vLLM æ•´ä½“æ¶æ„æ¦‚è§ˆ
â”‚
â”œâ”€â”€ 02-dl-fundamentals/                    # ç¬¬äºŒéƒ¨åˆ†ï¼šæ·±åº¦å­¦ä¹ åŸºç¡€
â”‚   â”œâ”€â”€ 01-neural-network-basics.md        # ç¥ç»ç½‘ç»œåŸºç¡€
â”‚   â”œâ”€â”€ 02-transformer-architecture.md     # Transformer æ¶æ„è¯¦è§£
â”‚   â”œâ”€â”€ 03-attention-mechanism.md          # æ³¨æ„åŠ›æœºåˆ¶åŸç†
â”‚   â”œâ”€â”€ 04-kv-cache-concept.md             # KV Cache æ¦‚å¿µ
â”‚   â””â”€â”€ 05-llm-generation-process.md       # LLM ç”Ÿæˆè¿‡ç¨‹
â”‚
â”œâ”€â”€ 03-core-modules/                       # ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ ¸å¿ƒæ¨¡å—è¯¦è§£
â”‚   â”œâ”€â”€ 01-paged-attention.md              # PagedAttention åˆ†é¡µæ³¨æ„åŠ›
â”‚   â”œâ”€â”€ 02-kv-cache-manager.md             # KV Cache ç®¡ç†å™¨
â”‚   â”œâ”€â”€ 03-block-pool.md                   # Block Pool å†…å­˜å—æ± 
â”‚   â”œâ”€â”€ 04-scheduler.md                    # è°ƒåº¦å™¨åŸç†
â”‚   â””â”€â”€ 05-continuous-batching.md          # è¿ç»­æ‰¹å¤„ç†æœºåˆ¶
â”‚
â”œâ”€â”€ 04-code-walkthrough/                   # ç¬¬å››éƒ¨åˆ†ï¼šä»£ç é“¾è·¯åˆ†æ
â”‚   â”œâ”€â”€ 01-entry-points.md                 # å…¥å£ç‚¹åˆ†æ
â”‚   â”œâ”€â”€ 02-request-lifecycle.md            # è¯·æ±‚ç”Ÿå‘½å‘¨æœŸ
â”‚   â”œâ”€â”€ 03-engine-core-flow.md             # å¼•æ“æ ¸å¿ƒæµç¨‹
â”‚   â”œâ”€â”€ 04-scheduling-deep-dive.md         # è°ƒåº¦æµç¨‹æ·±å…¥
â”‚   â”œâ”€â”€ 05-model-execution.md              # æ¨¡å‹æ‰§è¡Œæµç¨‹
â”‚   â””â”€â”€ 06-debug-guide.md                  # ä»£ç è°ƒè¯•æŒ‡å—
â”‚
â”œâ”€â”€ 05-advanced-topics/                    # ç¬¬äº”éƒ¨åˆ†ï¼šè¿›é˜¶ä¸»é¢˜
â”‚   â”œâ”€â”€ 01-quantization.md                 # é‡åŒ–æŠ€æœ¯
â”‚   â”œâ”€â”€ 02-speculative-decoding.md         # æŠ•æœºè§£ç 
â”‚   â””â”€â”€ 03-distributed-inference.md        # åˆ†å¸ƒå¼æ¨ç†
â”‚
â””â”€â”€ appendix/                              # é™„å½•
    â”œâ”€â”€ glossary.md                        # æœ¯è¯­è¡¨
    â”œâ”€â”€ code-map.md                        # ä»£ç æ–‡ä»¶ç´¢å¼•
    â””â”€â”€ references.md                      # å‚è€ƒèµ„æ–™
```

---

## å­¦ä¹ è·¯çº¿å›¾

æˆ‘ä»¬æä¾›ä¸¤æ¡å­¦ä¹ è·¯å¾„ï¼Œä½ å¯ä»¥æ ¹æ®è‡ªå·±çš„èƒŒæ™¯å’Œç›®æ ‡é€‰æ‹©åˆé€‚çš„è·¯çº¿ã€‚

### è·¯å¾„ä¸€ï¼šåŸºç¡€è·¯å¾„ï¼ˆæ¨èæ–°æ‰‹ï¼‰

é€‚åˆæ·±åº¦å­¦ä¹ åŸºç¡€è¾ƒè–„å¼±çš„è¯»è€…ï¼Œä»åŸºç¡€æ¦‚å¿µå­¦èµ·ã€‚

```mermaid
flowchart TD
    subgraph ç¬¬ä¸€é˜¶æ®µï¼šç†è§£é—®é¢˜
        A[README.md<br/>äº†è§£æ–‡æ¡£ç»“æ„] --> B[01-why-vllm.md<br/>ç†è§£é—®é¢˜èƒŒæ™¯]
        B --> C[02-llm-inference-challenges.md<br/>æ·±å…¥ç†è§£æŒ‘æˆ˜]
    end

    subgraph ç¬¬äºŒé˜¶æ®µï¼šå­¦ä¹ åŸºç¡€
        C --> D[01-neural-network-basics.md<br/>ç¥ç»ç½‘ç»œåŸºç¡€]
        D --> E[02-transformer-architecture.md<br/>Transformer æ¶æ„]
        E --> F[03-attention-mechanism.md<br/>æ³¨æ„åŠ›æœºåˆ¶]
        F --> G[04-kv-cache-concept.md<br/>KV Cache æ¦‚å¿µ]
        G --> H[05-llm-generation-process.md<br/>ç”Ÿæˆè¿‡ç¨‹]
    end

    subgraph ç¬¬ä¸‰é˜¶æ®µï¼šæŒæ¡æ ¸å¿ƒ
        H --> I[03-vllm-overview.md<br/>vLLM æ¶æ„å…¨è²Œ]
        I --> J[01-paged-attention.md<br/>PagedAttention]
        J --> K[05-continuous-batching.md<br/>è¿ç»­æ‰¹å¤„ç†]
    end

    subgraph ç¬¬å››é˜¶æ®µï¼šä»£ç å®è·µ
        K --> L[01-entry-points.md<br/>ä»£ç å…¥å£]
        L --> M[03-engine-core-flow.md<br/>å¼•æ“æµç¨‹]
        M --> N[glossary.md<br/>æœ¯è¯­å‚è€ƒ]
    end

    style A fill:#e1f5fe
    style N fill:#c8e6c9
```

**é¢„è®¡é˜…è¯»é‡**ï¼šçº¦ 70,000 å­—ï¼Œå»ºè®®åˆ† 5-7 å¤©å®Œæˆ

### è·¯å¾„äºŒï¼šè¿›é˜¶è·¯å¾„ï¼ˆé€‚åˆæœ‰åŸºç¡€çš„è¯»è€…ï¼‰

å¦‚æœä½ å·²ç»äº†è§£ Transformer å’Œ KV Cache çš„åŸºæœ¬æ¦‚å¿µï¼Œå¯ä»¥ç›´æ¥è¿›å…¥æ ¸å¿ƒå†…å®¹ã€‚

```mermaid
flowchart TD
    subgraph å¿«é€Ÿå…¥é—¨
        A[01-why-vllm.md] --> B[03-vllm-overview.md]
    end

    subgraph æ ¸å¿ƒæ¨¡å—
        B --> C[01-paged-attention.md]
        C --> D[02-kv-cache-manager.md]
        D --> E[03-block-pool.md]
        E --> F[04-scheduler.md]
        F --> G[05-continuous-batching.md]
    end

    subgraph ä»£ç æ·±å…¥
        G --> H[02-request-lifecycle.md]
        H --> I[04-scheduling-deep-dive.md]
        I --> J[05-model-execution.md]
        J --> K[06-debug-guide.md]
    end

    subgraph è¿›é˜¶ä¸»é¢˜
        K --> L[01-quantization.md]
        L --> M[02-speculative-decoding.md]
        M --> N[03-distributed-inference.md]
    end

    style A fill:#e1f5fe
    style N fill:#c8e6c9
```

**é¢„è®¡é˜…è¯»é‡**ï¼šçº¦ 50,000 å­—ï¼Œå»ºè®®åˆ† 3-5 å¤©å®Œæˆ

---

## å„éƒ¨åˆ†å†…å®¹æ¦‚è¦

### ç¬¬ä¸€éƒ¨åˆ†ï¼šå…¥é—¨ç¯‡

> **ç›®æ ‡**ï¼šç†è§£ä¸ºä»€ä¹ˆéœ€è¦ vLLMï¼Œå®ƒè§£å†³äº†ä»€ä¹ˆé—®é¢˜

| æ–‡æ¡£ | å†…å®¹ç®€ä»‹ | å…³é”®æ¦‚å¿µ |
|------|---------|---------|
| [01-why-vllm.md](01-introduction/01-why-vllm.md) | ä»‹ç» LLM æ¨ç†éƒ¨ç½²çš„ç—›ç‚¹ï¼Œä»¥åŠ vLLM çš„è§£å†³æ–¹æ¡ˆ | é™æ€æ‰¹å¤„ç†ã€æ˜¾å­˜ç¢ç‰‡åŒ–ã€ååé‡ |
| [02-llm-inference-challenges.md](01-introduction/02-llm-inference-challenges.md) | æ·±å…¥åˆ†ææ˜¾å­˜ã€è®¡ç®—ã€æ‰¹å¤„ç†ä¸‰å¤§æŒ‘æˆ˜ | Prefillã€Decodeã€TTFTã€TPS |
| [03-vllm-overview.md](01-introduction/03-vllm-overview.md) | vLLM æ•´ä½“æ¶æ„å’Œæ ¸å¿ƒç»„ä»¶ä»‹ç» | LLMEngineã€Schedulerã€ModelRunner |

### ç¬¬äºŒéƒ¨åˆ†ï¼šæ·±åº¦å­¦ä¹ åŸºç¡€

> **ç›®æ ‡**ï¼šä¸ºç†è§£ vLLM åŸç†æ‰“ä¸‹å¿…è¦çš„åŸºç¡€çŸ¥è¯†

| æ–‡æ¡£ | å†…å®¹ç®€ä»‹ | å…³é”®æ¦‚å¿µ |
|------|---------|---------|
| [01-neural-network-basics.md](02-dl-fundamentals/01-neural-network-basics.md) | ç¥ç»ç½‘ç»œçš„åŸºæœ¬æ¦‚å¿µå’Œç»„æˆ | ç¥ç»å…ƒã€æ¿€æ´»å‡½æ•°ã€å¼ é‡ |
| [02-transformer-architecture.md](02-dl-fundamentals/02-transformer-architecture.md) | Transformer æ¶æ„è¯¦ç»†å‰–æ | Embeddingã€ä½ç½®ç¼–ç ã€FFN |
| [03-attention-mechanism.md](02-dl-fundamentals/03-attention-mechanism.md) | æ³¨æ„åŠ›æœºåˆ¶çš„åŸç†å’Œè®¡ç®—è¿‡ç¨‹ | Q/K/Vã€å¤šå¤´æ³¨æ„åŠ›ã€å› æœæ©ç  |
| [04-kv-cache-concept.md](02-dl-fundamentals/04-kv-cache-concept.md) | KV Cache çš„ä½œç”¨å’Œå®ç°åŸç† | å¢é‡è®¡ç®—ã€ç¼“å­˜å¤ç”¨ |
| [05-llm-generation-process.md](02-dl-fundamentals/05-llm-generation-process.md) | LLM æ–‡æœ¬ç”Ÿæˆçš„å®Œæ•´æµç¨‹ | Prefillã€Decodeã€é‡‡æ ·ç­–ç•¥ |

### ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ ¸å¿ƒæ¨¡å—è¯¦è§£

> **ç›®æ ‡**ï¼šæ·±å…¥ç†è§£ vLLM çš„æ ¸å¿ƒåˆ›æ–°å’Œå®ç°

| æ–‡æ¡£ | å†…å®¹ç®€ä»‹ | å…³é”®æ¦‚å¿µ |
|------|---------|---------|
| [01-paged-attention.md](03-core-modules/01-paged-attention.md) | PagedAttention çš„è®¾è®¡æ€æƒ³å’Œå®ç° | Blockã€Block Tableã€Slot Mapping |
| [02-kv-cache-manager.md](03-core-modules/02-kv-cache-manager.md) | KV Cache ç®¡ç†å™¨çš„å·¥ä½œåŸç† | åˆ†é…ã€é‡Šæ”¾ã€å‰ç¼€ç¼“å­˜ |
| [03-block-pool.md](03-core-modules/03-block-pool.md) | å†…å­˜å—æ± çš„æ•°æ®ç»“æ„å’Œç®—æ³• | LRU é©±é€ã€å¼•ç”¨è®¡æ•° |
| [04-scheduler.md](03-core-modules/04-scheduler.md) | è°ƒåº¦å™¨çš„è®¾è®¡å’Œè°ƒåº¦ç®—æ³• | ä¼˜å…ˆçº§ã€æŠ¢å ã€èµ„æºç®¡ç† |
| [05-continuous-batching.md](03-core-modules/05-continuous-batching.md) | è¿ç»­æ‰¹å¤„ç†çš„åŸç†å’Œä¼˜åŠ¿ | è¿­ä»£çº§è°ƒåº¦ã€åŠ¨æ€æ‰¹å¤„ç† |

### ç¬¬å››éƒ¨åˆ†ï¼šä»£ç é“¾è·¯åˆ†æ

> **ç›®æ ‡**ï¼šè·Ÿè¸ªä»£ç æ‰§è¡Œè·¯å¾„ï¼Œç†è§£å®ç°ç»†èŠ‚

| æ–‡æ¡£ | å†…å®¹ç®€ä»‹ | å…³é”®ä»£ç æ–‡ä»¶ |
|------|---------|-------------|
| [01-entry-points.md](04-code-walkthrough/01-entry-points.md) | CLI å’Œ Python API å…¥å£åˆ†æ | `llm.py`, `main.py` |
| [02-request-lifecycle.md](04-code-walkthrough/02-request-lifecycle.md) | è¯·æ±‚ä»åˆ›å»ºåˆ°å®Œæˆçš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸ | `request.py` |
| [03-engine-core-flow.md](04-code-walkthrough/03-engine-core-flow.md) | å¼•æ“æ ¸å¿ƒçš„ step() å¾ªç¯åˆ†æ | `core.py`, `llm_engine.py` |
| [04-scheduling-deep-dive.md](04-code-walkthrough/04-scheduling-deep-dive.md) | schedule() æ–¹æ³•çš„è¯¦ç»†æµç¨‹ | `scheduler.py` |
| [05-model-execution.md](04-code-walkthrough/05-model-execution.md) | æ¨¡å‹æ‰§è¡Œå’Œé‡‡æ ·è¿‡ç¨‹ | `gpu_model_runner.py` |
| [06-debug-guide.md](04-code-walkthrough/06-debug-guide.md) | è°ƒè¯•æŠ€å·§å’Œå¸¸è§é—®é¢˜æ’æŸ¥ | - |

### ç¬¬äº”éƒ¨åˆ†ï¼šè¿›é˜¶ä¸»é¢˜

> **ç›®æ ‡**ï¼šäº†è§£ vLLM çš„é«˜çº§åŠŸèƒ½å’Œä¼˜åŒ–æŠ€æœ¯

| æ–‡æ¡£ | å†…å®¹ç®€ä»‹ |
|------|---------|
| [01-quantization.md](05-advanced-topics/01-quantization.md) | æ¨¡å‹é‡åŒ–æŠ€æœ¯ï¼šINT8ã€INT4ã€AWQã€GPTQ |
| [02-speculative-decoding.md](05-advanced-topics/02-speculative-decoding.md) | æŠ•æœºè§£ç åŠ é€ŸæŠ€æœ¯ |
| [03-distributed-inference.md](05-advanced-topics/03-distributed-inference.md) | å¼ é‡å¹¶è¡Œå’Œæµæ°´çº¿å¹¶è¡Œ |

### é™„å½•

| æ–‡æ¡£ | å†…å®¹ç®€ä»‹ |
|------|---------|
| [glossary.md](appendix/glossary.md) | æœ¯è¯­è¡¨ï¼šæ‰€æœ‰ä¸“ä¸šæœ¯è¯­çš„è§£é‡Š |
| [code-map.md](appendix/code-map.md) | ä»£ç æ–‡ä»¶ç´¢å¼•ï¼šæŒ‰åŠŸèƒ½åˆ†ç±»çš„æ–‡ä»¶åˆ—è¡¨ |
| [references.md](appendix/references.md) | å‚è€ƒèµ„æ–™ï¼šè®ºæ–‡ã€æ–‡æ¡£ã€åšå®¢é“¾æ¥ |

---

## æ ¸å¿ƒæ¦‚å¿µé€Ÿè§ˆ

åœ¨æ·±å…¥å­¦ä¹ ä¹‹å‰ï¼Œå…ˆå¿«é€Ÿäº†è§£å‡ ä¸ªæ ¸å¿ƒæ¦‚å¿µï¼š

### ä»€ä¹ˆæ˜¯ vLLMï¼Ÿ

vLLM æ˜¯ä¸€ä¸ªå¿«é€Ÿä¸”æ˜“ç”¨çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†å’ŒæœåŠ¡åº“ï¼Œç”±åŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡å¼€å‘ã€‚å®ƒçš„æ ¸å¿ƒåˆ›æ–°æ˜¯ **PagedAttention** ç®—æ³•ï¼Œé€šè¿‡ç±»ä¼¼æ“ä½œç³»ç»Ÿè™šæ‹Ÿå†…å­˜çš„æ–¹å¼ç®¡ç† KV Cacheï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†ååé‡ã€‚

### ä¸ºä»€ä¹ˆéœ€è¦ vLLMï¼Ÿ

```mermaid
graph LR
    subgraph ä¼ ç»Ÿæ–¹æ¡ˆçš„é—®é¢˜
        A[æ˜¾å­˜ç¢ç‰‡åŒ–] --> D[èµ„æºæµªè´¹]
        B[é™æ€æ‰¹å¤„ç†] --> D
        C[ä½ GPU åˆ©ç”¨ç‡] --> D
    end

    subgraph vLLM çš„è§£å†³æ–¹æ¡ˆ
        E[PagedAttention] --> H[é«˜ååé‡]
        F[è¿ç»­æ‰¹å¤„ç†] --> H
        G[é«˜æ•ˆå†…å­˜ç®¡ç†] --> H
    end

    D -.->|vs| H

    style D fill:#ffcdd2
    style H fill:#c8e6c9
```

### vLLM çš„æ ¸å¿ƒç»„ä»¶

```mermaid
graph TD
    subgraph ç”¨æˆ·æ¥å£
        A[LLM ç±»<br/>Python API]
        B[vllm serve<br/>CLI]
        C[OpenAI API<br/>HTTP æœåŠ¡]
    end

    subgraph å¼•æ“å±‚
        D[LLMEngine<br/>å¼•æ“åè°ƒ]
    end

    subgraph æ ¸å¿ƒå±‚
        E[EngineCore<br/>æ ¸å¿ƒé€»è¾‘]
        F[Scheduler<br/>è¯·æ±‚è°ƒåº¦]
        G[KVCacheManager<br/>ç¼“å­˜ç®¡ç†]
    end

    subgraph æ‰§è¡Œå±‚
        H[ModelExecutor<br/>æ¨¡å‹æ‰§è¡Œ]
        I[GPUModelRunner<br/>GPU è¿è¡Œ]
    end

    A --> D
    B --> D
    C --> D
    D --> E
    E --> F
    E --> H
    F --> G
    H --> I
```

---

## å¦‚ä½•ä½¿ç”¨æœ¬æ–‡æ¡£

### é˜…è¯»å»ºè®®

1. **å¾ªåºæ¸è¿›**ï¼šæŒ‰ç…§æ¨èçš„å­¦ä¹ è·¯å¾„é˜…è¯»ï¼Œä¸è¦è·³è¿‡åŸºç¡€éƒ¨åˆ†
2. **åŠ¨æ‰‹å®è·µ**ï¼šé‡åˆ°ä»£ç ç¤ºä¾‹æ—¶ï¼Œå°è¯•åœ¨æœ¬åœ°è¿è¡Œå’Œä¿®æ”¹
3. **ç»“åˆæºç **ï¼šé˜…è¯»æ–‡æ¡£æ—¶ï¼ŒåŒæ—¶æ‰“å¼€å¯¹åº”çš„æºä»£ç æ–‡ä»¶
4. **åšå¥½ç¬”è®°**ï¼šè®°å½•ä¸ç†è§£çš„æ¦‚å¿µï¼Œåå¤é˜…è¯»ç›¸å…³ç« èŠ‚

### ä»£ç é˜…è¯»æŠ€å·§

æœ¬æ–‡æ¡£ä¸­çš„ä»£ç å¼•ç”¨æ ¼å¼ä¸º `æ–‡ä»¶è·¯å¾„:è¡Œå·`ï¼Œä¾‹å¦‚ï¼š

- `vllm/entrypoints/llm.py:42` è¡¨ç¤º `llm.py` æ–‡ä»¶çš„ç¬¬ 42 è¡Œ
- `vllm/v1/core/sched/scheduler.py:313` è¡¨ç¤º `scheduler.py` æ–‡ä»¶çš„ç¬¬ 313 è¡Œ

å»ºè®®ä½¿ç”¨ IDEï¼ˆå¦‚ VSCode æˆ– PyCharmï¼‰æ‰“å¼€ vLLM æºç ï¼Œé…åˆæ–‡æ¡£ä¸€èµ·é˜…è¯»ã€‚

### å›¾è¡¨è¯´æ˜

æœ¬æ–‡æ¡£ä½¿ç”¨ Mermaid ç»˜åˆ¶å„ç±»å›¾è¡¨ï¼š

- **æµç¨‹å›¾ï¼ˆflowchartï¼‰**ï¼šå±•ç¤ºä»£ç æ‰§è¡Œæµç¨‹
- **æ—¶åºå›¾ï¼ˆsequenceDiagramï¼‰**ï¼šå±•ç¤ºç»„ä»¶ä¹‹é—´çš„äº¤äº’
- **ç±»å›¾ï¼ˆclassDiagramï¼‰**ï¼šå±•ç¤ºæ•°æ®ç»“æ„
- **çŠ¶æ€å›¾ï¼ˆstateDiagramï¼‰**ï¼šå±•ç¤ºçŠ¶æ€è½¬æ¢
- **é¥¼å›¾ï¼ˆpieï¼‰**ï¼šå±•ç¤ºå æ¯”åˆ†å¸ƒ

å¦‚æœä½ çš„ Markdown é˜…è¯»å™¨ä¸æ”¯æŒ Mermaidï¼Œå¯ä»¥ä½¿ç”¨ [Mermaid Live Editor](https://mermaid.live/) åœ¨çº¿æŸ¥çœ‹ã€‚

---

## å¿«é€Ÿå¼€å§‹

å¦‚æœä½ è¿«ä¸åŠå¾…æƒ³è¦å¼€å§‹å­¦ä¹ ï¼Œè¿™é‡Œæ˜¯ç¬¬ä¸€æ­¥ï¼š

**å¯¹äºæ·±åº¦å­¦ä¹ æ–°æ‰‹**ï¼š
```
å¼€å§‹é˜…è¯» â†’ 01-introduction/01-why-vllm.md
```

**å¯¹äºæœ‰ç»éªŒçš„å¼€å‘è€…**ï¼š
```
å¼€å§‹é˜…è¯» â†’ 01-introduction/03-vllm-overview.md
```

---

## æ–‡æ¡£ç‰ˆæœ¬

- **vLLM ç‰ˆæœ¬**ï¼šåŸºäº vLLM v1 æ¶æ„
- **æ–‡æ¡£ç‰ˆæœ¬**ï¼š1.0
- **æœ€åæ›´æ–°**ï¼š2025 å¹´ 1 æœˆ

---

## åé¦ˆä¸è´¡çŒ®

å¦‚æœä½ åœ¨é˜…è¯»è¿‡ç¨‹ä¸­å‘ç°ä»»ä½•é—®é¢˜æˆ–æœ‰æ”¹è¿›å»ºè®®ï¼Œæ¬¢è¿ï¼š

1. åœ¨ vLLM GitHub ä»“åº“æäº¤ Issue
2. ç›´æ¥æäº¤ Pull Request æ”¹è¿›æ–‡æ¡£

---

> **æç¤º**ï¼šå‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹ vLLM çš„æ·±åº¦å­¦ä¹ ä¹‹æ—…ï¼
>
> ğŸ‘‰ [å¼€å§‹é˜…è¯»ï¼šä¸ºä»€ä¹ˆéœ€è¦ vLLM](01-introduction/01-why-vllm.md)
